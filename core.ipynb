{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Please create a new folder named \"all\" to store all txt files of threads in the original folder.\n",
    "from bs4 import BeautifulSoup, Tag\n",
    "from collections import OrderedDict\n",
    "from pathlib import Path\n",
    "import os\n",
    "import nltk\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "from pandas import Series, DataFrame\n",
    "from pylab import rcParams\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics \n",
    "from sklearn.metrics import classification_report\n",
    "from graph_constructor import get_graph, get_digraph, get_graph_from_list, get_chronological_clustering_coef, get_chronological_pgrank\n",
    "# from key_student_extractor import get_key_student\n",
    "from preprocess import preprocess_text\n",
    "import networkx as nx\n",
    "import util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#html-css-javascript LgWwihnoEeWDtQoum3sFeQ\n",
    "#python_database eQJvsjn9EeWJaxK5AT4frw\n",
    "#python_network Y4DUPDpQEeWO-Qq6rEZAow\n",
    "#python 7A1yFTaREeWWBQrVFXqd1w\n",
    "#hybrid-mobile-development -gcU5xn4EeWwrBKfKrqlSQ\n",
    "#machine-learning Gtv4Xb1-EeS-ViIACwYKVQ\n",
    "#learning-how-to-learn GdeNrll1EeSROyIACtiVvg\n",
    "#angular-js 52blABnqEeW9dA4X94-nLQ\n",
    "#server-side-development ngZrURn5EeWwrBKfKrqlSQ\n",
    "#web-frameworks ycQnChn3EeWDtQoum3sFeQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef get_vectors_with_format_of_training_data(training_vectors, tfidf_training_model, testing_vectors, tfidf_testing_model):\\n    \\n    # word-index dictionaries\\n    training_term_index_dic = tfidf_training_model.vocabulary_\\n    testing_term_index_dic = tfidf_testing_model.vocabulary_\\n    \\n    # revert term dic\\n    training_index_term_dic = {}\\n    for term in training_term_index_dic:\\n        index = training_term_index_dic[term]\\n        training_index_term_dic[index] = term\\n        \\n    testing_index_term_dic = {}\\n    for term in testing_term_index_dic:\\n        index = testing_term_index_dic[term]\\n        testing_index_term_dic[index] = term\\n        \\n    training_vectors_as_list = training_vectors.tolist()\\n    combined_vector = training_vectors_as_list\\n    # iterate each word in the training vector. If we find the word in the testing dataset, assign the value. Otherwise\\n    # assign 0.\\n    for testing_vector in testing_vectors:\\n        current_vector_as_list = []\\n        for training_index in range (0, training_vectors.shape[1]):\\n            if training_index not in training_index_term_dic:\\n                print(\"Fatal error!!!\")\\n            else:\\n                term = training_index_term_dic[training_index]\\n                \\n                if term in testing_term_index_dic:\\n                    testing_index = testing_term_index_dic[term]\\n                    current_value = testing_vector[testing_index]\\n                    current_vector_as_list.append(current_value)\\n                    \\n                else:\\n                    current_vector_as_list.append(0)\\n\\n        combined_vector.append(current_vector_as_list)\\n        \\n    return np.asarray(combined_vector)\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def is_word(string):\n",
    "    for char in string:\n",
    "        if not char.isalpha():\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def tf_idf(docs, queries, tokenizer):\n",
    "    \"\"\"\n",
    "    performs TF-IDF vectorization for documents and queries\n",
    "    Parameters\n",
    "        ----------\n",
    "        docs : list\n",
    "            list of documents\n",
    "        queries : list\n",
    "            list of queries\n",
    "        tokenizer : custom tokenizer function\n",
    "    Returns\n",
    "    -------\n",
    "    tfs : sparse array,\n",
    "        tfidf vectors for documents. Each row corresponds to a document.\n",
    "    tfs_query: sparse array,\n",
    "        tfidf vectors for queries. Each row corresponds to a query.\n",
    "    dictionary: list\n",
    "        sorted dictionary\n",
    "    \"\"\"\n",
    "    model = str.maketrans(dict.fromkeys(string.punctuation))\n",
    "    processed_docs = [d.lower().translate(model) for d in docs]\n",
    "    processed_queries = [d.lower().translate(model) for d in queries]\n",
    "    tfidf = TfidfVectorizer(stop_words='english', tokenizer=tokenizer)\n",
    "    tfs = tfidf.fit_transform(processed_docs)\n",
    "    tfs_query = tfidf.transform(processed_queries)\n",
    "    return tfs, tfs_query, tfidf\n",
    "\n",
    "def tokenize_text(docs):\n",
    "    \"\"\"\n",
    "    custom tokenization function given a list of documents\n",
    "    Parameters\n",
    "        ----------\n",
    "        docs : string\n",
    "            a document\n",
    "    Returns\n",
    "    -------\n",
    "    stems : list\n",
    "        list of tokens\n",
    "    \"\"\"\n",
    "\n",
    "    text = ''\n",
    "    for d in docs:\n",
    "        text += '' + d\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stems = []\n",
    "    for item in tokens:\n",
    "        if is_word(item):\n",
    "            stems.append(stemmer.stem(item))\n",
    "    return stems\n",
    "\n",
    "def get_vectors(texts, queries):\n",
    "    descriptions = []\n",
    "    descriptions.append('')\n",
    "    vec_docs, vec_queries, tfidf_model = tf_idf(texts, queries, tokenize_text)\n",
    "    return vec_docs, vec_queries, tfidf_model\n",
    "\n",
    "def graph_of_thread(thread_id):\n",
    "    \n",
    "    graph = nx.DiGraph()\n",
    "    posts = thread_post_set_dic[thread_id]\n",
    "    \n",
    "    for each_post_id in posts:\n",
    "        graph.add_edge(each_post_id, thread_id)\n",
    "        comments = post_comment_set_dic[each_post_id]\n",
    "        for each_comment_id in comments:\n",
    "            graph.add_edge(each_comment_id, each_post_id)\n",
    "\n",
    "\n",
    "    return graph\n",
    "\n",
    "# Rubbish function\n",
    "'''\n",
    "def get_vectors_with_format_of_training_data(training_vectors, tfidf_training_model, testing_vectors, tfidf_testing_model):\n",
    "    \n",
    "    # word-index dictionaries\n",
    "    training_term_index_dic = tfidf_training_model.vocabulary_\n",
    "    testing_term_index_dic = tfidf_testing_model.vocabulary_\n",
    "    \n",
    "    # revert term dic\n",
    "    training_index_term_dic = {}\n",
    "    for term in training_term_index_dic:\n",
    "        index = training_term_index_dic[term]\n",
    "        training_index_term_dic[index] = term\n",
    "        \n",
    "    testing_index_term_dic = {}\n",
    "    for term in testing_term_index_dic:\n",
    "        index = testing_term_index_dic[term]\n",
    "        testing_index_term_dic[index] = term\n",
    "        \n",
    "    training_vectors_as_list = training_vectors.tolist()\n",
    "    combined_vector = training_vectors_as_list\n",
    "    # iterate each word in the training vector. If we find the word in the testing dataset, assign the value. Otherwise\n",
    "    # assign 0.\n",
    "    for testing_vector in testing_vectors:\n",
    "        current_vector_as_list = []\n",
    "        for training_index in range (0, training_vectors.shape[1]):\n",
    "            if training_index not in training_index_term_dic:\n",
    "                print(\"Fatal error!!!\")\n",
    "            else:\n",
    "                term = training_index_term_dic[training_index]\n",
    "                \n",
    "                if term in testing_term_index_dic:\n",
    "                    testing_index = testing_term_index_dic[term]\n",
    "                    current_value = testing_vector[testing_index]\n",
    "                    current_vector_as_list.append(current_value)\n",
    "                    \n",
    "                else:\n",
    "                    current_vector_as_list.append(0)\n",
    "\n",
    "        combined_vector.append(current_vector_as_list)\n",
    "        \n",
    "    return np.asarray(combined_vector)\n",
    "'''\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.57973867  0.81480247  0.        ]\n",
      " [ 0.6316672   0.44943642  0.          0.6316672 ]]\n",
      "{'love': 2, 'dog': 1, 'best': 0, 'world': 3}\n",
      "[[ 0.          0.          1.          0.        ]\n",
      " [ 0.          0.57973867  0.          0.81480247]]\n"
     ]
    }
   ],
   "source": [
    "# testing the new function \n",
    "\n",
    "text_a = 'I love my dog'\n",
    "text_b = 'My dog is the best in the world'\n",
    "text_c = 'I love my cat'\n",
    "text_d = 'The world cup is dogs'\n",
    "\n",
    "training_texts = []\n",
    "testing_texts = []\n",
    "\n",
    "training_texts.append(text_a)\n",
    "training_texts.append(text_b)\n",
    "testing_texts.append(text_c)\n",
    "testing_texts.append(text_d)\n",
    "\n",
    "training_vectors, testing_vectors, model = get_vectors(training_texts, testing_texts)\n",
    "\n",
    "training_vectors = training_vectors.toarray()\n",
    "testing_vectors = testing_vectors.toarray()\n",
    "\n",
    "print(training_vectors)\n",
    "print(model.vocabulary_)\n",
    "print(testing_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "please input the database name: courseraData.db\n"
     ]
    }
   ],
   "source": [
    "database = input(\"please input the database name: \")\n",
    "\n",
    "user_table = {}\n",
    "\n",
    "conn = util.create_connection(database)\n",
    "with conn:\n",
    "    cur = conn.cursor() \n",
    "    cur.execute(\"SELECT id, user_title FROM user\")\n",
    "    rows = cur.fetchall()\n",
    "    for each_user in rows:\n",
    "        user_id = each_user[0]\n",
    "        user_title = each_user[1]\n",
    "        user_table[user_id] = user_title\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# list with the same data length\n",
    "\n",
    "## texts\n",
    "texts = []\n",
    "training_texts = []\n",
    "testing_texts = []\n",
    "\n",
    "## ids\n",
    "ids = []\n",
    "training_ids = []\n",
    "testing_ids = []\n",
    "\n",
    "isreplied= []\n",
    "id_to_index = {}\n",
    "\n",
    "# for forum id alone\n",
    "forum_id_list = []\n",
    "\n",
    "# raw text dic\n",
    "thread_text_dic = {}\n",
    "post_text_dic = {}\n",
    "comment_text_dic = {}\n",
    "\n",
    "# container dic\n",
    "forum_type_dic = {}\n",
    "forum_graph_dic = {}\n",
    "forum_digraph_dic = {}\n",
    "thread_post_set_dic = {}\n",
    "post_comment_set_dic = {}\n",
    "thread_forum_dic = {}\n",
    "\n",
    "# feature dic\n",
    "num_of_post_dic = {}\n",
    "num_of_comment_dic = {}\n",
    "num_of_sen_dic = {}\n",
    "num_of_comment_for_post_dic = {}\n",
    "num_of_url_dic = {}\n",
    "num_of_timeref_dic = {}\n",
    "num_of_votes_dic = {}\n",
    "is_replied = {}\n",
    "starter_dic = {}\n",
    "\n",
    "# truncation\n",
    "thread_intervened_time_dic = {}\n",
    "thread_posted_time_dic = {}\n",
    "\n",
    "# problem thread\n",
    "replied_thread_without_reply_time = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "please input your course_id: -gcU5xn4EeWwrBKfKrqlSQ\n"
     ]
    }
   ],
   "source": [
    "course_id = input(\"please input your course_id: \")\n",
    "\n",
    "input_method = input(\"please press 'a' for manual, 'b' for semi-auto scan, others for auto: \")\n",
    "if input_method == 'a':\n",
    "    num_forum = input(\"please input the number of forums: \")\n",
    "    print(\"please input the forum ids in chronological order\")\n",
    "    for i in range(0, int(num_forum)):\n",
    "        new_forum_id = input(\"please input the current forum id: \")\n",
    "        forum_id_list.append(new_forum_id)\n",
    "        forum_type_dic[new_forum_id] = i\n",
    "        forum_graph_dic[new_forum_id] = get_graph(new_forum_id, database)\n",
    "        forum_digraph_dic[new_forum_id] = get_digraph(new_forum_id, database)\n",
    "\n",
    "elif input_method == 'b':\n",
    "    path = 'text/' + course_id + \"/\" + \"forum.txt\"\n",
    "    file = open(path, \"r\")\n",
    "    num_forum = int(file.readline())\n",
    "    num_forum = input(\"please input the number of forums: \")\n",
    "    for i in range (0, int(num_forum)):\n",
    "        new_forum_id = file.readline()\n",
    "        new_forum_id = new_forum_id.rstrip(os.linesep)\n",
    "        forum_id_list.append(new_forum_id)\n",
    "        forum_type_dic[new_forum_id] = i\n",
    "        forum_graph_dic[new_forum_id] = get_graph(new_forum_id, database)\n",
    "        forum_digraph_dic[new_forum_id] = get_digraph(new_forum_id, database)\n",
    "\n",
    "else:\n",
    "    path = 'text/' + course_id + \"/\" + \"forum.txt\"\n",
    "    file = open(path, \"r\")\n",
    "    num_forum = int(file.readline())\n",
    "    for i in range (0, num_forum):\n",
    "        new_forum_id = file.readline()\n",
    "        new_forum_id = new_forum_id.rstrip(os.linesep)\n",
    "        forum_id_list.append(new_forum_id)\n",
    "        forum_type_dic[new_forum_id] = i\n",
    "        forum_graph_dic[new_forum_id] = get_graph(new_forum_id, database)\n",
    "        forum_digraph_dic[new_forum_id] = get_digraph(new_forum_id, database)\n",
    "\n",
    "middleInd = int(input(\"please input the training set scale: \"))\n",
    "\n",
    "limit_message = \"WHERE courseid = \\'\"\n",
    "limit_message += course_id\n",
    "limit_message += \"\\'\"\n",
    "if int(num_forum) >= 1:\n",
    "    limit_message += \" AND (forumid = \\'\"\n",
    "    limit_message += forum_id_list[0]\n",
    "    limit_message += \"\\'\"\n",
    "\n",
    "for i in range(1, int(num_forum)):\n",
    "    limit_message += \" OR forumid = \\'\"\n",
    "    limit_message += forum_id_list[i]\n",
    "    limit_message += \"\\'\"\n",
    "\n",
    "if int(num_forum) >= 1:\n",
    "    limit_message += \")\"\n",
    "    \n",
    "print(\"done preprocessing!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract data from db.\n",
    "ids = []\n",
    "training_ids = []\n",
    "testing_ids = []\n",
    "\n",
    "conn = util.create_connection(database)\n",
    "with conn:\n",
    "    cur = conn.cursor()\n",
    "    thread_message = 'SELECT id, title, inst_replied, starter, forumid, posted_time FROM thread '\n",
    "    thread_message += limit_message\n",
    "    thread_message += ' ORDER BY posted_time'\n",
    "    print(thread_message)\n",
    "    cur.execute(thread_message)\n",
    "    threads = cur.fetchall()\n",
    "    for each_thread in threads:\n",
    "        \n",
    "        # All the info from this select operation\n",
    "        thread_id = each_thread[0]\n",
    "        thread_title = each_thread[1]\n",
    "        thread_intervention = each_thread[2]\n",
    "        thread_starter = each_thread[3]\n",
    "        thread_forum_id = each_thread[4]\n",
    "        thread_time = each_thread[5]\n",
    "        \n",
    "        # Initialize intervention time as -1\n",
    "        thread_intervened_time_dic[thread_id] = -1\n",
    "        if(thread_starter in user_table and (user_table[thread_starter] == 'Instructor' or user_table[thread_starter] == 'Staff')):           \n",
    "            continue\n",
    "\n",
    "        # Detect the earliest intervention.\n",
    "        if thread_intervention == 1:\n",
    "            post_message = \"SELECT user, post_time FROM post WHERE thread_id = \\\"\"\n",
    "            post_message += thread_id\n",
    "            post_message += \"\\\" ORDER BY post_time\"\n",
    "            cur.execute(post_message)\n",
    "            all_posts = cur.fetchall()\n",
    "            for each_post in all_posts:\n",
    "                poster = each_post[0]\n",
    "                post_time = each_post[1]\n",
    "                if poster in user_table and (user_table[poster] == 'Instructor' or user_table[poster] == 'Staff') and thread_intervened_time_dic[thread_id] == -1:\n",
    "                    thread_intervened_time_dic[thread_id] = post_time\n",
    "\n",
    "            comment_message = \"SELECT user, post_time FROM comment WHERE thread_id = \\\"\"\n",
    "            comment_message += thread_id\n",
    "            comment_message += \"\\\" ORDER BY post_time\"\n",
    "            cur.execute(comment_message)\n",
    "            all_comments = cur.fetchall()\n",
    "            for each_comment in all_comments:\n",
    "                commenter = each_comment[0]\n",
    "                comment_time = each_comment[1]\n",
    "                if commenter in user_table and (user_table[commenter] == 'Instructor' or user_table[commenter] == 'Staff'):\n",
    "                    if thread_intervened_time_dic[thread_id] == -1 or comment_time < thread_intervened_time_dic[thread_id]:\n",
    "\n",
    "                        thread_intervened_time_dic[thread_id] = comment_time\n",
    "                        \n",
    "            if thread_intervened_time_dic[thread_id] == -1:\n",
    "                replied_thread_without_reply_time.append(thread_id)\n",
    "        \n",
    "        # Memorize all the info in their respective dics\n",
    "        ids.append(thread_id)\n",
    "        thread_posted_time_dic[thread_id] = thread_time\n",
    "        thread_forum_dic[thread_id] = thread_forum_id\n",
    "        is_replied[thread_id] = thread_intervention\n",
    "        thread_post_set_dic[thread_id] = []\n",
    "        num_of_post_dic[thread_id] = 0\n",
    "        num_of_comment_dic[thread_id] = 0\n",
    "        num_of_votes_dic[thread_id] = 0\n",
    "        starter_dic[thread_id] = thread_starter\n",
    "        isreplied.append(thread_intervention)\n",
    "        thread_text_dic[thread_id] = thread_title\n",
    "\n",
    "    # extract all the posts\n",
    "    post_message = 'SELECT id, thread_id, votes, user, post_text, post_time FROM post '\n",
    "    post_message += limit_message\n",
    "    post_message += ' ORDER BY post_time'\n",
    "    cur.execute(post_message)\n",
    "    rows = cur.fetchall()\n",
    "    for each_post in rows:\n",
    "        \n",
    "        # All the info from this select operation\n",
    "        post_id = each_post[0]\n",
    "        post_thread_id = each_post[1]\n",
    "        post_vote = each_post[2]\n",
    "        poster = each_post[3]\n",
    "        post_text = each_post[4]\n",
    "        post_time = each_post[5]\n",
    "        \n",
    "        # If the home thread is intervened, continue.\n",
    "        if post_thread_id in is_replied and is_replied[post_thread_id] and thread_intervened_time_dic[post_thread_id] != -1 and post_time >= thread_intervened_time_dic[post_thread_id]:\n",
    "            continue\n",
    "        \n",
    "        # initialize all the features dic for posts\n",
    "        post_comment_set_dic[post_id] = []\n",
    "        num_of_comment_for_post_dic[post_id] = 0 \n",
    "        post_text_dic[post_id] = post_text\n",
    "\n",
    "        # if the thread is not initialized by the instructor, add thread level features\n",
    "        if post_thread_id in num_of_votes_dic:\n",
    "            num_of_votes_dic[post_thread_id] += post_vote\n",
    "            thread_post_set_dic[post_thread_id].append(post_id)\n",
    "            num_of_post_dic[post_thread_id] += 1\n",
    "\n",
    "\n",
    "    comment_message = 'SELECT id, thread_id, post_id, user, comment_text, post_time FROM comment '\n",
    "    comment_message += limit_message\n",
    "    comment_message += ' ORDER BY post_time'\n",
    "\n",
    "    cur.execute(comment_message)\n",
    "    print(comment_message)\n",
    "    rows = cur.fetchall()\n",
    "    for each_comment in rows:\n",
    "        \n",
    "        # All the info from this select operation\n",
    "        comment_id = each_comment[0]\n",
    "        comment_thread_id = each_comment[1]\n",
    "        comment_post_id = each_comment[2]\n",
    "        commenter = each_comment[3]\n",
    "        comment_text = each_comment[4]\n",
    "        comment_time = each_comment[5]\n",
    "\n",
    "        # if after intervention, truncate\n",
    "        if comment_thread_id in is_replied and is_replied[comment_thread_id] and thread_intervened_time_dic[comment_thread_id] != -1 and comment_time >= thread_intervened_time_dic[comment_thread_id]:\n",
    "            continue\n",
    " \n",
    "        # features\n",
    "        comment_text_dic[comment_id] = each_comment[4]\n",
    "\n",
    "        if comment_thread_id in num_of_comment_dic:\n",
    "            num_of_comment_dic[comment_thread_id] +=  1\n",
    "            \n",
    "        if comment_post_id in num_of_comment_for_post_dic:\n",
    "            num_of_comment_for_post_dic[comment_post_id] += 1\n",
    "            post_comment_set_dic[comment_post_id].append(comment_id)\n",
    "            \n",
    "print(len(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# construct the id to index dictionary and split the training ids and testing ids\n",
    "index = 0\n",
    "training_ids = []\n",
    "testing_ids = []\n",
    "\n",
    "# ensure add from scratch\n",
    "texts = []\n",
    "training_texts = []\n",
    "testing_texts = []\n",
    "\n",
    "for oneid in ids:\n",
    "    id_to_index[oneid] = index\n",
    "    if forum_type_dic[thread_forum_dic[oneid]] <= middleInd:\n",
    "        training_ids.append(oneid)\n",
    "    else:\n",
    "        testing_ids.append(oneid)\n",
    "\n",
    "    index += 1\n",
    "\n",
    "# process text\n",
    "for oneid in ids:\n",
    "    \n",
    "    filename = 'text/' + course_id + \"/\" + str(oneid) + '.txt'\n",
    "    f = open(filename, 'r', errors='ignore')\n",
    "    content = f.read()\n",
    "\n",
    "    num_url = content.count('a href')             \n",
    "    num_timeref = content.count('<TIMEREF>')\n",
    "    num_sen = content.count('.')\n",
    "    num_of_url_dic[oneid] = num_url\n",
    "    num_of_timeref_dic[oneid] = num_timeref\n",
    "    num_of_sen_dic[oneid] = num_sen\n",
    "\n",
    "    soup = BeautifulSoup(content, \"lxml\")\n",
    "\n",
    "    for tag in soup.find_all('code'):\n",
    "        tag.replaceWith('')\n",
    "\n",
    "    for tag in soup.find_all('a'):\n",
    "        tag.replaceWith('')\n",
    "\n",
    "    content = soup.get_text()\n",
    "    content = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', content, flags=re.MULTILINE)\n",
    "\n",
    "    if oneid in training_ids:\n",
    "        training_texts.append(content)\n",
    "    else:\n",
    "        testing_texts.append(content)\n",
    "    texts.append(content)\n",
    "\n",
    "# get the training vector and test vector\n",
    "training_vectors, testing_vectors, tfidf_model = get_vectors(training_texts, testing_texts)\n",
    "tmp_vectors = []\n",
    "current_training_index = 0\n",
    "current_testing_index = 0\n",
    "\n",
    "training_vectors = training_vectors.toarray()\n",
    "testing_vectors = testing_vectors.toarray()\n",
    "\n",
    "for oneid in ids:\n",
    "    if oneid in training_ids:\n",
    "        tmp_vectors.append(training_vectors[current_training_index])\n",
    "        current_training_index += 1\n",
    "    else:\n",
    "        tmp_vectors.append(testing_vectors[current_testing_index])\n",
    "        current_testing_index += 1\n",
    "\n",
    "if current_training_index != len(training_vectors) or current_testing_index != len(testing_vectors):\n",
    "    print(\"Fatal error!\")\n",
    "\n",
    "\n",
    "\n",
    "data_length = len(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      1.00      0.96       217\n",
      "          1       0.50      0.06      0.11        17\n",
      "\n",
      "avg / total       0.90      0.93      0.90       234\n",
      "\n",
      "did not extract\n",
      "52blABnqEeW9dA4X94-nLQ~ymvUqpzGEeW4ihIO4Pzp8Q\n",
      "4 and 5 slide pdfs missing\n",
      "\n",
      "Hi mates and professor,I just wanted to let you know that I can't find the pdfs of these slides.PS. My best wishes to you all in this course\n",
      "\n",
      "\n",
      "\n",
      "did not extract\n",
      "52blABnqEeW9dA4X94-nLQ~cynINp_1EeWlMBK9heOP5Q\n",
      "[Feedback] Missing \"angular.resource.min.js\" in Week 4 exercise (angular-resource) \n",
      "\n",
      "In the Week 4's \"Exercise (Instructions): Client-Server Communication using $resource\" , I think Instructor/staffs forgot to mention adding: into \"index.html\". Also none in the video. Thanks. \n",
      "\n",
      "\n",
      "\n",
      "did not extract\n",
      "52blABnqEeW9dA4X94-nLQ~1c7vMqGMEeWoGg6ulZMPEw\n",
      "why it doesn't tell me which test have failed???\n",
      "\n",
      "it just tells me that 3 tests have failed but i don't know which one \n",
      "\n",
      "\n",
      "\n",
      "did not extract\n",
      "52blABnqEeW9dA4X94-nLQ~icTDYKN2EeWoGg6ulZMPEw\n",
      "$resource vs $http\n",
      "\n",
      "Tendency is probably towards $http, so what are the benefits of using $resourcesto invoke REST services? (apart from from the REST verb methods)\n",
      "\n",
      "\n",
      "\n",
      "did not extract\n",
      "52blABnqEeW9dA4X94-nLQ~PE3q5aQBEeW2LwpPVHyhww\n",
      "Adding a comment - update dish\n",
      "\n",
      "Just a notice. The method shown by the teacher updates the entire dish structure (the dish itself and the comments)This is because when you add a comment and apply the update, you're actually doing an update on the dish and you see, through the developer console, the http request, you'll see the entire JSON data for the dish, similar to the one you see on the db.json.For the purpose of the demo is fine, but doing this on \"production\" could potentially destroy all the data on the server. For a typical anonymous user, the server would only allow you to post your comment and wouldn't allow you to touch the others ones.Again, this just a notice...\n",
      "\n",
      "\n",
      "\n",
      "did not extract\n",
      "52blABnqEeW9dA4X94-nLQ~bB_8aaZ4EeWacw7zR2f1aQ\n",
      "Week 4 : Directly use \"dist\" instead of copying to \"public\" (+proxying through browser-sync)\n",
      "\n",
      "As an alternate solution to the one proposed by Pr. Muppala, instead of creating a public directory and copying the content of the dist directory inside to be server by json-server, you might use the --static option.Here is the command I use, typed in a terminal from the root directory of the project:--watch server/db.json : use the db.json file stored in the conFusion/server/ sub-deirectory--static dist : serve static file from conFusion/dist/--port 3100 : use port 3100 (see below why)----In addition, you can even use browser-sync on top of json-server. While that later is running, start a new terminal, and in the project root directory use:Now, if you point your browser to http://localhost:3000 you will connect to browser-sync that will forward your request to json-server. The key advantage with this setup is, when you change the content of a static file (say, index.html) -- the page will be automagically reloaded on your browser. Even if the \"real\" server is json-server !(I let the gulp integration as an exercise to the reader ;)\n",
      "Great! One can also make a nice shell script from those commands to make it even easier to run!\n",
      "You can also just modify the gulpfile to also pipe the output to the 'public' folder.\n",
      "Another possibility is to add some more tasks to gulp to automatically copy all the files in the dist folder to the public directory. I got this to work but it took me a while to figure out that I had to have my copy task run after all the other tasks had already completed (I thought I had originally done that but apparently not).\n",
      "\n",
      "\n",
      "\n",
      "did not extract\n",
      "52blABnqEeW9dA4X94-nLQ~BXlYwKehEeW2NBLvq9LZHQ\n",
      "Week 4: Exercice \"Handling Erros in Client-Server Communication...\"  Why assign the resource twice?\n",
      "\n",
      "in the line 1, we assign the resource to the $scope.dish,  in the line 5, we re-assign the resource to it(if success). i've tried to delete the [ $scope.dish= ]  in the line 1, and it made no difference. So is there some reason else i didn't get here? \n",
      "I have been thinking about this as well! Following.\n",
      "well actually, it's useless to assigné it twice.I asked myself the Same question\n",
      "\n",
      "\n",
      "\n",
      "did not extract\n",
      "52blABnqEeW9dA4X94-nLQ~qfh1mKfAEeWiXxLB9mtqCw\n",
      "Http response code:301 and 404\n",
      "\n",
      "How does server decide whether 301 and 404 should be sent as response because in both cases, resource is not found ?\n",
      "\n",
      "\n",
      "\n",
      "did not extract\n",
      "52blABnqEeW9dA4X94-nLQ~0q7q3qfGEeWRTArgYkB6yQ\n",
      "Error: -1\n",
      "\n",
      "hi I'm stopping the json-serverI get Error: -1 instead of the expected 404 when the json-server is down.I debug in the browser and I break inbut I cannot spot the 404 even in the Network browser tools console, I can not see the 404 response.any idea why I'm getting this -1 ?Regards,\n",
      "\n",
      "\n",
      "\n",
      "did not extract\n",
      "52blABnqEeW9dA4X94-nLQ~aHhlkqp-EeWoGg6ulZMPEw\n",
      "Yo must be joking.\n",
      "\n",
      "Ran \"yo angular.\" After waiting five minutes for it to install several hundred Node packages to \"scaffold\" a project, I was curious and ran du -h -s.TWO HUNDRED MEGABYTES? Just for the scaffolding for ONE project?This \"utility\" is a joke.\n",
      "\n",
      "\n",
      "\n",
      "did not extract\n",
      "52blABnqEeW9dA4X94-nLQ~AsffP6z7EeWF6gpqp4BTmQ\n",
      "Unfortunate ambiguity with 'update' in assignment.\n",
      "\n",
      "This assignment doesn't require the optional {'update':{method:'PUT' }} to be configured in the services file for promotions or leadership because information is just fetched, however the tasks mention updating information on the web pages as though the data could change. So are we supposed to include the include the {'update':{method:'PUT'}} in the configuration, or will we be penalized because it wasn't necessary?Thanks to anyone who can help!\n",
      "\n",
      "\n",
      "\n",
      "did not extract\n",
      "52blABnqEeW9dA4X94-nLQ~3yIj5K3EEeWF2Q53QdZUbw\n",
      "Does Week 4 Exercise and Assignment require Gulp\n",
      "\n",
      "HiI just completed Week 3 and am starting Week 4. I had problems with Gulp on my Windows Machine and used Grunt up to now. My Question:Can Week 4 be completed using Grunt or do I need to find a solution for using Gulp?ThanksSean \n",
      "  A lot of us are in the same boat. Grunt seems to work fine however a few of us are having problems due to the course build-up using Gulp so far. See some of the other threads here.  \n",
      "\n",
      "\n",
      "\n",
      "did not extract\n",
      "52blABnqEeW9dA4X94-nLQ~iap1h7FeEeW2NBLvq9LZHQ\n",
      "Is it possible to extend the deadline for the Assignment 4?\n",
      "\n",
      "Many of us were traveling to be with our beloved families this holiday season. I believe that many find themselves in this same situation.\n",
      "I hope you had a happy holiday season.The assignment deadlines are  actually flexible. If you look in the Coursera help it states that for courses on the new platform (which this is) then there is no penalty for missing a deadline. The only problem is that there is no guarantee that there will be any peers around to mark it for you.\n",
      "I'm in the same boat, I've travelled for the holidays and it was difficult finishing the angular assignments in the gaps. I'll be driving all day Sunday, and as it is right now, I'll likely won't finish before Monday...\n",
      "\n",
      "\n",
      "\n",
      "did not extract\n",
      "52blABnqEeW9dA4X94-nLQ~inQimrR4EeW2LwpPVHyhww\n",
      "Can you use ng-href and not ui-sref\n",
      "\n",
      "Have got everything working on Json-server, index and menu but not the link in photos, would like to use ng-href as that was working and not ui-sref but when moving to '/menu/:id' page I get a $stateParams is not defined error and in the dishdetail.html page just looks like it's missing the right id:{{NUMBER}}.But the id changes as i move over each image . Well if anybody is still working on this then it would be great , but if not hope everyone has a good 2016 .\n",
      "If you want to use the router (and you do) you need to use ui-sref. Make sure the router code and .js files are correct.\n",
      "If you want to use the router (and you do) you need to use ui-sref. Make sure the router code and .js files are correct.\n",
      "Thanks Tp , have used <a ui-sref=\"app.dishdetails({id: dish.id})\"> in menu.html shows nothing at the bottom left of the screen when I move over photos. And have this set up in app.js , I guessing the problem is in the controllers.js but IndexController.js is working as pulling in Dish image, and MenueController is ok for Dishes as this pulls in all the DishesSo I guess the problem is here in this part of the MenuController or am in the wrong area all together , well thanks once again for answering .\n",
      "I think found what is wrong just in case thought I would put it here, I was using this in app.js when I think it should not be using ngRoute but ['ui.router'] I am not sure why but will go back and see the video on UI-Router again.\n",
      "\n",
      "\n",
      "\n",
      "did not extract\n",
      "52blABnqEeW9dA4X94-nLQ~eLdO-LYMEeWjcBKYJq1ZMQ\n",
      "Hi Jogesh, App is perfect running on virtual server, but ...\n",
      "\n",
      "I made a trial and transfered our, I mean your (Jogesh) Webapplication, to my virtual server (with a standard apache webserver) and it is running, I tried entering some comments and they are stored into the db.json.It's so perfect, what do you teached us.But now, what is with security for such an easy application? I mean following: The app is at  but I (and so everybody) can see all content in the Json-File:  This is not good :)Do we learn in the the 5th course, \"Server-side Development with NodeJS\", all about security? There is only a little notice: \"We touch upon authentication and security.\"If not, where do I find a starting-point (i.e. a tutorial) learning about making it safe?\n",
      "\n",
      "\n",
      "\n",
      "did not extract\n",
      "52blABnqEeW9dA4X94-nLQ~NMxvK7deEeWjxw7W9fJX5Q\n",
      "My first Android app , Thanks to Prof Muppala\n",
      "\n",
      "After completing this course, I started exploring Ionic (which will be taught in next course this Specialization) , I made a simple app with the knowledge that I have gained so far from this Specialization.Here is the play Store link :Please give your valuable feedback and rate the app.You can see the power of  \" List \" in the app.Thanks to Alberto for teaching us this Specialization, you know what I mean.\n",
      "\n",
      "\n",
      "\n",
      "rubbish extracted\n",
      "52blABnqEeW9dA4X94-nLQ~jU2haYiNEearVhJfoMXTAQ\n",
      "test cases failed with Error: Unexpected request: GET views/header.html\n",
      "\n",
      "Running test cases failing with multiple similar error, complaining about Unexpected request: GET views/header.html:Do you have any ideas how to fix that?\n",
      "I have found the solution, but it is kind of large.The problem is with the ui-router. See Stack Overflow discussion: What I've done to fix my confusionApp unit testing:I created conFusion\\test\\mock folder;Inside new folder I created stateMock.js file;Inside this file I placed the (as in URL above);In conFusion\\test\\karma.conf.js I added 'test/mock/stateMock.js' to the files;In conFusion\\test\\unit\\controllers\\menucontroller.js file in the begining I made following chages (see the ui-router comments):Again, I am not sure if it is correct way, because I am new to JavaScript, but it works for me!\n",
      "This is doen't work for me also(((\n",
      "Adding the following code to menucontroller.js fixed this for me:For reference here is my full file code:\n",
      "Thanks a lot Alexsander, It worked for me. \n",
      "Encountered the same issue.  Was able to fix it by adding the following to menucontroller.js test file:\n",
      "Thank you for the solution, Jason.\n",
      "You saved me!! Thank you!!\n",
      "Thanks, great workaround. :-)\n",
      "Thanks Jason!\n",
      "Thanks Jason!\n",
      "thanks a lot\n",
      "This solution doesn't work for me... oh my god, so many errors in this course and lessons... it makes me mad(Can not it be done with more quality?(\n",
      "Thank you very much! It works!!!\n",
      "Thanks Jason and Satya because i need both solutions!!!!\n",
      "Thanks Jason! It worked.I also, had to use the \"if (angular.element.cleanData) angular.element.cleanData(cleanUpNodes);\" to fix my errors. Thanks Olga and Satya.\n",
      "Nice Solution!!\n",
      "Thanks for noting this problem and offering the solution. \n",
      "It works but I don't know why. Could you please explain a little how you figure out this solution? Thanks!\n",
      "Thanks Jason, it worked for me!\n",
      "Worked for me also\n",
      "I tried those two solutions and others i found online.What solved the problem to me was just include the jQuery on the karma.conf.js files      'bower_components/jquery/dist/jquery.js'\n",
      "Thanks Alexander for your detailed answer, it works for me!\n",
      "This didnt work for me.. when i added that code, I got a new error :TypeError: undefined is not a constructor (evaluating 'angular.element.cleanData(cleanUpNodes)') in bower_components/angular-mocks/angular-mocks.js (line 2922)Any help is appreciated! Thanks.\n",
      "But replacing angular.element.cleanData(cleanUpNodes);with if (angular.element.cleanData) angular.element.cleanData(cleanUpNodes);in angular-mocks.js worked!\n",
      "They discuss another error, that is not solved by substitution in angular-mocks.js \n",
      "  Thanks Jason and Satya because i need both solutions!!!!  \n",
      "Thank you Jason, it worked for me too.\n",
      "Thank you Sataya!\n",
      "Jasons and Satyas Solution worked for me. In order for others to avoid searching around and wasting hours I summarize them here:Change your menucontroller.js to the following:2. in /bower_components/angular-mocks.js change to\n",
      "Hello, I had the same problem and I managed to resolve updating the \"bower update\" and selecting the most current version of angularjs 1.6.0 and added your code of item 1 only.Thanks.\n",
      "I had a completely different error, but I applied what you suggested and it works now, thank you!\n",
      "I tried your solution and it worked for me. Thanks Alexander!\n",
      "This solution worked for me 2...thanks Alexander, excellent wok! \n",
      "Thank you very much!\n",
      "Big thank you! This is very helpful and I have solved this problem!\n",
      "It solved my problem too! thanks a lot\n",
      "This solved my problem as well! Thanks Alexander. Off to the next exercise!\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_vectors = []\n",
    "test_vectors = []\n",
    "training_result = []\n",
    "test_result = []\n",
    "\n",
    "# test id\n",
    "x = np.asarray(tmp_vectors)\n",
    "\n",
    "# EDM 15\n",
    "additional_features= []\n",
    "\n",
    "for oneid in ids:\n",
    "    index = id_to_index[oneid]\n",
    "    origin_list = x[index].tolist()\n",
    "    li = []\n",
    "    forum_feature = forum_type_dic[thread_forum_dic[oneid]]\n",
    "    li.append(forum_feature)\n",
    "    numpost = num_of_post_dic[oneid]\n",
    "    li.append(numpost)\n",
    "    numcomment = num_of_comment_dic[oneid]\n",
    "    li.append(numcomment)\n",
    "    li.append(numpost + numcomment)\n",
    "    # will add the average # of comments per post\n",
    "    summ = 0\n",
    "    for postid in thread_post_set_dic[oneid]:\n",
    "        summ = summ + num_of_comment_for_post_dic[postid]\n",
    "     \n",
    "    if len(thread_post_set_dic[oneid]) == 0:\n",
    "        avr = 0\n",
    "    else:\n",
    "        avr = float(summ / len(thread_post_set_dic[oneid]))\n",
    "\n",
    "    li.append(avr)\n",
    "    numurl = num_of_url_dic[oneid]\n",
    "    li.append(numurl)\n",
    "    numsen = num_of_sen_dic[oneid]\n",
    "    li.append(numsen)\n",
    "    numvotes = num_of_votes_dic[oneid]\n",
    "    li.append(numvotes)\n",
    "\n",
    "    numtimeref = num_of_timeref_dic[oneid]\n",
    "    new_list = origin_list + li\n",
    "    tmp_vectors.append(new_list)\n",
    "\n",
    "    if forum_type_dic[thread_forum_dic[oneid]] <= middleInd:\n",
    "        training_vectors.append(new_list)\n",
    "        training_result.append(isreplied[index])\n",
    "    else:\n",
    "        test_vectors.append(new_list)\n",
    "        test_result.append(isreplied[index])\n",
    "\n",
    "\n",
    "x = np.asarray(tmp_vectors)\n",
    "y = np.asarray(isreplied)\n",
    "\n",
    "\n",
    "\n",
    "LogReg = LogisticRegression(class_weight = 'balanced')\n",
    "LogReg.fit(training_vectors, training_result)\n",
    "pred_result = LogReg.predict(test_vectors)\n",
    "with open('EDM.txt', 'w') as f:\n",
    "    print(classification_report(test_result, pred_result), file = f)\n",
    "print(classification_report(test_result, pred_result))\n",
    "\n",
    "for i in range(len(test_result)):\n",
    "    if test_result[i] == 1 and pred_result[i] == 0:\n",
    "        print('did not extract')\n",
    "        print(testing_ids[i])\n",
    "        print(texts[id_to_index[testing_ids[i]]])\n",
    "        print('\\n')\n",
    "    elif test_result[i] == 0 and pred_result[i] == 1:\n",
    "        print('rubbish extracted')\n",
    "        print(testing_ids[i])\n",
    "        print(texts[id_to_index[testing_ids[i]]])\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['52blABnqEeW9dA4X94-nLQ~xZtmASZVEealrBKadnqOlw', '52blABnqEeW9dA4X94-nLQ~o6Vo_imOEeaCnQ4uxkc-rQ']\n"
     ]
    }
   ],
   "source": [
    "# problematic threads\n",
    "print(replied_thread_without_reply_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52blABnqEeW9dA4X94-nLQ~LsrihJ0KEeWDQxJNxtbtRw\n",
      "52blABnqEeW9dA4X94-nLQ~iBuE0Z0fEeWDQxJNxtbtRw\n",
      "52blABnqEeW9dA4X94-nLQ~iPU0Yp3EEeWGxBJdkUHhbw\n",
      "52blABnqEeW9dA4X94-nLQ~c12CIJ3ZEeWIyQ6OVXax3w\n",
      "52blABnqEeW9dA4X94-nLQ~Is5qlZ3mEeWGxBJdkUHhbw\n",
      "52blABnqEeW9dA4X94-nLQ~h7NmyJ4DEeWuXAoPCBK-bw\n",
      "52blABnqEeW9dA4X94-nLQ~Mefab54YEeWwaw5NDAWlew\n",
      "52blABnqEeW9dA4X94-nLQ~YSZJ6p6LEeWGxBJdkUHhbw\n",
      "52blABnqEeW9dA4X94-nLQ~gnkgM566EeWGxBJdkUHhbw\n",
      "52blABnqEeW9dA4X94-nLQ~VKyT4p7qEeWILRIOm1V0SQ\n",
      "52blABnqEeW9dA4X94-nLQ~SlMzSp8eEeWOxhKdn19VDQ\n",
      "52blABnqEeW9dA4X94-nLQ~8NVj958eEeW1EBLvQzgrNw\n",
      "52blABnqEeW9dA4X94-nLQ~rrrSZZ90EeWlMBK9heOP5Q\n",
      "52blABnqEeW9dA4X94-nLQ~Lt_i-p92EeWz5w6C8C5zDQ\n",
      "52blABnqEeW9dA4X94-nLQ~H2RAQp-iEeWz5w6C8C5zDQ\n",
      "52blABnqEeW9dA4X94-nLQ~wiLD-J_FEeW1EBLvQzgrNw\n",
      "52blABnqEeW9dA4X94-nLQ~Gi4AC5_-EeWdtBKD_KfXqQ\n",
      "52blABnqEeW9dA4X94-nLQ~qnEvHKBFEeWiXxLB9mtqCw\n",
      "52blABnqEeW9dA4X94-nLQ~HWoOWKDmEeWMNQ43fvJXFw\n",
      "52blABnqEeW9dA4X94-nLQ~Xc0uiaDpEeWfHwqjLjBT9w\n",
      "52blABnqEeW9dA4X94-nLQ~Z33F5KFBEeW2dA51rBQ9Ew\n",
      "52blABnqEeW9dA4X94-nLQ~-WlOBaGiEeWjcBKYJq1ZMQ\n",
      "52blABnqEeW9dA4X94-nLQ~lOyYFaGkEeWiXxLB9mtqCw\n",
      "52blABnqEeW9dA4X94-nLQ~rWCY3qGmEeWF6gpqp4BTmQ\n",
      "52blABnqEeW9dA4X94-nLQ~ZcFoe6GzEeWfHwqjLjBT9w\n",
      "52blABnqEeW9dA4X94-nLQ~VbQ9X6HGEeWMNQ43fvJXFw\n",
      "52blABnqEeW9dA4X94-nLQ~fQyeV6HLEeW2LwpPVHyhww\n",
      "52blABnqEeW9dA4X94-nLQ~Z8wsWaHjEeW2NBLvq9LZHQ\n",
      "52blABnqEeW9dA4X94-nLQ~PCbJc6J7EeWiXxLB9mtqCw\n",
      "52blABnqEeW9dA4X94-nLQ~vO_WXaKGEeWRTArgYkB6yQ\n",
      "52blABnqEeW9dA4X94-nLQ~UJNdEqNkEeWG0xJGD15hdw\n",
      "52blABnqEeW9dA4X94-nLQ~q9SAFKOLEeWMNQ43fvJXFw\n",
      "52blABnqEeW9dA4X94-nLQ~b0ojgqOhEeW2LwpPVHyhww\n",
      "52blABnqEeW9dA4X94-nLQ~E3p4VKOmEeWacw7zR2f1aQ\n",
      "52blABnqEeW9dA4X94-nLQ~_Yi6GaPxEeW2dA51rBQ9Ew\n",
      "52blABnqEeW9dA4X94-nLQ~8RDXfqQREeWjcBKYJq1ZMQ\n",
      "52blABnqEeW9dA4X94-nLQ~uJCWWKQVEeWhLRIkesxXNw\n",
      "52blABnqEeW9dA4X94-nLQ~CiIEOqRIEeWMNQ43fvJXFw\n",
      "52blABnqEeW9dA4X94-nLQ~JXBOTaS5EeW2LwpPVHyhww\n",
      "52blABnqEeW9dA4X94-nLQ~FqjFt6TqEeW2dA51rBQ9Ew\n",
      "52blABnqEeW9dA4X94-nLQ~zecFUaTqEeWiXxLB9mtqCw\n",
      "52blABnqEeW9dA4X94-nLQ~QrHlkaUREeWoGg6ulZMPEw\n",
      "52blABnqEeW9dA4X94-nLQ~pgcrNKWmEeW2LwpPVHyhww\n",
      "52blABnqEeW9dA4X94-nLQ~r5gbw6WrEeWRTArgYkB6yQ\n",
      "52blABnqEeW9dA4X94-nLQ~Hv4K4qW_EeWjcBKYJq1ZMQ\n",
      "52blABnqEeW9dA4X94-nLQ~3JRTtaYvEeWfHwqjLjBT9w\n",
      "52blABnqEeW9dA4X94-nLQ~zgl1BKZXEeWHMgod_NjFNw\n",
      "52blABnqEeW9dA4X94-nLQ~lhIwK6aKEeWRTArgYkB6yQ\n",
      "52blABnqEeW9dA4X94-nLQ~wK1mOqaXEeWMNQ43fvJXFw\n",
      "52blABnqEeW9dA4X94-nLQ~XCCI3KbcEeWF2Q53QdZUbw\n",
      "52blABnqEeW9dA4X94-nLQ~rAUIP6cZEeWHMgod_NjFNw\n",
      "52blABnqEeW9dA4X94-nLQ~KGAbiKcfEeWoGg6ulZMPEw\n",
      "52blABnqEeW9dA4X94-nLQ~vybbiqc4EeWacw7zR2f1aQ\n",
      "52blABnqEeW9dA4X94-nLQ~BLDv4KdDEeW2NBLvq9LZHQ\n",
      "52blABnqEeW9dA4X94-nLQ~Dtv9Lae3EeWacw7zR2f1aQ\n",
      "52blABnqEeW9dA4X94-nLQ~E5aE7qfHEeWG0xJGD15hdw\n",
      "52blABnqEeW9dA4X94-nLQ~mxVy1qgdEeWfHwqjLjBT9w\n",
      "52blABnqEeW9dA4X94-nLQ~bShyE6gtEeWRTArgYkB6yQ\n",
      "52blABnqEeW9dA4X94-nLQ~fV4Bkah5EeW2LwpPVHyhww\n",
      "52blABnqEeW9dA4X94-nLQ~yrPUB6ieEeWhLRIkesxXNw\n",
      "52blABnqEeW9dA4X94-nLQ~SMT2vai2EeWhLRIkesxXNw\n",
      "52blABnqEeW9dA4X94-nLQ~7hqGnKi5EeWF6gpqp4BTmQ\n",
      "52blABnqEeW9dA4X94-nLQ~FVqsn6jhEeWRTArgYkB6yQ\n",
      "52blABnqEeW9dA4X94-nLQ~lMkN9qj7EeWG0xJGD15hdw\n",
      "52blABnqEeW9dA4X94-nLQ~aoVYPqkREeWhLRIkesxXNw\n",
      "52blABnqEeW9dA4X94-nLQ~efc26Kk6EeW2LwpPVHyhww\n",
      "52blABnqEeW9dA4X94-nLQ~44O-K6mDEeWiXxLB9mtqCw\n",
      "52blABnqEeW9dA4X94-nLQ~G8pUPqmEEeWF6gpqp4BTmQ\n",
      "52blABnqEeW9dA4X94-nLQ~xrY5xamvEeWF2Q53QdZUbw\n",
      "52blABnqEeW9dA4X94-nLQ~38kRbKnWEeW2NBLvq9LZHQ\n",
      "52blABnqEeW9dA4X94-nLQ~JMK47qn5EeWfHwqjLjBT9w\n",
      "52blABnqEeW9dA4X94-nLQ~hlT9uKoVEeWMNQ43fvJXFw\n",
      "52blABnqEeW9dA4X94-nLQ~tlfnE6r9EeWHMgod_NjFNw\n",
      "52blABnqEeW9dA4X94-nLQ~zcOSNKuOEeWF6gpqp4BTmQ\n",
      "52blABnqEeW9dA4X94-nLQ~pNlBfqwKEeWF6gpqp4BTmQ\n",
      "52blABnqEeW9dA4X94-nLQ~H2tRoaxWEeWMNQ43fvJXFw\n",
      "52blABnqEeW9dA4X94-nLQ~DdEHlKyXEeWoGg6ulZMPEw\n",
      "52blABnqEeW9dA4X94-nLQ~_Axs6ayXEeWfHwqjLjBT9w\n",
      "52blABnqEeW9dA4X94-nLQ~o6EkhqyrEeWG0xJGD15hdw\n",
      "52blABnqEeW9dA4X94-nLQ~V7yKiay1EeWF6gpqp4BTmQ\n",
      "52blABnqEeW9dA4X94-nLQ~1AqV3KzLEeWiXxLB9mtqCw\n",
      "52blABnqEeW9dA4X94-nLQ~iq2c5K0KEeW2LwpPVHyhww\n",
      "52blABnqEeW9dA4X94-nLQ~U0wB8q0qEeWRTArgYkB6yQ\n",
      "52blABnqEeW9dA4X94-nLQ~8ud4H62bEeW2dA51rBQ9Ew\n",
      "52blABnqEeW9dA4X94-nLQ~Nz-A4a2cEeWhLRIkesxXNw\n",
      "52blABnqEeW9dA4X94-nLQ~rowZsK34EeWRTArgYkB6yQ\n",
      "52blABnqEeW9dA4X94-nLQ~HYDZpK3-EeWiXxLB9mtqCw\n",
      "52blABnqEeW9dA4X94-nLQ~UgScI64YEeWRTArgYkB6yQ\n",
      "52blABnqEeW9dA4X94-nLQ~L46O9a4bEeWjcBKYJq1ZMQ\n",
      "52blABnqEeW9dA4X94-nLQ~6o59Mq5jEeWjcBKYJq1ZMQ\n",
      "52blABnqEeW9dA4X94-nLQ~zQZzYq5lEeWiXxLB9mtqCw\n",
      "52blABnqEeW9dA4X94-nLQ~kWG2Cq6YEeWG0xJGD15hdw\n",
      "52blABnqEeW9dA4X94-nLQ~uPYqIq78EeWjcBKYJq1ZMQ\n",
      "52blABnqEeW9dA4X94-nLQ~Avjs5a8VEeWHMgod_NjFNw\n",
      "52blABnqEeW9dA4X94-nLQ~nj7FI697EeW2LwpPVHyhww\n",
      "52blABnqEeW9dA4X94-nLQ~dAsF3q-8EeWacw7zR2f1aQ\n",
      "52blABnqEeW9dA4X94-nLQ~DQ1WSbAuEeWiXxLB9mtqCw\n",
      "52blABnqEeW9dA4X94-nLQ~nrldqbDcEeWoGg6ulZMPEw\n",
      "52blABnqEeW9dA4X94-nLQ~hEP3mrD5EeWMNQ43fvJXFw\n",
      "52blABnqEeW9dA4X94-nLQ~5uLA8bEjEeWacw7zR2f1aQ\n",
      "52blABnqEeW9dA4X94-nLQ~aje2ILE_EeWRTArgYkB6yQ\n",
      "52blABnqEeW9dA4X94-nLQ~AaGmRrFxEeWiXxLB9mtqCw\n",
      "52blABnqEeW9dA4X94-nLQ~zloJ1rIWEeWhLRIkesxXNw\n",
      "52blABnqEeW9dA4X94-nLQ~jL1fZLKKEeWMNQ43fvJXFw\n",
      "52blABnqEeW9dA4X94-nLQ~IAF8T7LTEeWRTArgYkB6yQ\n",
      "52blABnqEeW9dA4X94-nLQ~TYHWObWUEeWF6gpqp4BTmQ\n",
      "52blABnqEeW9dA4X94-nLQ~jU2haYiNEearVhJfoMXTAQ\n",
      "52blABnqEeW9dA4X94-nLQ~M6JKJ6VMEeaMbxJjq5r6sA\n",
      "p q r s:\n",
      "3\n",
      "105\n",
      "138\n",
      "534\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.78      0.86       217\n",
      "          1       0.14      0.47      0.22        17\n",
      "\n",
      "avg / total       0.89      0.76      0.81       234\n",
      "\n",
      "[[-0.00856925 -0.06243108 -0.09142388 ..., -0.03470893 -0.02926799\n",
      "  -1.55662795]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.82      0.87       217\n",
      "          1       0.13      0.35      0.19        17\n",
      "\n",
      "avg / total       0.88      0.78      0.82       234\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_vectors_enhanced = []\n",
    "test_vectors_enhanced = []\n",
    "tmp_vectors = []\n",
    "has_gratitude_list = []\n",
    "\n",
    "p = 0\n",
    "q = 0\n",
    "r = 0\n",
    "s = 0\n",
    "    \n",
    "for index in range(data_length):\n",
    "    li = []\n",
    "    oneid = ids[index]\n",
    "    origin_list = x[index].tolist()\n",
    "    post_list = thread_post_set_dic[oneid]\n",
    "    starting_post_text = preprocess_text(post_text_dic[post_list[0]])\n",
    "    \n",
    "    last_post_index = len(post_list) - 1\n",
    "    \n",
    "    has_key_post = False\n",
    "    for i in range(0, 3):\n",
    "        current_post_index = last_post_index - i\n",
    "        if current_post_index > 0:\n",
    "            current_post_id = post_list[current_post_index]\n",
    "            current_post_text = post_text_dic[current_post_id]\n",
    "            cleanr = re.compile('<.*?>')\n",
    "            current_post_text = re.sub(cleanr, '', current_post_text)\n",
    "            if len(current_post_text.split()) <= 15 and ('thank' in current_post_text or 'Thank' in current_post_text):\n",
    "                li.append(1)\n",
    "                has_key_post = True\n",
    "                break\n",
    "                \n",
    "            comment_list = post_comment_set_dic[current_post_id]\n",
    "            contribution = False\n",
    "            for current_comment_id in comment_list:\n",
    "                current_comment_text = comment_text_dic[current_comment_id]\n",
    "                if len(current_comment_text.split()) <= 15 and ('thank' in current_comment_text or 'Thank' in current_comment_text):\n",
    "                    contribution = True\n",
    "                    \n",
    "            if contribution:\n",
    "                li.append(1)\n",
    "                has_key_post = True\n",
    "                break\n",
    "    \n",
    "    if not has_key_post:\n",
    "        li.append(0)\n",
    "    else:\n",
    "        print(oneid)\n",
    "\n",
    "    \n",
    "    if has_key_post and is_replied[oneid] == 1:\n",
    "        p += 1\n",
    "        \n",
    "    elif has_key_post and is_replied[oneid] == 0:\n",
    "        q += 1\n",
    "    \n",
    "    elif not has_key_post and is_replied[oneid] == 1:\n",
    "        r += 1\n",
    "        \n",
    "    elif not has_key_post and is_replied[oneid] == 0:\n",
    "        s += 1\n",
    "\n",
    "\n",
    "    new_list = origin_list + li\n",
    "\n",
    "    tmp_vectors.append(new_list)\n",
    "    if forum_type_dic[thread_forum_dic[oneid]] <= middleInd:\n",
    "        training_vectors_enhanced.append(new_list)\n",
    "\n",
    "    else:\n",
    "        test_vectors_enhanced.append(new_list)\n",
    "\n",
    "print(\"p q r s:\")\n",
    "print(p)\n",
    "print(q)\n",
    "print(r)\n",
    "print(s)\n",
    "    \n",
    "    \n",
    "LogReg = LogisticRegression(class_weight = 'balanced')\n",
    "LogReg.fit(training_vectors_enhanced, training_result)\n",
    "pred_result_enhanced = LogReg.predict(test_vectors_enhanced)\n",
    "with open('EDM.txt', 'w') as f:\n",
    "    print(classification_report(test_result, pred_result_enhanced), file = f)\n",
    "print(classification_report(test_result, pred_result_enhanced))\n",
    "print(LogReg.coef_)\n",
    "\n",
    "novel_result = []\n",
    "\n",
    "for i in range(0, len(test_result)):\n",
    "    if test_vectors[i][len(test_vectors[0]) - 1] == 1:\n",
    "        novel_result.append(0)\n",
    "    else:\n",
    "        novel_result.append(pred_result_enhanced[i])\n",
    "        \n",
    "print(classification_report(test_result, novel_result))\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "did not extract\n",
      "52blABnqEeW9dA4X94-nLQ~ymvUqpzGEeW4ihIO4Pzp8Q\n",
      "4 and 5 slide pdfs missing\n",
      "\n",
      "Hi mates and professor,I just wanted to let you know that I can't find the pdfs of these slides.PS. My best wishes to you all in this course\n",
      "\n",
      "\n",
      "\n",
      "did not extract\n",
      "52blABnqEeW9dA4X94-nLQ~cynINp_1EeWlMBK9heOP5Q\n",
      "[Feedback] Missing \"angular.resource.min.js\" in Week 4 exercise (angular-resource) \n",
      "\n",
      "In the Week 4's \"Exercise (Instructions): Client-Server Communication using $resource\" , I think Instructor/staffs forgot to mention adding: into \"index.html\". Also none in the video. Thanks. \n",
      "\n",
      "\n",
      "\n",
      "did not extract\n",
      "52blABnqEeW9dA4X94-nLQ~1c7vMqGMEeWoGg6ulZMPEw\n",
      "why it doesn't tell me which test have failed???\n",
      "\n",
      "it just tells me that 3 tests have failed but i don't know which one \n",
      "\n",
      "\n",
      "\n",
      "did not extract\n",
      "52blABnqEeW9dA4X94-nLQ~icTDYKN2EeWoGg6ulZMPEw\n",
      "$resource vs $http\n",
      "\n",
      "Tendency is probably towards $http, so what are the benefits of using $resourcesto invoke REST services? (apart from from the REST verb methods)\n",
      "\n",
      "\n",
      "\n",
      "did not extract\n",
      "52blABnqEeW9dA4X94-nLQ~PE3q5aQBEeW2LwpPVHyhww\n",
      "Adding a comment - update dish\n",
      "\n",
      "Just a notice. The method shown by the teacher updates the entire dish structure (the dish itself and the comments)This is because when you add a comment and apply the update, you're actually doing an update on the dish and you see, through the developer console, the http request, you'll see the entire JSON data for the dish, similar to the one you see on the db.json.For the purpose of the demo is fine, but doing this on \"production\" could potentially destroy all the data on the server. For a typical anonymous user, the server would only allow you to post your comment and wouldn't allow you to touch the others ones.Again, this just a notice...\n",
      "\n",
      "\n",
      "\n",
      "did not extract\n",
      "52blABnqEeW9dA4X94-nLQ~bB_8aaZ4EeWacw7zR2f1aQ\n",
      "Week 4 : Directly use \"dist\" instead of copying to \"public\" (+proxying through browser-sync)\n",
      "\n",
      "As an alternate solution to the one proposed by Pr. Muppala, instead of creating a public directory and copying the content of the dist directory inside to be server by json-server, you might use the --static option.Here is the command I use, typed in a terminal from the root directory of the project:--watch server/db.json : use the db.json file stored in the conFusion/server/ sub-deirectory--static dist : serve static file from conFusion/dist/--port 3100 : use port 3100 (see below why)----In addition, you can even use browser-sync on top of json-server. While that later is running, start a new terminal, and in the project root directory use:Now, if you point your browser to http://localhost:3000 you will connect to browser-sync that will forward your request to json-server. The key advantage with this setup is, when you change the content of a static file (say, index.html) -- the page will be automagically reloaded on your browser. Even if the \"real\" server is json-server !(I let the gulp integration as an exercise to the reader ;)\n",
      "Great! One can also make a nice shell script from those commands to make it even easier to run!\n",
      "You can also just modify the gulpfile to also pipe the output to the 'public' folder.\n",
      "Another possibility is to add some more tasks to gulp to automatically copy all the files in the dist folder to the public directory. I got this to work but it took me a while to figure out that I had to have my copy task run after all the other tasks had already completed (I thought I had originally done that but apparently not).\n",
      "\n",
      "\n",
      "\n",
      "did not extract\n",
      "52blABnqEeW9dA4X94-nLQ~BXlYwKehEeW2NBLvq9LZHQ\n",
      "Week 4: Exercice \"Handling Erros in Client-Server Communication...\"  Why assign the resource twice?\n",
      "\n",
      "in the line 1, we assign the resource to the $scope.dish,  in the line 5, we re-assign the resource to it(if success). i've tried to delete the [ $scope.dish= ]  in the line 1, and it made no difference. So is there some reason else i didn't get here? \n",
      "I have been thinking about this as well! Following.\n",
      "well actually, it's useless to assigné it twice.I asked myself the Same question\n",
      "\n",
      "\n",
      "\n",
      "did not extract\n",
      "52blABnqEeW9dA4X94-nLQ~qfh1mKfAEeWiXxLB9mtqCw\n",
      "Http response code:301 and 404\n",
      "\n",
      "How does server decide whether 301 and 404 should be sent as response because in both cases, resource is not found ?\n",
      "\n",
      "\n",
      "\n",
      "did not extract\n",
      "52blABnqEeW9dA4X94-nLQ~0q7q3qfGEeWRTArgYkB6yQ\n",
      "Error: -1\n",
      "\n",
      "hi I'm stopping the json-serverI get Error: -1 instead of the expected 404 when the json-server is down.I debug in the browser and I break inbut I cannot spot the 404 even in the Network browser tools console, I can not see the 404 response.any idea why I'm getting this -1 ?Regards,\n",
      "\n",
      "\n",
      "\n",
      "did not extract\n",
      "52blABnqEeW9dA4X94-nLQ~aHhlkqp-EeWoGg6ulZMPEw\n",
      "Yo must be joking.\n",
      "\n",
      "Ran \"yo angular.\" After waiting five minutes for it to install several hundred Node packages to \"scaffold\" a project, I was curious and ran du -h -s.TWO HUNDRED MEGABYTES? Just for the scaffolding for ONE project?This \"utility\" is a joke.\n",
      "\n",
      "\n",
      "\n",
      "did not extract\n",
      "52blABnqEeW9dA4X94-nLQ~AsffP6z7EeWF6gpqp4BTmQ\n",
      "Unfortunate ambiguity with 'update' in assignment.\n",
      "\n",
      "This assignment doesn't require the optional {'update':{method:'PUT' }} to be configured in the services file for promotions or leadership because information is just fetched, however the tasks mention updating information on the web pages as though the data could change. So are we supposed to include the include the {'update':{method:'PUT'}} in the configuration, or will we be penalized because it wasn't necessary?Thanks to anyone who can help!\n",
      "\n",
      "\n",
      "\n",
      "did not extract\n",
      "52blABnqEeW9dA4X94-nLQ~3yIj5K3EEeWF2Q53QdZUbw\n",
      "Does Week 4 Exercise and Assignment require Gulp\n",
      "\n",
      "HiI just completed Week 3 and am starting Week 4. I had problems with Gulp on my Windows Machine and used Grunt up to now. My Question:Can Week 4 be completed using Grunt or do I need to find a solution for using Gulp?ThanksSean \n",
      "  A lot of us are in the same boat. Grunt seems to work fine however a few of us are having problems due to the course build-up using Gulp so far. See some of the other threads here.  \n",
      "\n",
      "\n",
      "\n",
      "did not extract\n",
      "52blABnqEeW9dA4X94-nLQ~iap1h7FeEeW2NBLvq9LZHQ\n",
      "Is it possible to extend the deadline for the Assignment 4?\n",
      "\n",
      "Many of us were traveling to be with our beloved families this holiday season. I believe that many find themselves in this same situation.\n",
      "I hope you had a happy holiday season.The assignment deadlines are  actually flexible. If you look in the Coursera help it states that for courses on the new platform (which this is) then there is no penalty for missing a deadline. The only problem is that there is no guarantee that there will be any peers around to mark it for you.\n",
      "I'm in the same boat, I've travelled for the holidays and it was difficult finishing the angular assignments in the gaps. I'll be driving all day Sunday, and as it is right now, I'll likely won't finish before Monday...\n",
      "\n",
      "\n",
      "\n",
      "did not extract\n",
      "52blABnqEeW9dA4X94-nLQ~inQimrR4EeW2LwpPVHyhww\n",
      "Can you use ng-href and not ui-sref\n",
      "\n",
      "Have got everything working on Json-server, index and menu but not the link in photos, would like to use ng-href as that was working and not ui-sref but when moving to '/menu/:id' page I get a $stateParams is not defined error and in the dishdetail.html page just looks like it's missing the right id:{{NUMBER}}.But the id changes as i move over each image . Well if anybody is still working on this then it would be great , but if not hope everyone has a good 2016 .\n",
      "If you want to use the router (and you do) you need to use ui-sref. Make sure the router code and .js files are correct.\n",
      "If you want to use the router (and you do) you need to use ui-sref. Make sure the router code and .js files are correct.\n",
      "Thanks Tp , have used <a ui-sref=\"app.dishdetails({id: dish.id})\"> in menu.html shows nothing at the bottom left of the screen when I move over photos. And have this set up in app.js , I guessing the problem is in the controllers.js but IndexController.js is working as pulling in Dish image, and MenueController is ok for Dishes as this pulls in all the DishesSo I guess the problem is here in this part of the MenuController or am in the wrong area all together , well thanks once again for answering .\n",
      "I think found what is wrong just in case thought I would put it here, I was using this in app.js when I think it should not be using ngRoute but ['ui.router'] I am not sure why but will go back and see the video on UI-Router again.\n",
      "\n",
      "\n",
      "\n",
      "did not extract\n",
      "52blABnqEeW9dA4X94-nLQ~eLdO-LYMEeWjcBKYJq1ZMQ\n",
      "Hi Jogesh, App is perfect running on virtual server, but ...\n",
      "\n",
      "I made a trial and transfered our, I mean your (Jogesh) Webapplication, to my virtual server (with a standard apache webserver) and it is running, I tried entering some comments and they are stored into the db.json.It's so perfect, what do you teached us.But now, what is with security for such an easy application? I mean following: The app is at  but I (and so everybody) can see all content in the Json-File:  This is not good :)Do we learn in the the 5th course, \"Server-side Development with NodeJS\", all about security? There is only a little notice: \"We touch upon authentication and security.\"If not, where do I find a starting-point (i.e. a tutorial) learning about making it safe?\n",
      "\n",
      "\n",
      "\n",
      "did not extract\n",
      "52blABnqEeW9dA4X94-nLQ~NMxvK7deEeWjxw7W9fJX5Q\n",
      "My first Android app , Thanks to Prof Muppala\n",
      "\n",
      "After completing this course, I started exploring Ionic (which will be taught in next course this Specialization) , I made a simple app with the knowledge that I have gained so far from this Specialization.Here is the play Store link :Please give your valuable feedback and rate the app.You can see the power of  \" List \" in the app.Thanks to Alberto for teaching us this Specialization, you know what I mean.\n",
      "\n",
      "\n",
      "\n",
      "rubbish extracted\n",
      "52blABnqEeW9dA4X94-nLQ~jU2haYiNEearVhJfoMXTAQ\n",
      "test cases failed with Error: Unexpected request: GET views/header.html\n",
      "\n",
      "Running test cases failing with multiple similar error, complaining about Unexpected request: GET views/header.html:Do you have any ideas how to fix that?\n",
      "I have found the solution, but it is kind of large.The problem is with the ui-router. See Stack Overflow discussion: What I've done to fix my confusionApp unit testing:I created conFusion\\test\\mock folder;Inside new folder I created stateMock.js file;Inside this file I placed the (as in URL above);In conFusion\\test\\karma.conf.js I added 'test/mock/stateMock.js' to the files;In conFusion\\test\\unit\\controllers\\menucontroller.js file in the begining I made following chages (see the ui-router comments):Again, I am not sure if it is correct way, because I am new to JavaScript, but it works for me!\n",
      "This is doen't work for me also(((\n",
      "Adding the following code to menucontroller.js fixed this for me:For reference here is my full file code:\n",
      "Thanks a lot Alexsander, It worked for me. \n",
      "Encountered the same issue.  Was able to fix it by adding the following to menucontroller.js test file:\n",
      "Thank you for the solution, Jason.\n",
      "You saved me!! Thank you!!\n",
      "Thanks, great workaround. :-)\n",
      "Thanks Jason!\n",
      "Thanks Jason!\n",
      "thanks a lot\n",
      "This solution doesn't work for me... oh my god, so many errors in this course and lessons... it makes me mad(Can not it be done with more quality?(\n",
      "Thank you very much! It works!!!\n",
      "Thanks Jason and Satya because i need both solutions!!!!\n",
      "Thanks Jason! It worked.I also, had to use the \"if (angular.element.cleanData) angular.element.cleanData(cleanUpNodes);\" to fix my errors. Thanks Olga and Satya.\n",
      "Nice Solution!!\n",
      "Thanks for noting this problem and offering the solution. \n",
      "It works but I don't know why. Could you please explain a little how you figure out this solution? Thanks!\n",
      "Thanks Jason, it worked for me!\n",
      "Worked for me also\n",
      "I tried those two solutions and others i found online.What solved the problem to me was just include the jQuery on the karma.conf.js files      'bower_components/jquery/dist/jquery.js'\n",
      "Thanks Alexander for your detailed answer, it works for me!\n",
      "This didnt work for me.. when i added that code, I got a new error :TypeError: undefined is not a constructor (evaluating 'angular.element.cleanData(cleanUpNodes)') in bower_components/angular-mocks/angular-mocks.js (line 2922)Any help is appreciated! Thanks.\n",
      "But replacing angular.element.cleanData(cleanUpNodes);with if (angular.element.cleanData) angular.element.cleanData(cleanUpNodes);in angular-mocks.js worked!\n",
      "They discuss another error, that is not solved by substitution in angular-mocks.js \n",
      "  Thanks Jason and Satya because i need both solutions!!!!  \n",
      "Thank you Jason, it worked for me too.\n",
      "Thank you Sataya!\n",
      "Jasons and Satyas Solution worked for me. In order for others to avoid searching around and wasting hours I summarize them here:Change your menucontroller.js to the following:2. in /bower_components/angular-mocks.js change to\n",
      "Hello, I had the same problem and I managed to resolve updating the \"bower update\" and selecting the most current version of angularjs 1.6.0 and added your code of item 1 only.Thanks.\n",
      "I had a completely different error, but I applied what you suggested and it works now, thank you!\n",
      "I tried your solution and it worked for me. Thanks Alexander!\n",
      "This solution worked for me 2...thanks Alexander, excellent wok! \n",
      "Thank you very much!\n",
      "Big thank you! This is very helpful and I have solved this problem!\n",
      "It solved my problem too! thanks a lot\n",
      "This solved my problem as well! Thanks Alexander. Off to the next exercise!\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(test_result)):\n",
    "    if test_result[i] == 1 and pred_result[i] == 0:\n",
    "        print('did not extract')\n",
    "        print(testing_ids[i])\n",
    "        print(texts[id_to_index[testing_ids[i]]])\n",
    "        print('\\n')\n",
    "    elif test_result[i] == 0 and pred_result[i] == 1:\n",
    "        print('rubbish extracted')\n",
    "        print(testing_ids[i])\n",
    "        print(texts[id_to_index[testing_ids[i]]])\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.02042192 -0.06138293  0.12861602 ..., -0.04245163 -1.04310731\n",
      "  -0.30940939]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.74      0.71      0.72        83\n",
      "          1       0.56      0.59      0.57        51\n",
      "\n",
      "avg / total       0.67      0.66      0.67       134\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# exploring clustering coefficient\n",
    "\n",
    "training_vectors_mix = []\n",
    "test_vectors_mix = []\n",
    "\n",
    "training_indices = []\n",
    "test_indices = []\n",
    "\n",
    "super_clustering_score = []\n",
    "normal_clustering_score = []\n",
    "\n",
    "x = np.asarray(tmp_vectors)\n",
    "y = np.asarray(isreplied)\n",
    "\n",
    "for index in range(data_length):\n",
    "    \n",
    "    oneid = ids[index]\n",
    "    origin_list = x[index].tolist()\n",
    "    starter = starter_dic[oneid]\n",
    "    forum_id = thread_forum_dic[oneid]\n",
    "    g = forum_graph_dic[forum_id]\n",
    "    user = starter_dic[oneid]\n",
    "    time = thread_posted_time_dic[oneid]\n",
    "    intervened = is_replied[oneid]\n",
    "    clu = get_chronological_clustering_coef(time, g, user)\n",
    "    if intervened:\n",
    "        super_clustering_score.append(clu)\n",
    "    else:\n",
    "        normal_clustering_score.append(clu)\n",
    "    origin_list.append(clu)\n",
    "\n",
    "    if forum_type_dic[thread_forum_dic[oneid]] <= middleInd:\n",
    "        training_vectors_mix.append(origin_list)\n",
    "\n",
    "    else:\n",
    "        test_vectors_mix.append(origin_list)\n",
    "\n",
    "\n",
    "LogReg = LogisticRegression(class_weight = 'balanced')\n",
    "LogReg.fit(training_vectors_mix, training_result)\n",
    "\n",
    "print(LogReg.coef_)\n",
    "pred_result_mix = LogReg.predict(test_vectors_mix)\n",
    "with open('EDM.txt', 'w') as f:\n",
    "    print(classification_report(test_result, pred_result_mix), file = f)\n",
    "print(classification_report(test_result, pred_result_mix))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAegAAAFJCAYAAABQEL5HAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJiIikhADNBERkYT+P3IyK28awLkMAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x118290f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAecAAAFJCAYAAAChG+XKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngqFxJoQQQgRD40wIIYQIhsaZEEIIEcxfvlbIAQiqOD8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x118200a20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(super_clustering_score, hist = False, rug = True);\n",
    "plt.show()\n",
    "\n",
    "sns.distplot(normal_clustering_score, hist = False, rug = True);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.02091583 -0.02658977 -0.03983184 ...,  0.01638936 -0.0427159\n",
      "  -1.29993875]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.63      0.72       119\n",
      "          1       0.44      0.69      0.54        49\n",
      "\n",
      "avg / total       0.72      0.65      0.66       168\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# exploring pgrank\n",
    "\n",
    "training_vectors_mix = []\n",
    "test_vectors_mix = []\n",
    "\n",
    "training_indices = []\n",
    "test_indices = []\n",
    "\n",
    "\n",
    "x = np.asarray(tmp_vectors)\n",
    "y = np.asarray(isreplied)\n",
    "\n",
    "for index in range(data_length):\n",
    "    \n",
    "    oneid = ids[index]\n",
    "    origin_list = x[index].tolist()\n",
    "    starter = starter_dic[oneid]\n",
    "    forum_id = thread_forum_dic[oneid]\n",
    "    g = forum_digraph_dic[forum_id]\n",
    "    user = starter_dic[oneid]\n",
    "    time = thread_posted_time_dic[oneid]\n",
    "    intervened = is_replied[oneid]\n",
    "    pgrank = get_chronological_pgrank(time, g, user)\n",
    "    \n",
    "    '''\n",
    "    if intervened:\n",
    "        super_clustering_score.append(clu)\n",
    "    else:\n",
    "        normal_clustering_score.append(clu)\n",
    "    origin_list.append(clu)\n",
    "    '''\n",
    "\n",
    "    if forum_type_dic[thread_forum_dic[oneid]] <= middleInd:\n",
    "        training_vectors_mix.append(origin_list)\n",
    "\n",
    "    else:\n",
    "        test_vectors_mix.append(origin_list)\n",
    "\n",
    "\n",
    "LogReg = LogisticRegression(class_weight = 'balanced')\n",
    "LogReg.fit(training_vectors_mix, training_result)\n",
    "\n",
    "print(LogReg.coef_)\n",
    "pred_result_mix = LogReg.predict(test_vectors_mix)\n",
    "with open('EDM.txt', 'w') as f:\n",
    "    print(classification_report(test_result, pred_result_mix), file = f)\n",
    "print(classification_report(test_result, pred_result_mix))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.02071005917159763, 0.0, 0, 0, 0.019774011299435026, 0.14285714285714285, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.005649717514124294, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.022598870056497175, 0.07142857142857142, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.01694915254237288, 0.06666666666666667, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.03389830508474576, 0.06060606060606061, 0, 0, 0, 0, 0, 0]\n",
      "[0.019774011299435026, 0.14285714285714285, 0, 0, 0, 0, 0, 0]\n",
      "[0.04519774011299435, 0.05, 0, 0, 0, 0, 0, 0]\n",
      "[0.0041841004184100415, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.011299435028248588, 0.16666666666666666, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.005649717514124294, 1.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.03954802259887005, 0.04395604395604396, 0, 0, 0, 0, 0, 0]\n",
      "[0.005649717514124294, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.00847457627118644, 0.3333333333333333, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.011299435028248588, 0.16666666666666666, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.002824858757062147, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.031073446327683614, 0.16363636363636364, 0, 0, 0, 0, 0, 0]\n",
      "[0.002824858757062147, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.031073446327683614, 0.16363636363636364, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.02824858757062147, 0.022222222222222223, 0, 0, 0, 0, 0, 0]\n",
      "[0.00847457627118644, 0.3333333333333333, 0, 0, 0, 0, 0, 0]\n",
      "[0.03954802259887005, 0.04395604395604396, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.005649717514124294, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.00847457627118644, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.005649717514124294, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.005649717514124294, 1.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.002824858757062147, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.019774011299435026, 0.14285714285714285, 0, 0, 0, 0, 0, 0]\n",
      "[0.014124293785310734, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.005649717514124294, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.04519774011299435, 0.1, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.04519774011299435, 0.08333333333333333, 0, 0, 0, 0, 0, 0]\n",
      "[0.014124293785310734, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.00847457627118644, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.005649717514124294, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.005649717514124294, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.005649717514124294, 1.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.04519774011299435, 0.08333333333333333, 0, 0, 0, 0, 0, 0]\n",
      "[0.00847457627118644, 0.3333333333333333, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.005649717514124294, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.02824858757062147, 0.17777777777777778, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.04519774011299435, 0.08333333333333333, 0, 0, 0, 0, 0, 0]\n",
      "[0.0041841004184100415, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.011299435028248588, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.02824858757062147, 0.17777777777777778, 0, 0, 0, 0, 0, 0]\n",
      "[0.00847457627118644, 0.3333333333333333, 0, 0, 0, 0, 0, 0]\n",
      "[0.00847457627118644, 0.3333333333333333, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.005649717514124294, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.011299435028248588, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.031073446327683614, 0.09090909090909091, 0, 0, 0, 0, 0, 0]\n",
      "[0.005649717514124294, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.00847457627118644, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.03389830508474576, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.03389830508474576, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.014124293785310734, 0.2, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.03389830508474576, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.03389830508474576, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.04519774011299435, 0.1, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.019774011299435026, 0.14285714285714285, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.002824858757062147, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.005917159763313609, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.22594142259414224, 0.002795248078266946, 0.00847457627118644, 0.3333333333333333, 0, 0, 0, 0]\n",
      "[0.0041841004184100415, 0.0, 0.01694915254237288, 0.0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.014124293785310734, 0.2, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.005917159763313609, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.014124293785310734, 0.2, 0, 0, 0, 0, 0, 0]\n",
      "[0.005649717514124294, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.005649717514124294, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.22594142259414224, 0.002795248078266946, 0.00847457627118644, 0.3333333333333333, 0, 0, 0, 0]\n",
      "[0.011299435028248588, 0.3333333333333333, 0, 0, 0, 0, 0, 0]\n",
      "[0.0041841004184100415, 0.0, 0.04519774011299435, 0.1, 0, 0, 0, 0]\n",
      "[0.0041841004184100415, 0.0, 0.04519774011299435, 0.1, 0, 0, 0, 0]\n",
      "[0, 0, 0.0041841004184100415, 0.0, 0.04519774011299435, 0.1, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.03389830508474576, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.002824858757062147, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.022598870056497175, 0.07142857142857142, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.002824858757062147, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.0041841004184100415, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.00847457627118644, 0.3333333333333333, 0, 0, 0, 0, 0, 0]\n",
      "[0.005917159763313609, 0.0, 0, 0, 0.04519774011299435, 0.08333333333333333, 0, 0]\n",
      "[0.00847457627118644, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.002824858757062147, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.002824858757062147, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.002824858757062147, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.002824858757062147, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.0790960451977401, 0.03439153439153439, 0, 0, 0, 0, 0, 0]\n",
      "[0.0790960451977401, 0.03439153439153439, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.00847457627118644, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.02824858757062147, 0.022222222222222223, 0, 0, 0, 0, 0, 0]\n",
      "[0.00847457627118644, 0.3333333333333333, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.005649717514124294, 1.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.0790960451977401, 0.03439153439153439, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.005649717514124294, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.0029585798816568047, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.01694915254237288, 0.13333333333333333, 0, 0, 0, 0, 0, 0]\n",
      "[0.002824858757062147, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.005649717514124294, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.00847457627118644, 0.3333333333333333, 0, 0, 0, 0, 0, 0]\n",
      "[0.00847457627118644, 0.3333333333333333, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.005649717514124294, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.002824858757062147, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.002824858757062147, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.005917159763313609, 0.0, 0, 0, 0.002824858757062147, 0.0, 0, 0]\n",
      "[0.00847457627118644, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0.0041841004184100415, 0.0, 0.031073446327683614, 0.16363636363636364, 0, 0]\n",
      "[0.0790960451977401, 0.03439153439153439, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0.0041841004184100415, 0.0, 0.031073446327683614, 0.16363636363636364, 0, 0]\n",
      "[0.005649717514124294, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.005649717514124294, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.0041841004184100415, 0.0, 0.011299435028248588, 0.5, 0, 0, 0, 0]\n",
      "[0.0790960451977401, 0.03439153439153439, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.03347280334728033, 0.03571428571428571, 0.005649717514124294, 0.0, 0, 0, 0, 0]\n",
      "[0.00847457627118644, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.03347280334728033, 0.03571428571428571, 0.005649717514124294, 0.0, 0, 0, 0, 0]\n",
      "[0.002824858757062147, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.011299435028248588, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.012552301255230124, 0.3333333333333333, 0, 0, 0, 0, 0, 0]\n",
      "[0.002824858757062147, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.00847457627118644, 0.3333333333333333, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.025423728813559324, 0.027777777777777776, 0, 0, 0, 0, 0, 0]\n",
      "[0.002824858757062147, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.0041841004184100415, 0.0, 0.00847457627118644, 0.3333333333333333, 0, 0, 0, 0]\n",
      "[0.005917159763313609, 0.0, 0, 0, 0.04519774011299435, 0.08333333333333333, 0, 0]\n",
      "[0.005917159763313609, 0.0, 0, 0, 0.04519774011299435, 0.08333333333333333, 0, 0]\n",
      "[0.005649717514124294, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.00847457627118644, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.00847457627118644, 0.3333333333333333, 0, 0, 0, 0, 0, 0]\n",
      "[0.00847457627118644, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.005649717514124294, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0.00847457627118644, 0.3333333333333333, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.025104602510460247, 0.06666666666666667, 0, 0, 0, 0, 0, 0]\n",
      "[0.005649717514124294, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.011299435028248588, 0.16666666666666666, 0, 0, 0, 0, 0, 0]\n",
      "[0.005649717514124294, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.04184100418410042, 0.0, 0.0790960451977401, 0.03439153439153439, 0, 0, 0, 0]\n",
      "[0.00847457627118644, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.011299435028248588, 0.16666666666666666, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.002824858757062147, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.00847457627118644, 0.3333333333333333, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.00847457627118644, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.0029585798816568047, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.0041841004184100415, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.00847457627118644, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0.005649717514124294, 0.0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.03347280334728033, 0.03571428571428571, 0.005649717514124294, 0.0, 0, 0, 0, 0]\n",
      "[0.00847457627118644, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.04184100418410042, 0.0, 0.0790960451977401, 0.03439153439153439, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.03347280334728033, 0.03571428571428571, 0.005649717514124294, 0.0, 0, 0, 0, 0]\n",
      "[0.02824858757062147, 0.022222222222222223, 0, 0, 0, 0, 0, 0]\n",
      "[0.025104602510460247, 0.06666666666666667, 0, 0, 0, 0, 0, 0]\n",
      "[0.025104602510460247, 0.06666666666666667, 0.002824858757062147, 0.0, 0, 0, 0, 0]\n",
      "[0.00847457627118644, 0.3333333333333333, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.01694915254237288, 0.06666666666666667, 0, 0, 0, 0, 0, 0]\n",
      "[0.002824858757062147, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.002824858757062147, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.025104602510460247, 0.06666666666666667, 0, 0, 0, 0, 0, 0]\n",
      "[0.014124293785310734, 0.1, 0, 0, 0, 0, 0, 0]\n",
      "[0.014124293785310734, 0.2, 0, 0, 0, 0, 0, 0]\n",
      "[0.014124293785310734, 0.2, 0, 0, 0, 0, 0, 0]\n",
      "[0.005649717514124294, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.014124293785310734, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.00847457627118644, 0.3333333333333333, 0, 0, 0, 0, 0, 0]\n",
      "[0.012552301255230124, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.00847457627118644, 0.3333333333333333, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.00847457627118644, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.002824858757062147, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.00847457627118644, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.01694915254237288, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.011299435028248588, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.03389830508474576, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.04184100418410042, 0.0, 0.0790960451977401, 0.03439153439153439, 0, 0, 0, 0]\n",
      "[0.04184100418410042, 0.0, 0.0790960451977401, 0.03439153439153439, 0, 0, 0, 0]\n",
      "[0.002824858757062147, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.029585798816568046, 0.08888888888888889, 0, 0, 0.002824858757062147, 0.0, 0, 0]\n",
      "[0.005649717514124294, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.02824858757062147, 0.17777777777777778, 0, 0, 0, 0, 0, 0]\n",
      "[0.03550295857988166, 0.030303030303030304, 0, 0, 0.002824858757062147, 0.0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.002824858757062147, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.00847457627118644, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.00847457627118644, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.0041841004184100415, 0.0, 0.002824858757062147, 0.0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.002824858757062147, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.00847457627118644, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.016736401673640166, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.002824858757062147, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.04184100418410042, 0.0, 0.0790960451977401, 0.03439153439153439, 0, 0, 0, 0]\n",
      "[0.04184100418410042, 0.0, 0.0790960451977401, 0.03439153439153439, 0, 0, 0, 0]\n",
      "[0.011299435028248588, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.005649717514124294, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.011299435028248588, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.002824858757062147, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.002824858757062147, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.005649717514124294, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.0041841004184100415, 0.0, 0.014124293785310734, 0.0, 0, 0, 0, 0]\n",
      "[0.002824858757062147, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.002824858757062147, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.002824858757062147, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.002824858757062147, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.02071005917159763, 0.047619047619047616, 0.03347280334728033, 0.03571428571428571, 0.005649717514124294, 0.0, 0, 0]\n",
      "[0.005649717514124294, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.012552301255230124, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.002824858757062147, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.05325443786982249, 0.0196078431372549, 0.04184100418410042, 0.0, 0.0790960451977401, 0.03439153439153439, 0, 0]\n",
      "[0.008368200836820083, 0.0, 0.002824858757062147, 0.0, 0, 0, 0, 0]\n",
      "[0.002824858757062147, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.011299435028248588, 0.16666666666666666, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.011299435028248588, 0.16666666666666666, 0, 0, 0, 0, 0, 0]\n",
      "[0.005649717514124294, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.005649717514124294, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.005649717514124294, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.03550295857988166, 0.030303030303030304, 0, 0, 0.002824858757062147, 0.0, 0, 0]\n",
      "[0.005649717514124294, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.002824858757062147, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.005649717514124294, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0.005649717514124294, 0.0, 0, 0, 0, 0]\n",
      "[0.01694915254237288, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.01694915254237288, 0.06666666666666667, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.05325443786982249, 0.0196078431372549, 0.04184100418410042, 0.0, 0.0790960451977401, 0.03439153439153439, 0, 0]\n",
      "[0.01694915254237288, 0.06666666666666667, 0, 0, 0, 0, 0, 0]\n",
      "[0.005649717514124294, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.005649717514124294, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.011299435028248588, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.005649717514124294, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.012552301255230124, 0.3333333333333333, 0.002824858757062147, 0.0, 0, 0, 0, 0]\n",
      "[0.002824858757062147, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.005649717514124294, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.005649717514124294, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.002824858757062147, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.002824858757062147, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.011834319526627219, 0.16666666666666666, 0, 0, 0, 0, 0, 0]\n",
      "[0.02071005917159763, 0.047619047619047616, 0.03347280334728033, 0.03571428571428571, 0.005649717514124294, 0.0, 0, 0]\n",
      "[0.005649717514124294, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.05325443786982249, 0.0196078431372549, 0.04184100418410042, 0.0, 0.0790960451977401, 0.03439153439153439, 0, 0]\n",
      "[0.002824858757062147, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.011299435028248588, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.0029585798816568047, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.002824858757062147, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.00847457627118644, 0.3333333333333333, 0, 0, 0, 0, 0, 0]\n",
      "[0.005649717514124294, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.01694915254237288, 0.06666666666666667, 0, 0, 0, 0, 0, 0]\n",
      "[0.002824858757062147, 0.0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.005649717514124294, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.005649717514124294, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.005649717514124294, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.002824858757062147, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.00847457627118644, 0.3333333333333333, 0, 0, 0, 0, 0, 0]\n",
      "[0.0029585798816568047, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.05325443786982249, 0.0196078431372549, 0.04184100418410042, 0.0, 0.0790960451977401, 0.03439153439153439, 0, 0]\n",
      "[0.002824858757062147, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.011299435028248588, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.002824858757062147, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.014124293785310734, 0.2, 0, 0, 0, 0, 0, 0]\n",
      "[0.002824858757062147, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.02824858757062147, 0.044444444444444446, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.023668639053254437, 0.10714285714285714, 0.02928870292887029, 0.09523809523809523, 0.002824858757062147, 0.0, 0, 0]\n",
      "[0.00847457627118644, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.002824858757062147, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.002824858757062147, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.012552301255230124, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.002824858757062147, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.00847457627118644, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.008875739644970414, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.023668639053254437, 0.10714285714285714, 0.02928870292887029, 0.09523809523809523, 0.002824858757062147, 0.0, 0, 0]\n",
      "[0.008875739644970414, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.016736401673640166, 0.16666666666666666, 0.005649717514124294, 1.0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0.002824858757062147, 0.0, 0, 0]\n",
      "[0.014124293785310734, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.011299435028248588, 0.3333333333333333, 0, 0, 0, 0, 0, 0]\n",
      "[0.016736401673640166, 0.0, 0.00847457627118644, 0.0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.025104602510460247, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.05325443786982249, 0.0196078431372549, 0.04184100418410042, 0.0, 0.0790960451977401, 0.03439153439153439, 0, 0]\n",
      "[0.002824858757062147, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.002824858757062147, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.005649717514124294, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.0041841004184100415, 0.0, 0.005649717514124294, 1.0, 0, 0, 0, 0]\n",
      "[0.0029585798816568047, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.0041841004184100415, 0.0, 0.002824858757062147, 0.0, 0, 0, 0, 0]\n",
      "[0.008368200836820083, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.02092050209205021, 0.2, 0.02824858757062147, 0.022222222222222223, 0, 0, 0, 0]\n",
      "[0.002824858757062147, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.014124293785310734, 0.2, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.012552301255230124, 0.3333333333333333, 0.002824858757062147, 0.0, 0, 0, 0, 0]\n",
      "[0.01775147928994083, 0.06666666666666667, 0, 0, 0, 0, 0, 0]\n",
      "[0.002824858757062147, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.008875739644970414, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.012552301255230124, 0.3333333333333333, 0, 0, 0, 0, 0, 0]\n",
      "[0.002824858757062147, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.0041841004184100415, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.02928870292887029, 0.047619047619047616, 0, 0, 0, 0, 0, 0]\n",
      "[0.008368200836820083, 0.0, 0.01694915254237288, 0.0, 0, 0, 0, 0]\n",
      "[0.02092050209205021, 0.2, 0.02824858757062147, 0.022222222222222223, 0, 0, 0, 0]\n",
      "[0.008875739644970414, 0.3333333333333333, 0.0041841004184100415, 0.0, 0.011299435028248588, 0.5, 0, 0]\n",
      "[0.0041841004184100415, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.008368200836820083, 0.0, 0.005649717514124294, 0.0, 0, 0, 0, 0]\n",
      "[0.025104602510460247, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.005649717514124294, 1.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.014124293785310734, 0.2, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.012552301255230124, 0.3333333333333333, 0.002824858757062147, 0.0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.016736401673640166, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.025104602510460247, 0.26666666666666666, 0, 0, 0, 0, 0, 0]\n",
      "[0.011299435028248588, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.025104602510460247, 0.2, 0, 0, 0, 0, 0, 0]\n",
      "[0.012552301255230124, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.03347280334728033, 0.03571428571428571, 0.04519774011299435, 0.05, 0, 0, 0, 0]\n",
      "[0.002824858757062147, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.008368200836820083, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.0041841004184100415, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.002824858757062147, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.0029585798816568047, 0.0, 0.0041841004184100415, 0.0, 0, 0, 0, 0]\n",
      "[0.002824858757062147, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.014124293785310734, 0.1, 0, 0, 0, 0, 0, 0]\n",
      "[0.012552301255230124, 0.3333333333333333, 0.002824858757062147, 0.0, 0, 0, 0, 0]\n",
      "[0.0041841004184100415, 0.0, 0.005649717514124294, 0.0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.008368200836820083, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.005917159763313609, 0.0, 0, 0, 0.011299435028248588, 0.3333333333333333, 0, 0]\n",
      "[0.023668639053254437, 0.10714285714285714, 0.02928870292887029, 0.09523809523809523, 0.002824858757062147, 0.0, 0, 0]\n",
      "[0.03550295857988166, 0.06060606060606061, 0.008368200836820083, 0.0, 0.01694915254237288, 0.0, 0, 0]\n",
      "[0.008875739644970414, 0.3333333333333333, 0.016736401673640166, 0.16666666666666666, 0, 0, 0, 0]\n",
      "[0.005649717514124294, 1.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.005917159763313609, 0.0, 0.0041841004184100415, 0.0, 0.00847457627118644, 0.3333333333333333, 0, 0]\n",
      "[0.008368200836820083, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.008368200836820083, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.002824858757062147, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.06804733727810651, 0.043478260869565216, 0, 0, 0.005649717514124294, 0.0, 0, 0]\n",
      "[0.016736401673640166, 0.0, 0.00847457627118644, 0.0, 0, 0, 0, 0]\n",
      "[0.0029585798816568047, 0.0, 0.016736401673640166, 0.0, 0.005649717514124294, 0.0, 0, 0]\n",
      "[0.008368200836820083, 0.0, 0.011299435028248588, 0.16666666666666666, 0, 0, 0, 0]\n",
      "[0.008368200836820083, 0.0, 0.005649717514124294, 0.0, 0, 0, 0, 0]\n",
      "[0, 0, 0.00847457627118644, 0.0, 0, 0, 0, 0]\n",
      "[0.008368200836820083, 1.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.0041841004184100415, 0.0, 0.00847457627118644, 0.0, 0, 0, 0, 0]\n",
      "[0.008368200836820083, 0.0, 0.005649717514124294, 0.0, 0, 0, 0, 0]\n",
      "[0.029585798816568046, 0.08888888888888889, 0, 0, 0.002824858757062147, 0.0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.0029585798816568047, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.016736401673640166, 0.0, 0.005649717514124294, 0.0, 0, 0, 0, 0]\n",
      "[0.025104602510460247, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.06804733727810651, 0.043478260869565216, 0, 0, 0.005649717514124294, 0.0, 0, 0]\n",
      "[0, 0, 0.002824858757062147, 0.0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0.005649717514124294, 1.0, 0, 0]\n",
      "[0.02071005917159763, 0.0, 0.050209205020920494, 0.06060606060606061, 0.019774011299435026, 0.047619047619047616, 0, 0]\n",
      "[0.011299435028248588, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.02092050209205021, 0.1, 0.005649717514124294, 0.0, 0, 0, 0, 0]\n",
      "[0.02092050209205021, 0.1, 0.005649717514124294, 0.0, 0, 0, 0, 0]\n",
      "[0.02092050209205021, 0.1, 0.005649717514124294, 0.0, 0, 0, 0, 0]\n",
      "[0.025104602510460247, 0.2, 0, 0, 0, 0, 0, 0]\n",
      "[0.0029585798816568047, 0.0, 0.012552301255230124, 0.3333333333333333, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.0041841004184100415, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.014792899408284023, 0.2, 0, 0, 0, 0, 0, 0]\n",
      "[0.012552301255230124, 0.0, 0.005649717514124294, 0.0, 0, 0, 0, 0]\n",
      "[0.005917159763313609, 0.0, 0.012552301255230124, 0.3333333333333333, 0.002824858757062147, 0.0, 0, 0]\n",
      "[0.008368200836820083, 0.0, 0.002824858757062147, 0.0, 0, 0, 0, 0]\n",
      "[0.008368200836820083, 0.0, 0.002824858757062147, 0.0, 0, 0, 0, 0]\n",
      "[0.008368200836820083, 1.0, 0.014124293785310734, 0.2, 0, 0, 0, 0]\n",
      "[0, 0, 0.002824858757062147, 0.0, 0, 0, 0, 0]\n",
      "[0, 0, 0.005649717514124294, 0.0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.012552301255230124, 0.0, 0.002824858757062147, 0.0, 0, 0, 0, 0]\n",
      "[0.02928870292887029, 0.047619047619047616, 0.005649717514124294, 0.0, 0, 0, 0, 0]\n",
      "[0.008368200836820083, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.019774011299435026, 0.047619047619047616, 0, 0, 0, 0, 0, 0]\n",
      "[0.0041841004184100415, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.008875739644970414, 0.3333333333333333, 0.016736401673640166, 0.16666666666666666, 0, 0, 0, 0]\n",
      "[0.023668639053254437, 0.03571428571428571, 0.012552301255230124, 0.0, 0.005649717514124294, 0.0, 0, 0]\n",
      "[0.008368200836820083, 0.0, 0.002824858757062147, 0.0, 0, 0, 0, 0]\n",
      "[0.014792899408284023, 0.1, 0.008368200836820083, 0.0, 0, 0, 0, 0]\n",
      "[0.0041841004184100415, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.05029585798816568, 0.007352941176470588, 0, 0, 0, 0, 0, 0]\n",
      "[0.023668639053254437, 0.03571428571428571, 0.012552301255230124, 0.0, 0.005649717514124294, 0.0, 0, 0]\n",
      "[0.023668639053254437, 0.03571428571428571, 0.012552301255230124, 0.0, 0.005649717514124294, 0.0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.005917159763313609, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.014792899408284023, 0.2, 0, 0, 0, 0, 0, 0]\n",
      "[0.0041841004184100415, 0.0, 0.022598870056497175, 0.0, 0, 0, 0, 0]\n",
      "[0.008368200836820083, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0.025423728813559324, 0.027777777777777776, 0, 0, 0, 0]\n",
      "[0.0041841004184100415, 0.0, 0.005649717514124294, 0.0, 0, 0, 0, 0]\n",
      "[0.005917159763313609, 0.0, 0.012552301255230124, 0.3333333333333333, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.008875739644970414, 0.3333333333333333, 0.016736401673640166, 0.16666666666666666, 0, 0, 0, 0]\n",
      "[0.0041841004184100415, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.008875739644970414, 0.0, 0, 0, 0.00847457627118644, 0.3333333333333333, 0, 0]\n",
      "[0.05029585798816568, 0.007352941176470588, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.005917159763313609, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.0041841004184100415, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.0029585798816568047, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.008368200836820083, 0.0, 0.01694915254237288, 0.0, 0, 0, 0, 0]\n",
      "[0.011834319526627219, 0.16666666666666666, 0, 0, 0.002824858757062147, 0.0, 0, 0]\n",
      "[0, 0, 0.025423728813559324, 0.027777777777777776, 0, 0, 0, 0]\n",
      "[0.002824858757062147, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.011834319526627219, 0.16666666666666666, 0, 0, 0, 0, 0, 0]\n",
      "[0.04437869822485207, 0.05714285714285714, 0.008368200836820083, 0.0, 0.014124293785310734, 0.2, 0, 0]\n",
      "[0.008368200836820083, 0.0, 0.002824858757062147, 0.0, 0, 0, 0, 0]\n",
      "[0.04437869822485207, 0.05714285714285714, 0.008368200836820083, 0.0, 0.014124293785310734, 0.2, 0, 0]\n",
      "[0.04437869822485207, 0.05714285714285714, 0.008368200836820083, 0.0, 0.014124293785310734, 0.2, 0, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.011834319526627219, 0.0, 0.0041841004184100415, 0.0, 0.005649717514124294, 0.0, 0, 0]\n",
      "[0.04437869822485207, 0.05714285714285714, 0, 0, 0.005649717514124294, 0.0, 0, 0]\n",
      "[0.0029585798816568047, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.008875739644970414, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.011834319526627219, 0.16666666666666666, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0.005649717514124294, 0.0, 0, 0]\n",
      "[0, 0, 0, 0, 0.002824858757062147, 0.0, 0, 0]\n",
      "[0.02928870292887029, 0.047619047619047616, 0.005649717514124294, 0.0, 0, 0, 0, 0]\n",
      "[0.023668639053254437, 0.07142857142857142, 0.02928870292887029, 0.047619047619047616, 0.005649717514124294, 0.0, 0, 0]\n",
      "[0.008875739644970414, 0.0, 0.0041841004184100415, 0.0, 0.002824858757062147, 0.0, 0, 0]\n",
      "[0.002824858757062147, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.014792899408284023, 0.2, 0.008368200836820083, 0.0, 0.005649717514124294, 0.0, 0, 0]\n",
      "[0.014792899408284023, 0.2, 0.008368200836820083, 0.0, 0.005649717514124294, 0.0, 0, 0]\n",
      "[0.014792899408284023, 0.2, 0.008368200836820083, 0.0, 0.005649717514124294, 0.0, 0, 0]\n",
      "[0.0029585798816568047, 0.0, 0.0041841004184100415, 0.0, 0.005649717514124294, 0.0, 0, 0]\n",
      "[0.02071005917159763, 0.047619047619047616, 0, 0, 0.011299435028248588, 0.0, 0, 0]\n",
      "[0.014792899408284023, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.011834319526627219, 0.16666666666666666, 0.016736401673640166, 0.0, 0.005649717514124294, 0.0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.008875739644970414, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.011834319526627219, 0.0, 0.0041841004184100415, 0.0, 0.005649717514124294, 0.0, 0, 0]\n",
      "[0.011834319526627219, 0.16666666666666666, 0.016736401673640166, 0.0, 0.005649717514124294, 0.0, 0, 0]\n",
      "[0.0029585798816568047, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.0029585798816568047, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.0029585798816568047, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.0041841004184100415, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.011834319526627219, 0.16666666666666666, 0.0041841004184100415, 0.0, 0.022598870056497175, 0.0, 0, 0]\n",
      "[0.0029585798816568047, 0.0, 0, 0, 0.002824858757062147, 0.0, 0, 0]\n",
      "[0.04437869822485207, 0.05714285714285714, 0, 0, 0.005649717514124294, 0.0, 0, 0]\n",
      "[0, 0, 0.012552301255230124, 0.3333333333333333, 0.00847457627118644, 0.0, 0, 0]\n",
      "[0.1301775147928994, 0.0021141649048625794, 0, 0, 0, 0, 0, 0]\n",
      "[0.011834319526627219, 0.16666666666666666, 0.0041841004184100415, 0.0, 0.022598870056497175, 0.0, 0, 0]\n",
      "[0.02824858757062147, 0.044444444444444446, 0, 0, 0, 0, 0, 0]\n",
      "[0.0029585798816568047, 0.0, 0.008368200836820083, 0.0, 0.00847457627118644, 0.3333333333333333, 0, 0]\n",
      "[0.008875739644970414, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.008875739644970414, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.02824858757062147, 0.044444444444444446, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0.002824858757062147, 0.0, 0, 0]\n",
      "[0.011834319526627219, 0.3333333333333333, 0.025104602510460247, 0.2, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.02071005917159763, 0.0, 0, 0, 0.019774011299435026, 0.14285714285714285, 0, 0]\n",
      "[0.0029585798816568047, 0.0, 0.012552301255230124, 0.3333333333333333, 0.005649717514124294, 0.0, 0, 0]\n",
      "[0.0029585798816568047, 0.0, 0, 0, 0.002824858757062147, 0.0, 0, 0]\n",
      "[0.0029585798816568047, 0.0, 0, 0, 0.022598870056497175, 0.03571428571428571, 0, 0]\n",
      "[0, 0, 0, 0, 0.002824858757062147, 0.0, 0, 0]\n",
      "[0.008875739644970414, 0.0, 0.0041841004184100415, 0.0, 0.002824858757062147, 0.0, 0, 0]\n",
      "[0.023668639053254437, 0.0, 0.008368200836820083, 0.0, 0.022598870056497175, 0.07142857142857142, 0, 0]\n",
      "[0.005917159763313609, 0.0, 0.0041841004184100415, 0.0, 0.00847457627118644, 0.0, 0, 0]\n",
      "[0.008875739644970414, 0.0, 0.0041841004184100415, 0.0, 0.002824858757062147, 0.0, 0, 0]\n",
      "[0.0029585798816568047, 0.0, 0.0041841004184100415, 0.0, 0.002824858757062147, 0.0, 0, 0]\n",
      "[0.0029585798816568047, 0.0, 0, 0, 0.005649717514124294, 1.0, 0, 0]\n",
      "[0.005649717514124294, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.005917159763313609, 1.0, 0, 0, 0.011299435028248588, 0.0, 0, 0]\n",
      "[0, 0, 0.008368200836820083, 1.0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.011834319526627219, 0.16666666666666666, 0.02092050209205021, 0.0, 0, 0, 0, 0]\n",
      "[0.00847457627118644, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.008875739644970414, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.01775147928994083, 0.06666666666666667, 0.0041841004184100415, 0.0, 0.011299435028248588, 0.0, 0, 0]\n",
      "[0.023668639053254437, 0.0, 0, 0, 0.002824858757062147, 0.0, 0, 0]\n",
      "[0.011834319526627219, 0.3333333333333333, 0.025104602510460247, 0.2, 0, 0, 0, 0]\n",
      "[0.00847457627118644, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.0029585798816568047, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.008875739644970414, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.02071005917159763, 0.0, 0, 0, 0.019774011299435026, 0.14285714285714285, 0, 0]\n",
      "[0.005917159763313609, 0.0, 0.0041841004184100415, 0.0, 0.005649717514124294, 1.0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.0029585798816568047, 0.0, 0.0041841004184100415, 0.0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.008875739644970414, 0.3333333333333333, 0.008368200836820083, 0.0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.0029585798816568047, 0.0, 0, 0, 0.002824858757062147, 0.0, 0, 0]\n",
      "[0.011834319526627219, 0.0, 0.0041841004184100415, 0.0, 0.002824858757062147, 0.0, 0, 0]\n",
      "[0.011834319526627219, 0.0, 0.0041841004184100415, 0.0, 0.002824858757062147, 0.0, 0, 0]\n",
      "[0.014792899408284023, 0.2, 0, 0, 0, 0, 0, 0]\n",
      "[0.0029585798816568047, 0.0, 0.008368200836820083, 0.0, 0.011299435028248588, 0.0, 0, 0]\n",
      "[0.04437869822485207, 0.05714285714285714, 0.008368200836820083, 0.0, 0.014124293785310734, 0.2, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.011834319526627219, 0.0, 0, 0, 0.005649717514124294, 1.0, 0, 0]\n",
      "[0.02071005917159763, 0.047619047619047616, 0, 0, 0.00847457627118644, 0.0, 0, 0]\n",
      "[0.04437869822485207, 0.05714285714285714, 0.008368200836820083, 0.0, 0.014124293785310734, 0.2, 0, 0]\n",
      "[0.011834319526627219, 0.3333333333333333, 0.025104602510460247, 0.2, 0, 0, 0, 0]\n",
      "[0.008368200836820083, 0.0, 0.002824858757062147, 0.0, 0, 0, 0, 0]\n",
      "[0.005917159763313609, 0.0, 0.012552301255230124, 0.0, 0.002824858757062147, 0.0, 0, 0]\n",
      "[0.005917159763313609, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.01775147928994083, 0.06666666666666667, 0.0041841004184100415, 0.0, 0.011299435028248588, 0.0, 0, 0]\n",
      "[0.0029585798816568047, 0.0, 0, 0, 0.002824858757062147, 0.0, 0, 0]\n",
      "[0.005917159763313609, 0.0, 0.0041841004184100415, 0.0, 0, 0, 0, 0]\n",
      "[0.008875739644970414, 0.3333333333333333, 0.008368200836820083, 0.0, 0, 0, 0, 0]\n",
      "[0.011834319526627219, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0.002824858757062147, 0.0, 0, 0]\n",
      "[0.02071005917159763, 0.09523809523809523, 0, 0, 0.025423728813559324, 0.027777777777777776, 0, 0]\n",
      "[0.011834319526627219, 0.0, 0.0041841004184100415, 0.0, 0.002824858757062147, 0.0, 0, 0]\n",
      "[0.011834319526627219, 0.0, 0.0041841004184100415, 0.0, 0.002824858757062147, 0.0, 0, 0]\n",
      "[0.023668639053254437, 0.0, 0.008368200836820083, 0.0, 0.022598870056497175, 0.07142857142857142, 0, 0]\n",
      "[0.011834319526627219, 0.16666666666666666, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0.012552301255230124, 0.0, 0, 0, 0, 0]\n",
      "[0.008875739644970414, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.005917159763313609, 0.0, 0.0041841004184100415, 0.0, 0.005649717514124294, 1.0, 0, 0]\n",
      "[0.005917159763313609, 0.0, 0, 0, 0.01694915254237288, 0.06666666666666667, 0, 0]\n",
      "[0.011834319526627219, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0.008368200836820083, 1.0, 0.005649717514124294, 0.0, 0, 0]\n",
      "[0.02071005917159763, 0.0, 0.0041841004184100415, 0.0, 0, 0, 0, 0]\n",
      "[0.011834319526627219, 0.16666666666666666, 0, 0, 0, 0, 0, 0]\n",
      "[0.014792899408284023, 0.1, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.011834319526627219, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.014792899408284023, 0.1, 0, 0, 0, 0, 0, 0]\n",
      "[0.01775147928994083, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.011834319526627219, 0.0, 0.008368200836820083, 0.0, 0.005649717514124294, 0.0, 0, 0]\n",
      "[0.008368200836820083, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.02071005917159763, 0.0, 0.0041841004184100415, 0.0, 0, 0, 0, 0]\n",
      "[0.008875739644970414, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.005917159763313609, 0.0, 0, 0, 0.002824858757062147, 0.0, 0, 0]\n",
      "[0.005917159763313609, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.008875739644970414, 0.0, 0.012552301255230124, 0.3333333333333333, 0, 0, 0, 0]\n",
      "[0.0029585798816568047, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.005917159763313609, 0.0, 0, 0, 0.002824858757062147, 0.0, 0, 0]\n",
      "[0.011834319526627219, 0.16666666666666666, 0.016736401673640166, 0.0, 0.005649717514124294, 0.0, 0, 0]\n",
      "[0, 0, 0.012552301255230124, 0.3333333333333333, 0.00847457627118644, 0.0, 0, 0]\n",
      "[0, 0, 0.0041841004184100415, 0.0, 0, 0, 0, 0]\n",
      "[0.0029585798816568047, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.008875739644970414, 0.0, 0.008368200836820083, 1.0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.008875739644970414, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.008368200836820083, 0.0, 0.002824858757062147, 0.0, 0, 0, 0, 0]\n",
      "[0.005917159763313609, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.011834319526627219, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0.03765690376569037, 0.027777777777777776, 0.002824858757062147, 0.0, 0, 0]\n",
      "[0.023668639053254437, 0.0, 0, 0, 0.002824858757062147, 0.0, 0, 0]\n",
      "[0, 0, 0.002824858757062147, 0.0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.005917159763313609, 0.0, 0.008368200836820083, 0.0, 0.002824858757062147, 0.0, 0, 0]\n",
      "[0.0029585798816568047, 0.0, 0.008368200836820083, 0.0, 0, 0, 0, 0]\n",
      "[0.005917159763313609, 0.0, 0, 0, 0.04519774011299435, 0.08333333333333333, 0, 0]\n",
      "[0.008875739644970414, 0.0, 0.008368200836820083, 1.0, 0, 0, 0, 0]\n",
      "[0.005917159763313609, 0.0, 0.008368200836820083, 0.0, 0.002824858757062147, 0.0, 0, 0]\n",
      "[0.008875739644970414, 0.0, 0.008368200836820083, 1.0, 0, 0, 0, 0]\n",
      "[0.04437869822485207, 0.05714285714285714, 0.008368200836820083, 0.0, 0.014124293785310734, 0.2, 0, 0]\n",
      "[0.005917159763313609, 0.0, 0.008368200836820083, 0.0, 0.002824858757062147, 0.0, 0, 0]\n",
      "[0.005917159763313609, 0.0, 0, 0, 0.04519774011299435, 0.08333333333333333, 0, 0]\n",
      "[0.005917159763313609, 0.0, 0, 0, 0.011299435028248588, 0.0, 0, 0]\n",
      "[0.0029585798816568047, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.005917159763313609, 0.0, 0.008368200836820083, 1.0, 0.00847457627118644, 0.3333333333333333, 0, 0]\n",
      "[0.005917159763313609, 0.0, 0, 0, 0.01694915254237288, 0.06666666666666667, 0, 0]\n",
      "[0.005917159763313609, 0.0, 0.0041841004184100415, 0.0, 0.005649717514124294, 0.0, 0, 0]\n",
      "[0.005917159763313609, 0.0, 0.0041841004184100415, 0.0, 0.005649717514124294, 0.0, 0, 0]\n",
      "[0.005917159763313609, 0.0, 0, 0, 0.01694915254237288, 0.06666666666666667, 0, 0]\n",
      "[0, 0, 0, 0, 0.002824858757062147, 0.0, 0, 0]\n",
      "[0.0029585798816568047, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0.012552301255230124, 0.3333333333333333, 0.002824858757062147, 0.0, 0, 0]\n",
      "[0.023668639053254437, 0.0, 0, 0, 0.002824858757062147, 0.0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.005917159763313609, 0.0, 0, 0, 0.01694915254237288, 0.06666666666666667, 0, 0]\n",
      "[0.0029585798816568047, 0.0, 0.0041841004184100415, 0.0, 0.002824858757062147, 0.0, 0, 0]\n",
      "[0.011834319526627219, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.016736401673640166, 0.0, 0.00847457627118644, 0.0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.023668639053254437, 0.0, 0.008368200836820083, 0.0, 0.022598870056497175, 0.07142857142857142, 0, 0]\n",
      "[0.02071005917159763, 0.047619047619047616, 0, 0, 0.00847457627118644, 0.0, 0, 0]\n",
      "[0, 0, 0, 0, 0.002824858757062147, 0.0, 0, 0]\n",
      "[0.005917159763313609, 0.0, 0, 0, 0.005649717514124294, 1.0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0.002824858757062147, 0.0, 0, 0]\n",
      "[0.01775147928994083, 0.2, 0.025104602510460247, 0.06666666666666667, 0.002824858757062147, 0.0, 0, 0]\n",
      "[0.0041841004184100415, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.008368200836820083, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.0029585798816568047, 0.0, 0, 0, 0.002824858757062147, 0.0, 0, 0]\n",
      "[0.005917159763313609, 0.0, 0, 0, 0.005649717514124294, 1.0, 0, 0]\n",
      "[0.014124293785310734, 0.2, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0.0041841004184100415, 0.0, 0, 0, 0, 0]\n",
      "[0.023668639053254437, 0.0, 0.008368200836820083, 0.0, 0.022598870056497175, 0.07142857142857142, 0, 0]\n",
      "[0.005917159763313609, 0.0, 0.0041841004184100415, 0.0, 0.00847457627118644, 0.3333333333333333, 0, 0]\n",
      "[0.0029585798816568047, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.0029585798816568047, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.0041841004184100415, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.023668639053254437, 0.07142857142857142, 0.012552301255230124, 0.3333333333333333, 0.014124293785310734, 0.2, 0, 0]\n",
      "[0.0029585798816568047, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.002824858757062147, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.002824858757062147, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0.0041841004184100415, 0.0, 0, 0, 0, 0]\n",
      "[0.008368200836820083, 0.0, 0.014124293785310734, 0.2, 0, 0, 0, 0]\n",
      "[0, 0, 0.0041841004184100415, 0.0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.002824858757062147, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.002824858757062147, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.03954802259887005, 0.01098901098901099, 0, 0, 0, 0, 0, 0]\n",
      "[0.01694915254237288, 0.06666666666666667, 0, 0, 0, 0, 0, 0]\n",
      "[0.023668639053254437, 0.03571428571428571, 0.0041841004184100415, 0.0, 0.002824858757062147, 0.0, 0, 0]\n",
      "[0.008875739644970414, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.005649717514124294, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.28      0.43       217\n",
      "          1       0.08      0.82      0.15        17\n",
      "\n",
      "avg / total       0.89      0.32      0.41       234\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.84      0.89       217\n",
      "          1       0.13      0.29      0.18        17\n",
      "\n",
      "avg / total       0.88      0.80      0.84       234\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_vectors_mix = []\n",
    "test_vectors_mix = []\n",
    "\n",
    "training_vectors_cen = []\n",
    "test_vectors_cen = []\n",
    "\n",
    "training_indices = []\n",
    "test_indices = []\n",
    "\n",
    "\n",
    "\n",
    "for index in range(data_length):\n",
    "    li = []\n",
    "    oneid = ids[index]\n",
    "    origin_list = x[index].tolist()\n",
    "    starter = starter_dic[oneid]\n",
    "    forum_id = thread_forum_dic[oneid]\n",
    "    forum_index = forum_type_dic[forum_id]\n",
    "    for i in range(0, 4):\n",
    "        current_forum_index = forum_index - i\n",
    "        if current_forum_index <= 0:\n",
    "            li.append(0)\n",
    "            li.append(0)\n",
    "            continue\n",
    "        else:\n",
    "            g = forum_graph_dic[forum_id_list[current_forum_index]]\n",
    "            cent = nx.degree_centrality(g)\n",
    "            clu = nx.clustering(g)\n",
    "            #modified\n",
    "            if starter in cent:\n",
    "                li.append(cent[starter])\n",
    "                li.append(clu[starter])\n",
    "            else:\n",
    "                li.append(0)\n",
    "                li.append(0)\n",
    "\n",
    "\n",
    "    print(li)\n",
    "    new_list = origin_list + li\n",
    "\n",
    "    if forum_type_dic[thread_forum_dic[oneid]] <= middleInd:\n",
    "        training_vectors_mix.append(new_list)\n",
    "        training_vectors_cen.append(li)\n",
    "\n",
    "    else:\n",
    "        test_vectors_mix.append(new_list)\n",
    "        test_vectors_cen.append(li)\n",
    "\n",
    "\n",
    "\n",
    "#x = np.asarray(latest_vectors)      \n",
    "y = np.asarray(isreplied)\n",
    "\n",
    "'''\n",
    "kf = KFold(data_length, n_folds = 5, shuffle=False, random_state = 18)\n",
    "print(kf)\n",
    "for train_index, test_index in kf:\n",
    "    x_train, x_test = x[train_index], x[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "'''  \n",
    "LogReg = LogisticRegression(class_weight = 'balanced')\n",
    "LogReg.fit(training_vectors_cen, training_result)\n",
    "pred_result_cen = LogReg.predict(test_vectors_cen)\n",
    "with open('EDM.txt', 'w') as f:\n",
    "    print(classification_report(test_result, pred_result_cen), file = f)\n",
    "print(classification_report(test_result, pred_result_cen))\n",
    "\n",
    "\n",
    "\n",
    "LogReg = LogisticRegression(class_weight = 'balanced')\n",
    "LogReg.fit(training_vectors_mix, training_result)\n",
    "pred_result_mix = LogReg.predict(test_vectors_mix)\n",
    "with open('EDM.txt', 'w') as f:\n",
    "    print(classification_report(test_result, pred_result_mix), file = f)\n",
    "print(classification_report(test_result, pred_result_mix))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.13889355 -0.083231   -0.03194887 ...,  0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(LogReg.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{9400121: 0.05002841573897636, 3637530: 0.05029585798816568, 124313: 0.050345234093795294, 541944: 0.05067174870921526, 3677: 0.05091847698037227, 5534303: 0.0541665295266197, 13296001: 0.0546357099465717, 6935156: 0.05578220073918687, 11814913: 0.056224724808254786, 14925104: 0.057416267942583726, 5146477: 0.06220095693779904, 5781489: 0.06339174849146433, 8963841: 0.06440520749882908, 11822375: 0.06461726902821757, 15597251: 0.06472733610901375, 6206340: 0.06546896686195389, 15052810: 0.06687119284698288, 15731817: 0.07251127634182558, 3868043: 0.07309253463874824, 5621541: 0.0736970547922308, 15199263: 0.07828508096126294, 151006: 0.08952844593036649, 7470685: 0.09290358788901258, 3853969: 0.09725469749509735, 4404: 0.10377982912807587, 4375186: 0.10781750985420463, 15662154: 0.11940140946324501, 13157464: 0.1301775147928994, 14893116: 0.19333024323252423, 11069561: 0.23441599886532868}\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "\n",
    "user_cent_sum_dic = {}\n",
    "for forum_id in forum_graph_dic:\n",
    "    cent = nx.degree_centrality(forum_graph_dic[forum_id])\n",
    "    for user in cent:\n",
    "        if user in user_table and user_table[user] != 'Student':\n",
    "            continue\n",
    "            \n",
    "        if user not in user_cent_sum_dic:\n",
    "            user_cent_sum_dic[user] = 0\n",
    "        user_cent_sum_dic[user] += cent[user]\n",
    " \n",
    "user_cent_sum_dic = sorted(user_cent_sum_dic.items(), key=operator.itemgetter(1))\n",
    "good_student_dic = {}\n",
    "for tup in user_cent_sum_dic:\n",
    "    if tup[1] > 0.05:\n",
    "        good_student_dic[tup[0]] = tup[1]\n",
    "\n",
    "print(good_student_dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we try to run the code again but with extra truncation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# demographics of all threads\n",
    "course_name_set = set()\n",
    "course_id_set = set()\n",
    "course_num_weeks_dic = {}\n",
    "course_num_threads_dic = {}\n",
    "course_num_posts_dic = {}\n",
    "course_num_comments_dic = {}\n",
    "course_num_pos_threads_dic = {}\n",
    "course_num_neg_threads_dic = {}\n",
    "\n",
    "\n",
    "\n",
    "conn = util.create_connection(database)\n",
    "with conn:\n",
    "    cur = conn.cursor()\n",
    "    forum_message = 'select id, forumname, courseid, coursename, numthreads from forum'\n",
    "    cur.execute(forum_message)\n",
    "    forums = cur.fetchall()\n",
    "    for each_forum in forums:\n",
    "        \n",
    "        # All the info from this select operation\n",
    "        forum_id = each_forum[0]\n",
    "        forum_name = each_forum[1]\n",
    "        course_id = each_forum[2]\n",
    "        course_name = each_forum[3]\n",
    "        num_threads = each_forum[4]\n",
    "        course_name_set.add(course_name)\n",
    "        course_id_set.add(course_id)\n",
    "        if course_name not in course_num_threads_dic:\n",
    "            \n",
    "            course_num_weeks_dic[course_name] = 0\n",
    "            course_num_threads_dic[course_name] = 0\n",
    "            course_num_neg_threads_dic[course_name] = 0\n",
    "            course_num_pos_threads_dic[course_name] = 0\n",
    "            course_num_posts_dic[course_name] = 0\n",
    "            course_num_comments_dic[course_name] = 0\n",
    "        \n",
    "        if 'Week' in forum_name:\n",
    "            \n",
    "            course_num_weeks_dic[course_name] += 1\n",
    "            course_num_threads_dic[course_name] += num_threads\n",
    "            thread_message = 'select inst_replied from thread where forumid == \\''\n",
    "            thread_message += forum_id\n",
    "            thread_message += '\\''\n",
    "            cur.execute(thread_message)\n",
    "            firsts = cur.fetchall()\n",
    "            for inst in firsts:\n",
    "                if inst[0] == 1:\n",
    "                    course_num_pos_threads_dic[course_name] += 1\n",
    "                else:\n",
    "                    course_num_neg_threads_dic[course_name] += 1\n",
    "\n",
    "            post_message = 'select * from post where forumid == \\''\n",
    "            post_message += forum_id\n",
    "            post_message += '\\''\n",
    "            cur.execute(post_message)\n",
    "            seconds = cur.fetchall()\n",
    "            course_num_posts_dic[course_name] += len(seconds)\n",
    "            \n",
    "            comment_message = 'select * from comment where forumid == \\''\n",
    "            comment_message += forum_id\n",
    "            comment_message += '\\''\n",
    "            cur.execute(comment_message)\n",
    "            thirds = cur.fetchall()\n",
    "            course_num_comments_dic[course_name] += len(thirds)\n",
    "\n",
    "\n",
    "        \n",
    "course_num_threads_dic = sorted(course_num_threads_dic.items(), key=operator.itemgetter(1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wharton-operations-analytics; num of threads: 0; num of pos: 0; num of neg: 0; num of posts: 0\n",
      "corporate-entrepreneurs-opportunity; num of threads: 1; num of pos: 0; num of neg: 1; num of posts: 2\n",
      "motion-and-kinetics; num of threads: 1; num of pos: 1; num of neg: 0; num of posts: 6\n",
      "wharton-people-analytics; num of threads: 1; num of pos: 0; num of neg: 1; num of posts: 1\n",
      "cloud-applications-part1; num of threads: 2; num of pos: 0; num of neg: 2; num of posts: 36\n",
      "accounting-analytics; num of threads: 3; num of pos: 1; num of neg: 2; num of posts: 6\n",
      "competitive-strategy; num of threads: 3; num of pos: 3; num of neg: 0; num of posts: 6\n",
      "photography-techniques; num of threads: 3; num of pos: 0; num of neg: 3; num of posts: 15\n",
      "text-mining; num of threads: 3; num of pos: 2; num of neg: 1; num of posts: 7\n",
      "typography; num of threads: 3; num of pos: 0; num of neg: 3; num of posts: 5\n",
      "weight-loss-plan; num of threads: 3; num of pos: 0; num of neg: 3; num of posts: 271\n",
      "how-to-write-a-resume; num of threads: 4; num of pos: 0; num of neg: 4; num of posts: 59\n",
      "professional-emails-english; num of threads: 4; num of pos: 0; num of neg: 4; num of posts: 283\n",
      "advanced-excel; num of threads: 5; num of pos: 0; num of neg: 5; num of posts: 119\n",
      "datavisualization; num of threads: 5; num of pos: 0; num of neg: 5; num of posts: 22\n",
      "photo-composition; num of threads: 5; num of pos: 0; num of neg: 5; num of posts: 6\n",
      "teach-light-color; num of threads: 5; num of pos: 1; num of neg: 4; num of posts: 8\n",
      "bioinformatics; num of threads: 6; num of pos: 0; num of neg: 6; num of posts: 9\n",
      "image-making; num of threads: 6; num of pos: 1; num of neg: 5; num of posts: 143\n",
      "antimicrobial-resistance; num of threads: 7; num of pos: 5; num of neg: 2; num of posts: 30\n",
      "cs-tech-interview; num of threads: 7; num of pos: 0; num of neg: 7; num of posts: 48\n",
      "excel-analysis; num of threads: 7; num of pos: 1; num of neg: 6; num of posts: 240\n",
      "ignite-creativity; num of threads: 7; num of pos: 1; num of neg: 6; num of posts: 450\n",
      "algorithmic-thinking-1; num of threads: 8; num of pos: 8; num of neg: 0; num of posts: 20\n",
      "camera-control; num of threads: 8; num of pos: 3; num of neg: 5; num of posts: 15\n",
      "decision-making; num of threads: 8; num of pos: 0; num of neg: 8; num of posts: 585\n",
      "clinical-terminology; num of threads: 9; num of pos: 2; num of neg: 7; num of posts: 40\n",
      "digital-analytics; num of threads: 9; num of pos: 1; num of neg: 8; num of posts: 145\n",
      "engineering-mechanics-statics; num of threads: 9; num of pos: 6; num of neg: 3; num of posts: 26\n",
      "plato-dialogues; num of threads: 9; num of pos: 0; num of neg: 9; num of posts: 16\n",
      "business-english-intro; num of threads: 10; num of pos: 0; num of neg: 10; num of posts: 403\n",
      "how-to-write-a-scientific-paper; num of threads: 10; num of pos: 0; num of neg: 10; num of posts: 186\n",
      "neuromarketing; num of threads: 10; num of pos: 0; num of neg: 10; num of posts: 28\n",
      "fundamentals-of-graphic-design; num of threads: 11; num of pos: 1; num of neg: 10; num of posts: 23\n",
      "negotiation-skills; num of threads: 11; num of pos: 2; num of neg: 9; num of posts: 39\n",
      "personal-branding; num of threads: 11; num of pos: 3; num of neg: 8; num of posts: 772\n",
      "sales-strategies; num of threads: 11; num of pos: 0; num of neg: 11; num of posts: 809\n",
      "advanced-data-structures; num of threads: 12; num of pos: 3; num of neg: 9; num of posts: 47\n",
      "how-to-create-a-website; num of threads: 12; num of pos: 0; num of neg: 12; num of posts: 322\n",
      "wharton-contagious-viral-marketing; num of threads: 12; num of pos: 1; num of neg: 11; num of posts: 15\n",
      "wharton-customer-analytics; num of threads: 12; num of pos: 3; num of neg: 9; num of posts: 20\n",
      "android-app; num of threads: 13; num of pos: 4; num of neg: 9; num of posts: 642\n",
      "java-programming-arrays-lists-data; num of threads: 13; num of pos: 9; num of neg: 4; num of posts: 34\n",
      "prresearch; num of threads: 13; num of pos: 10; num of neg: 3; num of posts: 40\n",
      "american-law; num of threads: 14; num of pos: 13; num of neg: 1; num of posts: 37\n",
      "cancer; num of threads: 14; num of pos: 9; num of neg: 5; num of posts: 30\n",
      "clinical-trials; num of threads: 14; num of pos: 0; num of neg: 14; num of posts: 236\n",
      "data-structures; num of threads: 14; num of pos: 1; num of neg: 13; num of posts: 520\n",
      "guitar; num of threads: 14; num of pos: 0; num of neg: 14; num of posts: 33\n",
      "journalism; num of threads: 14; num of pos: 10; num of neg: 4; num of posts: 189\n",
      "microeconomics; num of threads: 14; num of pos: 0; num of neg: 14; num of posts: 305\n",
      "big-data-management; num of threads: 15; num of pos: 3; num of neg: 12; num of posts: 417\n",
      "excel-data-analysis; num of threads: 15; num of pos: 4; num of neg: 11; num of posts: 35\n",
      "ml-clustering-and-retrieval; num of threads: 15; num of pos: 13; num of neg: 2; num of posts: 37\n",
      "plantknows; num of threads: 15; num of pos: 10; num of neg: 5; num of posts: 15\n",
      "cyber-security-domain; num of threads: 16; num of pos: 8; num of neg: 8; num of posts: 36\n",
      "global-diplomacy; num of threads: 16; num of pos: 3; num of neg: 13; num of posts: 46\n",
      "hadoop; num of threads: 16; num of pos: 3; num of neg: 13; num of posts: 215\n",
      "html; num of threads: 16; num of pos: 0; num of neg: 16; num of posts: 52\n",
      "intro-programming; num of threads: 16; num of pos: 11; num of neg: 5; num of posts: 48\n",
      "marketing-digital; num of threads: 16; num of pos: 4; num of neg: 12; num of posts: 135\n",
      "supply-chain-logistics; num of threads: 16; num of pos: 1; num of neg: 15; num of posts: 934\n",
      "build-a-computer; num of threads: 17; num of pos: 12; num of neg: 5; num of posts: 77\n",
      "data-visualization-tableau; num of threads: 17; num of pos: 2; num of neg: 15; num of posts: 709\n",
      "introcss; num of threads: 17; num of pos: 6; num of neg: 11; num of posts: 42\n",
      "learn-chinese; num of threads: 17; num of pos: 9; num of neg: 8; num of posts: 64\n",
      "photography; num of threads: 17; num of pos: 7; num of neg: 10; num of posts: 333\n",
      "algorithms-greedy; num of threads: 18; num of pos: 0; num of neg: 18; num of posts: 49\n",
      "erasmus-econometrics; num of threads: 18; num of pos: 8; num of neg: 10; num of posts: 99\n",
      "exposure-photography; num of threads: 18; num of pos: 12; num of neg: 6; num of posts: 36\n",
      "logical-fallacies; num of threads: 18; num of pos: 0; num of neg: 18; num of posts: 231\n",
      "moralities; num of threads: 18; num of pos: 4; num of neg: 14; num of posts: 56\n",
      "single-variable-calculus; num of threads: 18; num of pos: 18; num of neg: 0; num of posts: 48\n",
      "systematic-review; num of threads: 18; num of pos: 12; num of neg: 6; num of posts: 42\n",
      "website-coding; num of threads: 18; num of pos: 16; num of neg: 2; num of posts: 53\n",
      "algorithms-divide-conquer; num of threads: 19; num of pos: 4; num of neg: 15; num of posts: 49\n",
      "food-and-health; num of threads: 19; num of pos: 13; num of neg: 6; num of posts: 66\n",
      "gamification; num of threads: 19; num of pos: 9; num of neg: 10; num of posts: 585\n",
      "magic-middle-ages; num of threads: 19; num of pos: 1; num of neg: 18; num of posts: 81\n",
      "c-plus-plus-a; num of threads: 20; num of pos: 11; num of neg: 9; num of posts: 43\n",
      "ml-classification; num of threads: 20; num of pos: 9; num of neg: 11; num of posts: 34\n",
      "bayesian-statistics; num of threads: 21; num of pos: 5; num of neg: 16; num of posts: 196\n",
      "big-data-machine-learning; num of threads: 21; num of pos: 4; num of neg: 17; num of posts: 377\n",
      "r-programming-environment; num of threads: 21; num of pos: 0; num of neg: 21; num of posts: 40\n",
      "modern-art-ideas; num of threads: 22; num of pos: 9; num of neg: 13; num of posts: 596\n",
      "uva-darden-market-analytics; num of threads: 22; num of pos: 12; num of neg: 10; num of posts: 350\n",
      "uva-darden-project-management; num of threads: 22; num of pos: 9; num of neg: 13; num of posts: 82\n",
      "algorithms-on-graphs; num of threads: 23; num of pos: 0; num of neg: 23; num of posts: 217\n",
      "financial-markets; num of threads: 23; num of pos: 0; num of neg: 23; num of posts: 82\n",
      "research-methods; num of threads: 23; num of pos: 19; num of neg: 4; num of posts: 375\n",
      "analytics-tableau; num of threads: 24; num of pos: 10; num of neg: 14; num of posts: 326\n",
      "develop-your-musicianship; num of threads: 24; num of pos: 15; num of neg: 9; num of posts: 60\n",
      "inferential-statistics-intro; num of threads: 25; num of pos: 23; num of neg: 2; num of posts: 94\n",
      "mafash; num of threads: 25; num of pos: 8; num of neg: 17; num of posts: 111\n",
      "parprog1; num of threads: 25; num of pos: 12; num of neg: 13; num of posts: 89\n",
      "mandarin-chinese-1; num of threads: 26; num of pos: 6; num of neg: 20; num of posts: 652\n",
      "ml-regression; num of threads: 26; num of pos: 5; num of neg: 21; num of posts: 118\n",
      "english-principles; num of threads: 27; num of pos: 2; num of neg: 25; num of posts: 62\n",
      "java-for-android; num of threads: 28; num of pos: 24; num of neg: 4; num of posts: 76\n",
      "bayesian; num of threads: 29; num of pos: 15; num of neg: 14; num of posts: 139\n",
      "law-student; num of threads: 29; num of pos: 0; num of neg: 29; num of posts: 225\n",
      "business; num of threads: 31; num of pos: 8; num of neg: 23; num of posts: 109\n",
      "careerdevelopment; num of threads: 31; num of pos: 12; num of neg: 19; num of posts: 117\n",
      "analytics-business-metrics; num of threads: 34; num of pos: 20; num of neg: 14; num of posts: 905\n",
      "understanding-arguments; num of threads: 34; num of pos: 0; num of neg: 34; num of posts: 3677\n",
      "analytics-excel; num of threads: 35; num of pos: 22; num of neg: 13; num of posts: 458\n",
      "memory-and-movies; num of threads: 35; num of pos: 29; num of neg: 6; num of posts: 185\n",
      "dsp; num of threads: 36; num of pos: 29; num of neg: 7; num of posts: 81\n",
      "game-development; num of threads: 36; num of pos: 3; num of neg: 33; num of posts: 144\n",
      "single-page-web-apps-with-angularjs; num of threads: 36; num of pos: 28; num of neg: 8; num of posts: 85\n",
      "robotics-flight; num of threads: 39; num of pos: 3; num of neg: 36; num of posts: 86\n",
      "database-management; num of threads: 40; num of pos: 0; num of neg: 40; num of posts: 76\n",
      "classical-composition; num of threads: 41; num of pos: 2; num of neg: 39; num of posts: 147\n",
      "public-relations; num of threads: 42; num of pos: 14; num of neg: 28; num of posts: 158\n",
      "crypto; num of threads: 43; num of pos: 30; num of neg: 13; num of posts: 161\n",
      "big-data-introduction; num of threads: 44; num of pos: 22; num of neg: 22; num of posts: 2084\n",
      "probabilistic-graphical-models; num of threads: 44; num of pos: 10; num of neg: 34; num of posts: 126\n",
      "teach-online; num of threads: 45; num of pos: 27; num of neg: 18; num of posts: 859\n",
      "game-programming; num of threads: 53; num of pos: 8; num of neg: 45; num of posts: 148\n",
      "probability-intro; num of threads: 54; num of pos: 43; num of neg: 11; num of posts: 247\n",
      "analytics-mysql; num of threads: 58; num of pos: 21; num of neg: 37; num of posts: 199\n",
      "android-programming; num of threads: 59; num of pos: 2; num of neg: 57; num of posts: 136\n",
      "ml-foundations; num of threads: 59; num of pos: 15; num of neg: 44; num of posts: 293\n",
      "programming-languages; num of threads: 62; num of pos: 30; num of neg: 32; num of posts: 369\n",
      "science-of-meditation; num of threads: 67; num of pos: 56; num of neg: 11; num of posts: 249\n",
      "uva-darden-design-thinking-innovation; num of threads: 69; num of pos: 35; num of neg: 34; num of posts: 220\n",
      "interactive-python-1; num of threads: 72; num of pos: 22; num of neg: 50; num of posts: 415\n",
      "algorithmic-toolbox; num of threads: 80; num of pos: 31; num of neg: 49; num of posts: 448\n",
      "object-oriented-java; num of threads: 81; num of pos: 21; num of neg: 60; num of posts: 482\n",
      "python-databases; num of threads: 103; num of pos: 30; num of neg: 73; num of posts: 482\n",
      "algorithm-design-analysis; num of threads: 114; num of pos: 21; num of neg: 93; num of posts: 418\n",
      "algorithms-part1; num of threads: 133; num of pos: 25; num of neg: 108; num of posts: 336\n",
      "calculus1; num of threads: 150; num of pos: 74; num of neg: 76; num of posts: 315\n",
      "html-css-javascript; num of threads: 151; num of pos: 3; num of neg: 148; num of posts: 441\n",
      "python-network-data; num of threads: 205; num of pos: 145; num of neg: 60; num of posts: 564\n",
      "learning-how-to-learn; num of threads: 238; num of pos: 49; num of neg: 189; num of posts: 1127\n",
      "python; num of threads: 263; num of pos: 171; num of neg: 92; num of posts: 804\n",
      "hybrid-mobile-development; num of threads: 357; num of pos: 75; num of neg: 282; num of posts: 1013\n",
      "server-side-development; num of threads: 526; num of pos: 190; num of neg: 336; num of posts: 1419\n",
      "web-frameworks; num of threads: 538; num of pos: 189; num of neg: 349; num of posts: 1688\n",
      "angular-js; num of threads: 784; num of pos: 142; num of neg: 642; num of posts: 2242\n",
      "machine-learning; num of threads: 2177; num of pos: 1486; num of neg: 691; num of posts: 6060\n"
     ]
    }
   ],
   "source": [
    "course_column = []\n",
    "week_column = []\n",
    "thread_column = []\n",
    "pos_column = []\n",
    "neg_column = []\n",
    "post_column = []\n",
    "comment_column = []\n",
    "ratio_column = []\n",
    "\n",
    "for tup in course_num_threads_dic:\n",
    "    print(tup[0] + '; num of threads: ' + str(tup[1]) + '; num of pos: ' + str(course_num_pos_threads_dic[tup[0]]) + '; num of neg: ' + str(course_num_neg_threads_dic[tup[0]]) + '; num of posts: ' + str(course_num_posts_dic[tup[0]]))\n",
    "    course_column.append(tup[0])\n",
    "    week_column.append(course_num_weeks_dic[tup[0]])\n",
    "    thread_column.append(tup[1])\n",
    "    pos_column.append(course_num_pos_threads_dic[tup[0]])\n",
    "    neg_column.append(course_num_neg_threads_dic[tup[0]])\n",
    "    post_column.append(course_num_posts_dic[tup[0]])\n",
    "    comment_column.append(course_num_comments_dic[tup[0]])\n",
    "    if course_num_neg_threads_dic[tup[0]] == 0:\n",
    "        ratio_column.append('N')\n",
    "    else:\n",
    "        ratio_column.append(float(course_num_pos_threads_dic[tup[0]]) / course_num_neg_threads_dic[tup[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dic = {}\n",
    "\n",
    "data_dic['course'] = course_column\n",
    "data_dic['week'] = week_column\n",
    "data_dic['threads'] = thread_column\n",
    "data_dic['pos_threads'] = pos_column\n",
    "data_dic['neg_threads'] = neg_column\n",
    "data_dic['ratio'] = ratio_column\n",
    "data_dic['posts'] = post_column\n",
    "data_dic['comments'] = comment_column\n",
    "\n",
    "\n",
    "df = DataFrame(data_dic)\n",
    "writer = pd.ExcelWriter('demographical_data.xlsx', engine='xlsxwriter')\n",
    "df.to_excel(writer, sheet_name='Sheet1')\n",
    "writer.save()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "bower does not work\n",
    "\n",
    "<co-content><text>Hi Guys,</text><text>I'm trying to install angular route ( I'm working on windows ), but when I type in 'bower install....' then I get the error message \"bower is not recognised as an internal or external command, operable program or batch file\". Even when I type in the full path to where bower is, I get the same message.</text><text>Does anyone know what is going on?</text><text>Thankx!</text><text>Niki</text></co-content>\n",
    "\n",
    "<co-content><text>You have probably have not installed bower globally</text><text>Try to open cmd and type: <strong>npm install -g bower </strong></text><text>Make sure you are running the command at the location that bower.json lives / presents.</text></co-content>\n",
    "\n",
    "<co-content><text>yup, that did the trick.</text><text/><text>Thank you!</text></co-content>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
