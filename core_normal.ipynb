{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Please create a new folder named \"all\" to store all txt files of threads in the original folder.\n",
    "from bs4 import BeautifulSoup, Tag\n",
    "from collections import OrderedDict\n",
    "from pathlib import Path\n",
    "import os\n",
    "import nltk\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "from pandas import Series, DataFrame\n",
    "from pylab import rcParams\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics \n",
    "from sklearn.metrics import classification_report\n",
    "from graph_constructor import get_graph, get_digraph, get_graph_from_list, get_chronological_clustering_coef, get_chronological_pgrank\n",
    "# from key_student_extractor import get_key_student\n",
    "from preprocess import preprocess_text\n",
    "import networkx as nx\n",
    "import util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#html-css-javascript LgWwihnoEeWDtQoum3sFeQ\n",
    "#python_database eQJvsjn9EeWJaxK5AT4frw\n",
    "#python_network Y4DUPDpQEeWO-Qq6rEZAow\n",
    "#python 7A1yFTaREeWWBQrVFXqd1w\n",
    "#hybrid-mobile-development -gcU5xn4EeWwrBKfKrqlSQ\n",
    "#machine-learning Gtv4Xb1-EeS-ViIACwYKVQ\n",
    "#learning-how-to-learn GdeNrll1EeSROyIACtiVvg\n",
    "#angular-js 52blABnqEeW9dA4X94-nLQ\n",
    "#server-side-development ngZrURn5EeWwrBKfKrqlSQ\n",
    "#web-frameworks ycQnChn3EeWDtQoum3sFeQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef get_vectors_with_format_of_training_data(training_vectors, tfidf_training_model, testing_vectors, tfidf_testing_model):\\n    \\n    # word-index dictionaries\\n    training_term_index_dic = tfidf_training_model.vocabulary_\\n    testing_term_index_dic = tfidf_testing_model.vocabulary_\\n    \\n    # revert term dic\\n    training_index_term_dic = {}\\n    for term in training_term_index_dic:\\n        index = training_term_index_dic[term]\\n        training_index_term_dic[index] = term\\n        \\n    testing_index_term_dic = {}\\n    for term in testing_term_index_dic:\\n        index = testing_term_index_dic[term]\\n        testing_index_term_dic[index] = term\\n        \\n    training_vectors_as_list = training_vectors.tolist()\\n    combined_vector = training_vectors_as_list\\n    # iterate each word in the training vector. If we find the word in the testing dataset, assign the value. Otherwise\\n    # assign 0.\\n    for testing_vector in testing_vectors:\\n        current_vector_as_list = []\\n        for training_index in range (0, training_vectors.shape[1]):\\n            if training_index not in training_index_term_dic:\\n                print(\"Fatal error!!!\")\\n            else:\\n                term = training_index_term_dic[training_index]\\n                \\n                if term in testing_term_index_dic:\\n                    testing_index = testing_term_index_dic[term]\\n                    current_value = testing_vector[testing_index]\\n                    current_vector_as_list.append(current_value)\\n                    \\n                else:\\n                    current_vector_as_list.append(0)\\n\\n        combined_vector.append(current_vector_as_list)\\n        \\n    return np.asarray(combined_vector)\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def is_word(string):\n",
    "    for char in string:\n",
    "        if not char.isalpha():\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def get_bit_list(length, index):\n",
    "    \n",
    "    bit_list = []\n",
    "    for i in range(length):\n",
    "        if not i == index:\n",
    "            bit_list.append(0)\n",
    "        else:\n",
    "            bit_list.append(1)\n",
    "    \n",
    "    return bit_list\n",
    "\n",
    "def tf_idf(docs, queries, tokenizer):\n",
    "    \"\"\"\n",
    "    performs TF-IDF vectorization for documents and queries\n",
    "    Parameters\n",
    "        ----------\n",
    "        docs : list\n",
    "            list of documents\n",
    "        queries : list\n",
    "            list of queries\n",
    "        tokenizer : custom tokenizer function\n",
    "    Returns\n",
    "    -------\n",
    "    tfs : sparse array,\n",
    "        tfidf vectors for documents. Each row corresponds to a document.\n",
    "    tfs_query: sparse array,\n",
    "        tfidf vectors for queries. Each row corresponds to a query.\n",
    "    dictionary: list\n",
    "        sorted dictionary\n",
    "    \"\"\"\n",
    "    model = str.maketrans(dict.fromkeys(string.punctuation))\n",
    "    processed_docs = [d.lower().translate(model) for d in docs]\n",
    "    processed_queries = [d.lower().translate(model) for d in queries]\n",
    "    tfidf = TfidfVectorizer(stop_words='english', tokenizer=tokenizer)\n",
    "    tfs = tfidf.fit_transform(processed_docs)\n",
    "    tfs_query = tfidf.transform(processed_queries)\n",
    "    return tfs, tfs_query, tfidf\n",
    "\n",
    "def tokenize_text(docs):\n",
    "    \"\"\"\n",
    "    custom tokenization function given a list of documents\n",
    "    Parameters\n",
    "        ----------\n",
    "        docs : string\n",
    "            a document\n",
    "    Returns\n",
    "    -------\n",
    "    stems : list\n",
    "        list of tokens\n",
    "    \"\"\"\n",
    "\n",
    "    text = ''\n",
    "    for d in docs:\n",
    "        text += '' + d\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stems = []\n",
    "    for item in tokens:\n",
    "        if is_word(item):\n",
    "            stems.append(stemmer.stem(item))\n",
    "    return stems\n",
    "\n",
    "def get_vectors(texts, queries):\n",
    "    descriptions = []\n",
    "    descriptions.append('')\n",
    "    vec_docs, vec_queries, tfidf_model = tf_idf(texts, queries, tokenize_text)\n",
    "    return vec_docs, vec_queries, tfidf_model\n",
    "\n",
    "def graph_of_thread(thread_id):\n",
    "    \n",
    "    graph = nx.DiGraph()\n",
    "    posts = thread_post_set_dic[thread_id]\n",
    "    \n",
    "    for each_post_id in posts:\n",
    "        graph.add_edge(each_post_id, thread_id)\n",
    "        comments = post_comment_set_dic[each_post_id]\n",
    "        for each_comment_id in comments:\n",
    "            graph.add_edge(each_comment_id, each_post_id)\n",
    "\n",
    "\n",
    "    return graph\n",
    "\n",
    "# Rubbish function\n",
    "'''\n",
    "def get_vectors_with_format_of_training_data(training_vectors, tfidf_training_model, testing_vectors, tfidf_testing_model):\n",
    "    \n",
    "    # word-index dictionaries\n",
    "    training_term_index_dic = tfidf_training_model.vocabulary_\n",
    "    testing_term_index_dic = tfidf_testing_model.vocabulary_\n",
    "    \n",
    "    # revert term dic\n",
    "    training_index_term_dic = {}\n",
    "    for term in training_term_index_dic:\n",
    "        index = training_term_index_dic[term]\n",
    "        training_index_term_dic[index] = term\n",
    "        \n",
    "    testing_index_term_dic = {}\n",
    "    for term in testing_term_index_dic:\n",
    "        index = testing_term_index_dic[term]\n",
    "        testing_index_term_dic[index] = term\n",
    "        \n",
    "    training_vectors_as_list = training_vectors.tolist()\n",
    "    combined_vector = training_vectors_as_list\n",
    "    # iterate each word in the training vector. If we find the word in the testing dataset, assign the value. Otherwise\n",
    "    # assign 0.\n",
    "    for testing_vector in testing_vectors:\n",
    "        current_vector_as_list = []\n",
    "        for training_index in range (0, training_vectors.shape[1]):\n",
    "            if training_index not in training_index_term_dic:\n",
    "                print(\"Fatal error!!!\")\n",
    "            else:\n",
    "                term = training_index_term_dic[training_index]\n",
    "                \n",
    "                if term in testing_term_index_dic:\n",
    "                    testing_index = testing_term_index_dic[term]\n",
    "                    current_value = testing_vector[testing_index]\n",
    "                    current_vector_as_list.append(current_value)\n",
    "                    \n",
    "                else:\n",
    "                    current_vector_as_list.append(0)\n",
    "\n",
    "        combined_vector.append(current_vector_as_list)\n",
    "        \n",
    "    return np.asarray(combined_vector)\n",
    "'''\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.57973867  0.81480247  0.        ]\n",
      " [ 0.6316672   0.44943642  0.          0.6316672 ]]\n",
      "{'love': 2, 'dog': 1, 'best': 0, 'world': 3}\n",
      "[[ 0.          0.          1.          0.        ]\n",
      " [ 0.          0.57973867  0.          0.81480247]]\n"
     ]
    }
   ],
   "source": [
    "# testing the new function \n",
    "\n",
    "text_a = 'I love my dog'\n",
    "text_b = 'My dog is the best in the world'\n",
    "text_c = 'I love my cat'\n",
    "text_d = 'The world cup is dogs'\n",
    "\n",
    "training_texts = []\n",
    "testing_texts = []\n",
    "\n",
    "training_texts.append(text_a)\n",
    "training_texts.append(text_b)\n",
    "testing_texts.append(text_c)\n",
    "testing_texts.append(text_d)\n",
    "\n",
    "training_vectors, testing_vectors, model = get_vectors(training_texts, testing_texts)\n",
    "\n",
    "training_vectors = training_vectors.toarray()\n",
    "testing_vectors = testing_vectors.toarray()\n",
    "\n",
    "print(training_vectors)\n",
    "print(model.vocabulary_)\n",
    "print(testing_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "please input the database name: courseraData.db\n"
     ]
    }
   ],
   "source": [
    "database = input(\"please input the database name: \")\n",
    "\n",
    "user_table = {}\n",
    "\n",
    "conn = util.create_connection(database)\n",
    "with conn:\n",
    "    cur = conn.cursor() \n",
    "    cur.execute(\"SELECT id, user_title FROM user\")\n",
    "    rows = cur.fetchall()\n",
    "    for each_user in rows:\n",
    "        user_id = each_user[0]\n",
    "        user_title = each_user[1]\n",
    "        user_table[user_id] = user_title\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# list with the same data length\n",
    "\n",
    "## texts\n",
    "texts = []\n",
    "training_texts = []\n",
    "testing_texts = []\n",
    "\n",
    "## ids\n",
    "ids = []\n",
    "training_ids = []\n",
    "testing_ids = []\n",
    "\n",
    "isreplied= []\n",
    "id_to_index = {}\n",
    "\n",
    "# for forum id alone\n",
    "forum_id_list = []\n",
    "\n",
    "# raw text dic\n",
    "thread_text_dic = {}\n",
    "post_text_dic = {}\n",
    "comment_text_dic = {}\n",
    "\n",
    "# container dic\n",
    "forum_type_dic = {}\n",
    "forum_graph_dic = {}\n",
    "forum_digraph_dic = {}\n",
    "thread_post_set_dic = {}\n",
    "post_comment_set_dic = {}\n",
    "thread_forum_dic = {}\n",
    "\n",
    "# feature dic\n",
    "num_of_post_dic = {}\n",
    "num_of_comment_dic = {}\n",
    "num_of_sen_dic = {}\n",
    "num_of_comment_for_post_dic = {}\n",
    "num_of_url_dic = {}\n",
    "num_of_timeref_dic = {}\n",
    "num_of_votes_dic = {}\n",
    "is_replied = {}\n",
    "starter_dic = {}\n",
    "\n",
    "# truncation\n",
    "thread_intervened_time_dic = {}\n",
    "thread_posted_time_dic = {}\n",
    "\n",
    "# problem thread\n",
    "replied_thread_without_reply_time = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "please input your course_id: ngZrURn5EeWwrBKfKrqlSQ\n",
      "please press 'a' for manual, 'b' for semi-auto scan, others for auto: q\n"
     ]
    }
   ],
   "source": [
    "course_id = input(\"please input your course_id: \")\n",
    "\n",
    "input_method = input(\"please press 'a' for manual, 'b' for semi-auto scan, others for auto: \")\n",
    "if input_method == 'a':\n",
    "    num_forum = input(\"please input the number of forums: \")\n",
    "    print(\"please input the forum ids in chronological order\")\n",
    "    for i in range(0, int(num_forum)):\n",
    "        new_forum_id = input(\"please input the current forum id: \")\n",
    "        forum_id_list.append(new_forum_id)\n",
    "        forum_type_dic[new_forum_id] = i\n",
    "        forum_graph_dic[new_forum_id] = get_graph(new_forum_id, database)\n",
    "        forum_digraph_dic[new_forum_id] = get_digraph(new_forum_id, database)\n",
    "\n",
    "elif input_method == 'b':\n",
    "    path = 'text/' + course_id + \"/\" + \"forum.txt\"\n",
    "    file = open(path, \"r\")\n",
    "    num_forum = int(file.readline())\n",
    "    num_forum = input(\"please input the number of forums: \")\n",
    "    for i in range (0, int(num_forum)):\n",
    "        new_forum_id = file.readline()\n",
    "        new_forum_id = new_forum_id.rstrip(os.linesep)\n",
    "        forum_id_list.append(new_forum_id)\n",
    "        forum_type_dic[new_forum_id] = i\n",
    "        forum_graph_dic[new_forum_id] = get_graph(new_forum_id, database)\n",
    "        forum_digraph_dic[new_forum_id] = get_digraph(new_forum_id, database)\n",
    "\n",
    "else:\n",
    "    path = 'text/' + course_id + \"/\" + \"forum.txt\"\n",
    "    file = open(path, \"r\")\n",
    "    num_forum = int(file.readline())\n",
    "    for i in range (0, num_forum):\n",
    "        new_forum_id = file.readline()\n",
    "        new_forum_id = new_forum_id.rstrip(os.linesep)\n",
    "        forum_id_list.append(new_forum_id)\n",
    "        forum_type_dic[new_forum_id] = i\n",
    "        forum_graph_dic[new_forum_id] = get_graph(new_forum_id, database)\n",
    "        forum_digraph_dic[new_forum_id] = get_digraph(new_forum_id, database)\n",
    "\n",
    "middleInd = int(input(\"please input the training set scale: \"))\n",
    "\n",
    "limit_message = \"WHERE courseid = \\'\"\n",
    "limit_message += course_id\n",
    "limit_message += \"\\'\"\n",
    "if int(num_forum) >= 1:\n",
    "    limit_message += \" AND (forumid = \\'\"\n",
    "    limit_message += forum_id_list[0]\n",
    "    limit_message += \"\\'\"\n",
    "\n",
    "for i in range(1, int(num_forum)):\n",
    "    limit_message += \" OR forumid = \\'\"\n",
    "    limit_message += forum_id_list[i]\n",
    "    limit_message += \"\\'\"\n",
    "\n",
    "if int(num_forum) >= 1:\n",
    "    limit_message += \")\"\n",
    "    \n",
    "print(\"done preprocessing!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT id, title, inst_replied, starter, forumid, posted_time FROM thread WHERE courseid = 'GdeNrll1EeSROyIACtiVvg' AND (forumid = 'GdeNrll1EeSROyIACtiVvg' OR forumid = 'm4nU0ygeEeaZ8Apto8QB_w' OR forumid = 'm5T43ygeEea7jBLLHPwd0w') ORDER BY posted_time\n",
      "SELECT id, thread_id, post_id, user, comment_text, post_time FROM comment WHERE courseid = 'GdeNrll1EeSROyIACtiVvg' AND (forumid = 'GdeNrll1EeSROyIACtiVvg' OR forumid = 'm4nU0ygeEeaZ8Apto8QB_w' OR forumid = 'm5T43ygeEea7jBLLHPwd0w') ORDER BY post_time\n",
      "101\n"
     ]
    }
   ],
   "source": [
    "# extract data from db.\n",
    "ids = []\n",
    "training_ids = []\n",
    "testing_ids = []\n",
    "\n",
    "conn = util.create_connection(database)\n",
    "with conn:\n",
    "    cur = conn.cursor()\n",
    "    thread_message = 'SELECT id, title, inst_replied, starter, forumid, posted_time FROM thread '\n",
    "    thread_message += limit_message\n",
    "    thread_message += ' ORDER BY posted_time'\n",
    "    print(thread_message)\n",
    "    cur.execute(thread_message)\n",
    "    threads = cur.fetchall()\n",
    "    for each_thread in threads:\n",
    "        \n",
    "        # All the info from this select operation\n",
    "        thread_id = each_thread[0]\n",
    "        thread_title = each_thread[1]\n",
    "        thread_intervention = each_thread[2]\n",
    "        thread_starter = each_thread[3]\n",
    "        thread_forum_id = each_thread[4]\n",
    "        thread_time = each_thread[5]\n",
    "        \n",
    "        # Initialize intervention time as -1\n",
    "        thread_intervened_time_dic[thread_id] = -1\n",
    "        if(thread_starter in user_table and (user_table[thread_starter] == 'Instructor' or user_table[thread_starter] == 'Staff')):           \n",
    "            continue\n",
    "\n",
    "        # Detect the earliest intervention.\n",
    "        if thread_intervention == 1:\n",
    "            post_message = \"SELECT user, post_time FROM post WHERE thread_id = \\\"\"\n",
    "            post_message += thread_id\n",
    "            post_message += \"\\\" ORDER BY post_time\"\n",
    "            cur.execute(post_message)\n",
    "            all_posts = cur.fetchall()\n",
    "            for each_post in all_posts:\n",
    "                poster = each_post[0]\n",
    "                post_time = each_post[1]\n",
    "                if poster in user_table and (user_table[poster] == 'Instructor' or user_table[poster] == 'Staff') and thread_intervened_time_dic[thread_id] == -1:\n",
    "                    thread_intervened_time_dic[thread_id] = post_time\n",
    "\n",
    "            comment_message = \"SELECT user, post_time FROM comment WHERE thread_id = \\\"\"\n",
    "            comment_message += thread_id\n",
    "            comment_message += \"\\\" ORDER BY post_time\"\n",
    "            cur.execute(comment_message)\n",
    "            all_comments = cur.fetchall()\n",
    "            for each_comment in all_comments:\n",
    "                commenter = each_comment[0]\n",
    "                comment_time = each_comment[1]\n",
    "                if commenter in user_table and (user_table[commenter] == 'Instructor' or user_table[commenter] == 'Staff'):\n",
    "                    if thread_intervened_time_dic[thread_id] == -1 or comment_time < thread_intervened_time_dic[thread_id]:\n",
    "\n",
    "                        thread_intervened_time_dic[thread_id] = comment_time\n",
    "                        \n",
    "            if thread_intervened_time_dic[thread_id] == -1:\n",
    "                replied_thread_without_reply_time.append(thread_id)\n",
    "        \n",
    "        # Memorize all the info in their respective dics\n",
    "        ids.append(thread_id)\n",
    "        thread_posted_time_dic[thread_id] = thread_time\n",
    "        thread_forum_dic[thread_id] = thread_forum_id\n",
    "        is_replied[thread_id] = thread_intervention\n",
    "        thread_post_set_dic[thread_id] = []\n",
    "        num_of_post_dic[thread_id] = 0\n",
    "        num_of_comment_dic[thread_id] = 0\n",
    "        num_of_votes_dic[thread_id] = 0\n",
    "        starter_dic[thread_id] = thread_starter\n",
    "        isreplied.append(thread_intervention)\n",
    "        thread_text_dic[thread_id] = thread_title\n",
    "\n",
    "    # extract all the posts\n",
    "    post_message = 'SELECT id, thread_id, votes, user, post_text, post_time FROM post '\n",
    "    post_message += limit_message\n",
    "    post_message += ' ORDER BY post_time'\n",
    "    cur.execute(post_message)\n",
    "    rows = cur.fetchall()\n",
    "    for each_post in rows:\n",
    "        \n",
    "        # All the info from this select operation\n",
    "        post_id = each_post[0]\n",
    "        post_thread_id = each_post[1]\n",
    "        post_vote = each_post[2]\n",
    "        poster = each_post[3]\n",
    "        post_text = each_post[4]\n",
    "        post_time = each_post[5]\n",
    "        \n",
    "        # If the home thread is intervened, continue.\n",
    "        if post_thread_id in is_replied and is_replied[post_thread_id] and thread_intervened_time_dic[post_thread_id] != -1 and post_time >= thread_intervened_time_dic[post_thread_id]:\n",
    "            continue\n",
    "        \n",
    "        # initialize all the features dic for posts\n",
    "        post_comment_set_dic[post_id] = []\n",
    "        num_of_comment_for_post_dic[post_id] = 0 \n",
    "        post_text_dic[post_id] = post_text\n",
    "\n",
    "        # if the thread is not initialized by the instructor, add thread level features\n",
    "        if post_thread_id in num_of_votes_dic:\n",
    "            num_of_votes_dic[post_thread_id] += post_vote\n",
    "            thread_post_set_dic[post_thread_id].append(post_id)\n",
    "            num_of_post_dic[post_thread_id] += 1\n",
    "\n",
    "\n",
    "    comment_message = 'SELECT id, thread_id, post_id, user, comment_text, post_time FROM comment '\n",
    "    comment_message += limit_message\n",
    "    comment_message += ' ORDER BY post_time'\n",
    "\n",
    "    cur.execute(comment_message)\n",
    "    print(comment_message)\n",
    "    rows = cur.fetchall()\n",
    "    for each_comment in rows:\n",
    "        \n",
    "        # All the info from this select operation\n",
    "        comment_id = each_comment[0]\n",
    "        comment_thread_id = each_comment[1]\n",
    "        comment_post_id = each_comment[2]\n",
    "        commenter = each_comment[3]\n",
    "        comment_text = each_comment[4]\n",
    "        comment_time = each_comment[5]\n",
    "\n",
    "        # if after intervention, truncate\n",
    "        if comment_thread_id in is_replied and is_replied[comment_thread_id] and thread_intervened_time_dic[comment_thread_id] != -1 and comment_time >= thread_intervened_time_dic[comment_thread_id]:\n",
    "            continue\n",
    " \n",
    "        # features\n",
    "        comment_text_dic[comment_id] = each_comment[4]\n",
    "\n",
    "        if comment_thread_id in num_of_comment_dic:\n",
    "            num_of_comment_dic[comment_thread_id] +=  1\n",
    "            \n",
    "        if comment_post_id in num_of_comment_for_post_dic:\n",
    "            num_of_comment_for_post_dic[comment_post_id] += 1\n",
    "            post_comment_set_dic[comment_post_id].append(comment_id)\n",
    "            \n",
    "print(len(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# construct the id to index dictionary and split the training ids and testing ids\n",
    "index = 0\n",
    "training_ids = []\n",
    "testing_ids = []\n",
    "\n",
    "# ensure add from scratch\n",
    "texts = []\n",
    "training_texts = []\n",
    "testing_texts = []\n",
    "\n",
    "# storing the ids for the training set and test set\n",
    "for oneid in ids:\n",
    "    id_to_index[oneid] = index\n",
    "    if forum_type_dic[thread_forum_dic[oneid]] <= middleInd:\n",
    "        training_ids.append(oneid)\n",
    "    else:\n",
    "        testing_ids.append(oneid)\n",
    "\n",
    "    index += 1\n",
    "\n",
    "# process text\n",
    "for oneid in ids:\n",
    "    \n",
    "    filename = 'text/' + course_id + \"/\" + str(oneid) + '.txt'\n",
    "    f = open(filename, 'r', errors='ignore')\n",
    "    content = f.read()\n",
    "    \n",
    "    # features\n",
    "    num_url = content.count('a href')             \n",
    "    num_timeref = content.count('<TIMEREF>')\n",
    "    sen_list = re.split(r'[.!?]+', content)\n",
    "    num_sen = len(sen_list)\n",
    "    \n",
    "    # store features\n",
    "    num_of_url_dic[oneid] = num_url\n",
    "    num_of_timeref_dic[oneid] = num_timeref\n",
    "    num_of_sen_dic[oneid] = num_sen\n",
    "\n",
    "    soup = BeautifulSoup(content, \"lxml\")\n",
    "\n",
    "    for tag in soup.find_all('code'):\n",
    "        tag.replaceWith('')\n",
    "\n",
    "    for tag in soup.find_all('a'):\n",
    "        tag.replaceWith('')\n",
    "\n",
    "    content = soup.get_text()\n",
    "    content = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', content, flags=re.MULTILINE)\n",
    "\n",
    "    if oneid in training_ids:\n",
    "        training_texts.append(content)\n",
    "    else:\n",
    "        testing_texts.append(content)\n",
    "    texts.append(content)\n",
    "\n",
    "# get the training vector and test vector\n",
    "training_vectors, testing_vectors, tfidf_model = get_vectors(training_texts, testing_texts)\n",
    "tmp_vectors = []\n",
    "current_training_index = 0\n",
    "current_testing_index = 0\n",
    "\n",
    "training_vectors = training_vectors.toarray()\n",
    "testing_vectors = testing_vectors.toarray()\n",
    "\n",
    "for oneid in ids:\n",
    "    if oneid in training_ids:\n",
    "        tmp_vectors.append(training_vectors[current_training_index])\n",
    "        current_training_index += 1\n",
    "    else:\n",
    "        tmp_vectors.append(testing_vectors[current_testing_index])\n",
    "        current_testing_index += 1\n",
    "\n",
    "if current_training_index != len(training_vectors) or current_testing_index != len(testing_vectors):\n",
    "    print(\"Fatal error!\")\n",
    "\n",
    "\n",
    "\n",
    "data_length = len(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      1.00      0.96        48\n",
      "          1       1.00      0.33      0.50         6\n",
      "\n",
      "avg / total       0.93      0.93      0.91        54\n",
      "\n",
      "did not extract\n",
      "GdeNrll1EeSROyIACtiVvg~rgWlD4O2EeWwyg6oWwzVsw\n",
      "technical issue\n",
      "the video by the name of summing up memory is not working\n",
      "\n",
      "\n",
      "\n",
      "did not extract\n",
      "GdeNrll1EeSROyIACtiVvg~-VECyI6iEeW7vBKNDxH-cw\n",
      "the cue that launches into procrastination\n",
      "In my case, I think the cue that launches me into procrastination is the difficulties of the subject that I'm learning and when I don't have results on my research, I feel so sad, but sometimes to overcome that feelings I say to myself that it's part of learning and with extra efforts I can do it. what do you think?\n",
      "\n",
      "\n",
      "\n",
      "did not extract\n",
      "GdeNrll1EeSROyIACtiVvg~FFr6JJAuEeW9ThKFn3E1-Q\n",
      "Process versus product: loafing?\n",
      "I would like to ask a question related to the process versus product lecture. Although I think focussing on the process is a great way of cancelling out procrastination, I was wondering whether it might contribute to loafing during your study session? After all, one is no longer concerned about finishing a product in a set time, instead they are focussed on spending a certain amount of time on studying. Is there not a danger that one would slow their learning pace down, after all, there is no focuss on finishing the product, and hereby increase difficulties with finishing the product?I would like to get some feedback on this matter in the hopes that we can all learn from this.Kind regards,Jules\n",
      "\n",
      "\n",
      "\n",
      "did not extract\n",
      "GdeNrll1EeSROyIACtiVvg~WXlvmpGlEeWuoA7BXrrLbQ\n",
      "How can we distract ourselves from our phones? Tough.\n",
      "I have tried shutting off my phone, or keeping it someplace far removed from my study, but it's tough. Can anyone suggest a more offbeat solution to this type of a problem? Thanks.\n",
      "Switch your SIM out to a phone that does nothing but make telephone calls?\n",
      "Thanks..! \n",
      "I find that my phone itself isn't the issue, it's the different types of distractions it provides. Getting rid of it altogether isn't really practical because I need to be available to certain people.Calls are generally important enough that I'll take them even if I'm focusingTime-sink apps like facebook, twitter, and instagram have to be consciously opened.I use an android app called \"Clockwork Tomato\" that keeps the screen on when I'm doing a pomodoro, that way I can't open any other apps without having to actively close the pomodoro - something that makes me feel bad so I don't do it.Notifications can be muted so you don't get distracted by the noise. I leave them on now because I'm practising changing my reaction to \"Okay I just received an [email/text/message/facebook], but I'm already focusing on X. I'll check that at an appropriate time.\".Hope this helps!\n",
      "Fantastic idea! Thank you so much! :D\n",
      "There is a program called \"Forest\" and if you touch your phone within 30 minutes the tree dies.. And I refused to install Facebook messenger app.\n",
      "Fantastic idea!  Thank you so much! :D\n",
      "I simply did not install some apps. For example to use Facebook (9gag and many more) I have to start a browser, log in and then i have a very lousy version of Facebook. Not that fun to check it every 10 minutes.Home on my desktop computer is basically the same. I made some setting at my browser so it deletes all the forms and log in data every time I close it, so basically I have to manually log in every time...trust me is annoying and I use it less also because of that. For me is working good.Is a lot of self discipline but it comes with the time. Good luck!\n",
      "I didn't install FB messenger app. It is annoying sometimes I can't get other people's message immediately, but the app creates a huge lag and distraction that I felt better without.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_vectors = []\n",
    "test_vectors = []\n",
    "training_result = []\n",
    "test_result = []\n",
    "\n",
    "# normalize the term array\n",
    "x = np.asarray(tmp_vectors)\n",
    "x = sklearn.preprocessing.normalize(x, norm='l2', axis=0, copy=True, return_norm=False)\n",
    "\n",
    "### EDM 15\n",
    "\n",
    "# additional vectors to be appended later\n",
    "additional_vectors = []\n",
    "\n",
    "for oneid in ids:\n",
    "    \n",
    "    # get the index from the first to the last in the id list\n",
    "    index = id_to_index[oneid]\n",
    "    origin_list = x[index].tolist()\n",
    "    \n",
    "    # temporary list storing the newly added features    \n",
    "    forum_index = forum_type_dic[thread_forum_dic[oneid]]\n",
    "    forum_bit_list = get_bit_list(int(num_forum), int(forum_index))\n",
    "    li = forum_bit_list\n",
    "    \n",
    "    numpost = num_of_post_dic[oneid]\n",
    "    li.append(numpost)\n",
    "    numcomment = num_of_comment_dic[oneid]\n",
    "    li.append(numcomment)\n",
    "    li.append(numpost + numcomment)\n",
    "    \n",
    "    # will add the average # of comments per post\n",
    "    summ = 0\n",
    "    for postid in thread_post_set_dic[oneid]:\n",
    "        summ = summ + num_of_comment_for_post_dic[postid]\n",
    "     \n",
    "    if len(thread_post_set_dic[oneid]) == 0:\n",
    "        avr = 0\n",
    "    else:\n",
    "        avr = float(summ / len(thread_post_set_dic[oneid]))\n",
    "\n",
    "    li.append(avr)\n",
    "    numurl = num_of_url_dic[oneid]\n",
    "    li.append(numurl)\n",
    "    numsen = num_of_sen_dic[oneid]\n",
    "    li.append(numsen)\n",
    "    numvotes = num_of_votes_dic[oneid]\n",
    "    li.append(numvotes)\n",
    "\n",
    "    numtimeref = num_of_timeref_dic[oneid]\n",
    "    additional_vectors.append(li)\n",
    "\n",
    "# normalize the new vectors (max_min norm)\n",
    "additional_vectors = np.asarray(additional_vectors)\n",
    "scaler = sklearn.preprocessing.MinMaxScaler()\n",
    "scaler.fit(additional_vectors)\n",
    "additional_vectors = scaler.transform(additional_vectors)\n",
    "\n",
    "# append new feature vectors to the term vectors\n",
    "x_edm = np.append(x, additional_vectors, axis = 1)\n",
    "y_edm = np.asarray(isreplied)\n",
    "\n",
    "\n",
    "for oneid in ids:\n",
    "    \n",
    "    # get the index from the first to the last in the id list\n",
    "    index = id_to_index[oneid]\n",
    "    origin_list = x_edm[index].tolist()\n",
    "    \n",
    "    # split the vectors into training and test set\n",
    "    forum_id = thread_forum_dic[oneid]\n",
    "    forum_number = forum_type_dic[forum_id]\n",
    "    \n",
    "    if forum_number <= middleInd:\n",
    "        training_vectors.append(origin_list)\n",
    "        training_result.append(isreplied[index])\n",
    "    else:\n",
    "        test_vectors.append(origin_list)\n",
    "        test_result.append(isreplied[index])\n",
    "\n",
    "LogReg = LogisticRegression(class_weight = 'balanced')\n",
    "LogReg.fit(training_vectors, training_result)\n",
    "pred_result = LogReg.predict(test_vectors)\n",
    "with open('EDM.txt', 'w') as f:\n",
    "    print(classification_report(test_result, pred_result), file = f)\n",
    "print(classification_report(test_result, pred_result))\n",
    "\n",
    "for i in range(len(test_result)):\n",
    "    if test_result[i] == 1 and pred_result[i] == 0:\n",
    "        print('did not extract')\n",
    "        print(testing_ids[i])\n",
    "        print(texts[id_to_index[testing_ids[i]]])\n",
    "        print('\\n')\n",
    "    elif test_result[i] == 0 and pred_result[i] == 1:\n",
    "        print('rubbish extracted')\n",
    "        print(testing_ids[i])\n",
    "        print(texts[id_to_index[testing_ids[i]]])\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GdeNrll1EeSROyIACtiVvg~pwkebYn6EeWrDQr9ZhnP1w', 'GdeNrll1EeSROyIACtiVvg~zaOw6IvVEeWJvQrEo78pXQ', 'GdeNrll1EeSROyIACtiVvg~dLJogI6nEeW7vBKNDxH-cw', 'GdeNrll1EeSROyIACtiVvg~gA9FUY_NEeWvqwoBxhpjvw', 'GdeNrll1EeSROyIACtiVvg~uNRHvLqzEeWARArsHojuCw']\n"
     ]
    }
   ],
   "source": [
    "# problematic threads\n",
    "print(replied_thread_without_reply_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "35\n",
      "47\n"
     ]
    }
   ],
   "source": [
    "# track the number of positive instances in the training set\n",
    "pos_result = 0\n",
    "neg_result = 0\n",
    "\n",
    "for bit in training_result:\n",
    "    if not bit == 0:\n",
    "        pos_result += 1\n",
    "    else:\n",
    "        neg_result += 1\n",
    "        \n",
    "print(pos_result)\n",
    "print(neg_result)\n",
    "\n",
    "print(len(training_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ycQnChn3EeWDtQoum3sFeQ~I-bNhXaOEeWpCwrPNVXTlw\n",
      "ycQnChn3EeWDtQoum3sFeQ~UwsgVXaZEeWXygocDxikNQ\n",
      "ycQnChn3EeWDtQoum3sFeQ~fjQk3XcwEeWy9BLlActDnw\n",
      "ycQnChn3EeWDtQoum3sFeQ~TyDN33c1EeWdPxJ-h_8T7w\n",
      "ycQnChn3EeWDtQoum3sFeQ~K5ZanHdUEeWpCwrPNVXTlw\n",
      "ycQnChn3EeWDtQoum3sFeQ~kpULKnedEeWGywq_F70CwQ\n",
      "ycQnChn3EeWDtQoum3sFeQ~KuPqs3i9EeWPJBI9QEQKiQ\n",
      "ycQnChn3EeWDtQoum3sFeQ~q2zbz3jlEeWDzg4yGnIlTw\n",
      "ycQnChn3EeWDtQoum3sFeQ~eHjFXHkLEeWWpw7gge8ZbQ\n",
      "ycQnChn3EeWDtQoum3sFeQ~sOq-_3kQEeWtPhJmZh_6JQ\n",
      "ycQnChn3EeWDtQoum3sFeQ~e1t60XlSEeWXygocDxikNQ\n",
      "ycQnChn3EeWDtQoum3sFeQ~lDbxqnmIEeWgtQq7brjg6Q\n",
      "ycQnChn3EeWDtQoum3sFeQ~xDuZx3nAEeWtPhJmZh_6JQ\n",
      "ycQnChn3EeWDtQoum3sFeQ~lgl-g3nJEeWpCwrPNVXTlw\n",
      "ycQnChn3EeWDtQoum3sFeQ~th4JXnodEeWWpw7gge8ZbQ\n",
      "ycQnChn3EeWDtQoum3sFeQ~Q48m8HpSEeWqXg7D-Kwb-Q\n",
      "ycQnChn3EeWDtQoum3sFeQ~7abI7XsNEeWdPxJ-h_8T7w\n",
      "ycQnChn3EeWDtQoum3sFeQ~_O6chnsPEeWqXg7D-Kwb-Q\n",
      "ycQnChn3EeWDtQoum3sFeQ~MF9jAHs8EeWGywq_F70CwQ\n",
      "ycQnChn3EeWDtQoum3sFeQ~KWyzmHteEeWDzg4yGnIlTw\n",
      "ycQnChn3EeWDtQoum3sFeQ~h-cmant5EeWb4g4qCdqdUQ\n",
      "ycQnChn3EeWDtQoum3sFeQ~27p78XuAEeWb4g4qCdqdUQ\n",
      "ycQnChn3EeWDtQoum3sFeQ~KedMs3uBEeWy9BLlActDnw\n",
      "ycQnChn3EeWDtQoum3sFeQ~J3vYJ3uREeWtPhJmZh_6JQ\n",
      "ycQnChn3EeWDtQoum3sFeQ~ngnl_3v5EeWWpw7gge8ZbQ\n",
      "ycQnChn3EeWDtQoum3sFeQ~vO-EtXwREeWtPhJmZh_6JQ\n",
      "ycQnChn3EeWDtQoum3sFeQ~sb97Iny8EeWdPxJ-h_8T7w\n",
      "ycQnChn3EeWDtQoum3sFeQ~st6dX36CEeWMKRINpuifKQ\n",
      "ycQnChn3EeWDtQoum3sFeQ~KdsheH8xEeWtPQ4SI9a3Aw\n",
      "ycQnChn3EeWDtQoum3sFeQ~aVo9939JEeWXKRJ71r2fxw\n",
      "ycQnChn3EeWDtQoum3sFeQ~bmSr5H9fEeW25Arf55S8eQ\n",
      "ycQnChn3EeWDtQoum3sFeQ~8mC4NX-uEeWFDQ4Rhv_F_Q\n",
      "ycQnChn3EeWDtQoum3sFeQ~w566dH_REeWXKRJ71r2fxw\n",
      "ycQnChn3EeWDtQoum3sFeQ~uG21FH_fEeWZiRK7avAQuw\n",
      "ycQnChn3EeWDtQoum3sFeQ~3YWZuYB0EeWzCgp9bpb0bQ\n",
      "ycQnChn3EeWDtQoum3sFeQ~AO8v_YEgEeWm9hKwN2_UgQ\n",
      "ycQnChn3EeWDtQoum3sFeQ~pxvxSoGTEeWZiRK7avAQuw\n",
      "ycQnChn3EeWDtQoum3sFeQ~llhxkoH0EeWOJwp8z3qGpQ\n",
      "ycQnChn3EeWDtQoum3sFeQ~La6Fv4JoEeWA9w4ipzCZ2Q\n",
      "ycQnChn3EeWDtQoum3sFeQ~bmRv1YLcEeWL6AoNmIctaQ\n",
      "ycQnChn3EeWDtQoum3sFeQ~vSuGfYMbEeW93g4-ZzZ2VQ\n",
      "ycQnChn3EeWDtQoum3sFeQ~e6TvK4SNEeWFYRKN6-z0qQ\n",
      "ycQnChn3EeWDtQoum3sFeQ~LryltoVJEeWZCxLh4264pw\n",
      "ycQnChn3EeWDtQoum3sFeQ~9cleJ4WEEeWFYRKN6-z0qQ\n",
      "ycQnChn3EeWDtQoum3sFeQ~QiF0ToWYEeWOJwp8z3qGpQ\n",
      "ycQnChn3EeWDtQoum3sFeQ~MshcJYW-EeWFYRKN6-z0qQ\n",
      "ycQnChn3EeWDtQoum3sFeQ~XW3-GYXBEeWOJwp8z3qGpQ\n",
      "ycQnChn3EeWDtQoum3sFeQ~SGEW54XnEeWTlApPZ_t2iQ\n",
      "ycQnChn3EeWDtQoum3sFeQ~qUklFIZAEeW93g4-ZzZ2VQ\n",
      "ycQnChn3EeWDtQoum3sFeQ~I1-v3YeyEeWNJg5wP7QpjQ\n",
      "ycQnChn3EeWDtQoum3sFeQ~4Z0HmYjeEeW7vBKNDxH-cw\n",
      "ycQnChn3EeWDtQoum3sFeQ~eEeigomzEeWH0w7fQkp2-w\n",
      "ycQnChn3EeWDtQoum3sFeQ~2HVhZonCEeWD3RImE8PCdQ\n",
      "ycQnChn3EeWDtQoum3sFeQ~d98w3YnwEeWJ8Q5UvS9Vkw\n",
      "ycQnChn3EeWDtQoum3sFeQ~ev50kYrdEeWJvQrEo78pXQ\n",
      "ycQnChn3EeWDtQoum3sFeQ~XcBUE4vNEeWwEwr2ChLK8Q\n",
      "ycQnChn3EeWDtQoum3sFeQ~b3o8r40GEeWJvQrEo78pXQ\n",
      "ycQnChn3EeWDtQoum3sFeQ~rTlpKY66EeWJvQrEo78pXQ\n",
      "ycQnChn3EeWDtQoum3sFeQ~r9cjCHCCEea8hBI6Erv2mQ\n",
      "p q r s:\n",
      "4\n",
      "55\n",
      "185\n",
      "294\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.77      0.73      0.75        77\n",
      "          1       0.42      0.47      0.44        32\n",
      "\n",
      "avg / total       0.66      0.65      0.66       109\n",
      "\n",
      "[[ 0.22501599 -0.09575347 -0.09275001 ..., -0.32855934  0.30441343\n",
      "  -1.25314398]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.77      0.73      0.75        77\n",
      "          1       0.42      0.47      0.44        32\n",
      "\n",
      "avg / total       0.66      0.65      0.66       109\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_vectors_enhanced = []\n",
    "test_vectors_enhanced = []\n",
    "tmp_vectors = []\n",
    "has_gratitude_list = []\n",
    "\n",
    "p = 0\n",
    "q = 0\n",
    "r = 0\n",
    "s = 0\n",
    "    \n",
    "for index in range(data_length):\n",
    "    \n",
    "    li = []\n",
    "    oneid = ids[index]\n",
    "    origin_list = x_edm[index].tolist()\n",
    "    post_list = thread_post_set_dic[oneid]\n",
    "    starting_post_text = preprocess_text(post_text_dic[post_list[0]])\n",
    "    \n",
    "    last_post_index = len(post_list) - 1\n",
    "    \n",
    "    has_key_post = False\n",
    "    for i in range(0, 3):\n",
    "        current_post_index = last_post_index - i\n",
    "        if current_post_index > 0:\n",
    "            current_post_id = post_list[current_post_index]\n",
    "            current_post_text = post_text_dic[current_post_id]\n",
    "            cleanr = re.compile('<.*?>')\n",
    "            current_post_text = re.sub(cleanr, '', current_post_text)\n",
    "            if len(current_post_text.split()) <= 15 and ('thank' in current_post_text or 'Thank' in current_post_text):\n",
    "                li.append(1)\n",
    "                has_key_post = True\n",
    "                break\n",
    "                \n",
    "            comment_list = post_comment_set_dic[current_post_id]\n",
    "            contribution = False\n",
    "            for current_comment_id in comment_list:\n",
    "                current_comment_text = comment_text_dic[current_comment_id]\n",
    "                if len(current_comment_text.split()) <= 15 and ('thank' in current_comment_text or 'Thank' in current_comment_text):\n",
    "                    contribution = True\n",
    "                    \n",
    "            if contribution:\n",
    "                li.append(1)\n",
    "                has_key_post = True\n",
    "                break\n",
    "    \n",
    "    if not has_key_post:\n",
    "        li.append(0)\n",
    "    else:\n",
    "        print(oneid)\n",
    "\n",
    "    \n",
    "    if has_key_post and is_replied[oneid] == 1:\n",
    "        p += 1\n",
    "        \n",
    "    elif has_key_post and is_replied[oneid] == 0:\n",
    "        q += 1\n",
    "    \n",
    "    elif not has_key_post and is_replied[oneid] == 1:\n",
    "        r += 1\n",
    "        \n",
    "    elif not has_key_post and is_replied[oneid] == 0:\n",
    "        s += 1\n",
    "\n",
    "\n",
    "    new_list = origin_list + li\n",
    "\n",
    "    tmp_vectors.append(new_list)\n",
    "    if forum_type_dic[thread_forum_dic[oneid]] <= middleInd:\n",
    "        training_vectors_enhanced.append(new_list)\n",
    "\n",
    "    else:\n",
    "        test_vectors_enhanced.append(new_list)\n",
    "\n",
    "print(\"p q r s:\")\n",
    "print(p)\n",
    "print(q)\n",
    "print(r)\n",
    "print(s)\n",
    "    \n",
    "    \n",
    "LogReg = LogisticRegression(class_weight = 'balanced')\n",
    "LogReg.fit(training_vectors_enhanced, training_result)\n",
    "pred_result_enhanced = LogReg.predict(test_vectors_enhanced)\n",
    "with open('EDM.txt', 'w') as f:\n",
    "    print(classification_report(test_result, pred_result_enhanced), file = f)\n",
    "print(classification_report(test_result, pred_result_enhanced))\n",
    "print(LogReg.coef_)\n",
    "\n",
    "novel_result = []\n",
    "\n",
    "for i in range(0, len(test_result)):\n",
    "    if test_vectors[i][len(test_vectors[0]) - 1] == 1:\n",
    "        novel_result.append(0)\n",
    "    else:\n",
    "        novel_result.append(pred_result_enhanced[i])\n",
    "        \n",
    "print(classification_report(test_result, novel_result))\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.         ...,  0.          0.02684564\n",
      "   0.06666667]\n",
      " [ 0.          0.          0.         ...,  0.          0.01342282  0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.00671141  0.        ]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.0625      0.19463087\n",
      "   0.42222222]\n",
      " [ 0.          0.          0.         ...,  0.          0.06040268  0.2       ]\n",
      " [ 0.          0.          0.         ...,  0.          0.0738255\n",
      "   0.06666667]]\n"
     ]
    }
   ],
   "source": [
    "print(x_edm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "did not extract\n",
      "ycQnChn3EeWDtQoum3sFeQ~MRtFO3c5EeWdPxJ-h_8T7w\n",
      "Dear all, use external resources (Bootstrap CDN) for CSS and JS \n",
      "\n",
      "While reviewing my classmates I realized that it's hard to examine HTML pages because of missed styles.I suggest using external resources for CSS and JS as professor Jogesh mentioned before (Bootstrap CDN - . Or simply host files on your Google drive - OR you can use my compilation-------- before </head> --------<link rel=\"stylesheet\" href=\"https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css\">    <link rel=\"stylesheet\" href=\"https://googledrive.com/host/0BwxB5DwSCJhrVnRZWi04SjNsOEE\">    <link rel=\"stylesheet\" href=\"https://d396qusza40orc.cloudfront.net/phoenixassets/web-frameworks/bootstrap-social.css\"> <link rel=\"stylesheet\" href=\"https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css\">AND-------- before </body> --------<script src=\"https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js\"></script>    <script src=\"https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js\"></script>\n",
      "I saw this on the first review -  ... and also the second I go back to look for in discussion this probably \"all people problem\". Too late for me, my own task will work bad and its no possible to edit. I will evaluate good all. And good solution Artem. Hope it will work for someone else\n",
      "I think if they wanted that they would have done that in the aboutus.html that we downloaded. But instead it refers to the locally installed bootstrap. The assignment instructions suggest that the reviewers download the assignment they are grading and view it locally: \"A reviewer should easily be able to take your file and substitute it into their own web project and see it working correctly\".  Just be careful and back up your original.\n",
      "Yep, I agree with you, Robert. But it takes a bunch of time downloading file in a proper directory, renaming (maybe), opening in a browser, etc. Instead of this you could simply click on a link and do your review much faster online. But yes, there is a suggestion that everybody should be able download it locally.Roberto, I just wanted to mention that there is a room for improvement:) Nothing else. Evaluation could not depend on this.. for sure:))Thanks for the feedback, guys.\n",
      "Confusion?? When downloading a students aboutus.html, the links change for all stylesheets etc, incl links pages to amazon. Why is this?  \n",
      "Sorry, figured it out. Didn't download as HTML but instead \"complete web site\"\n",
      "\n",
      "\n",
      "\n",
      "rubbish extracted\n",
      "ycQnChn3EeWDtQoum3sFeQ~DzmfgXeiEeWDzg4yGnIlTw\n",
      "brackets.io\n",
      "\n",
      "People,I suggest you strongly download Brackets.I never haven't used this tool.In last year, I used sublime for frontend development and Eclipse IDE for other stuff...But I could experiment the tool suggested by Jogesh K. Muppala, and that was a valuable recommendation.Live preview, debug integration, css editor... Good designed features!\n",
      "Here's a list of top open source editors:\n",
      "\n",
      "\n",
      "\n",
      "did not extract\n",
      "ycQnChn3EeWDtQoum3sFeQ~fT-_j3teEeWWpw7gge8ZbQ\n",
      "Assignment 4\n",
      "\n",
      "On assignment 4, we are supposed to remove the data attributes from the Reserve Table button and the Login button and add JS code to operate the modals.  I understand that part of the assignment, but are we supposed to do the same for the close/cancel buttons inside the modals as well? Should we also do this for the alert box inside the reserve form modal?Also, do we need to create a separate <script> tag for each of these operations, or is it okay to simply add the code to the script tag we've already created for the carousel?\n",
      "\n",
      "\n",
      "\n",
      "did not extract\n",
      "ycQnChn3EeWDtQoum3sFeQ~Q12ypHzKEeWb4g4qCdqdUQ\n",
      "Assignment 4, show/hide modal\n",
      "\n",
      "\"...  modal will be shown/hidden when the ... button is clicked:  \"When the Login or Reserve Table button is clicked, with proper JS setup, the modal will be triggered.  At this point, the rest of the page should be grey-out.  There's no access to the button anymore. Clicking anywhere outside the modal will close the modal, there's no need to hide it manually.May be I read into the lines too much. Let me know if there's a need to implement modal('hide')   ThanksW\n",
      "\n",
      "\n",
      "\n",
      "did not extract\n",
      "ycQnChn3EeWDtQoum3sFeQ~fraO8H2kEeWGywq_F70CwQ\n",
      "Assignment 4 : Multiple way of doing it, but does not match well with the grading choices\n",
      "\n",
      "We have lots of freedom to achieve assignment 4.Unfortunately, it feels to me the choices/options when doing peer reviews seems focused on only one possible solution.More concretely:The [element X] has been given an id so that it can be referred to appropriately within the JavaScript code (2 x 10pts)What if the student used the `onclick` attribute to trigger the JavaScript code, as it was explained in course 1 of the specialization ?What if the student used the `href` attribute to designate the modal associated with the button ?Correct JavaScript code has been added \n",
      "to the end of the page so that the [...] modal is shown/hidden when the \n",
      "Login link is clicked (2 x 30pts).What if the student invoke directly the proper method from its `onclik` handler ?What if the student wrote only one JavaScript function and used parameter to select the right modal to toggle ?What if the student wrote only one JavaScript function and used the `href` attribute of the source element to select the right modal to toggle ?As of myself, I would consider all these solution as \"valid\". What is your opinion about that ?\n",
      "\n",
      "\n",
      "\n",
      "rubbish extracted\n",
      "ycQnChn3EeWDtQoum3sFeQ~QtJHhX7cEeWNtQ7GfsGVkQ\n",
      "Bower in ubuntu\n",
      "\n",
      "I had to add this in Ubunto to get it workingAnother recommendation I saw was to install the nodejs-legacyBut, the first one worked for me so I never tried legacy.\n",
      "This technique was recommended on the nodejs github wiki.This should work for most debian based distros.\n",
      "\n",
      "\n",
      "\n",
      "did not extract\n",
      "ycQnChn3EeWDtQoum3sFeQ~PKMSWYMVEeWOJwp8z3qGpQ\n",
      "Shouldn't radio button better a better fit?\n",
      "\n",
      "Just wondering, wouldn't radio button group a better fit since you can have either pause or play.This is what I did instead:     <div class=\"btn-group\" id=\"carouselButtons\" data-toggle=\"buttons\">\t\t\t\t\t\t<label class=\"btn btn-success btn-xs\" id=\"carousel-pause\">\t\t\t\t\t\t\t<input type=\"radio\" autocomplete=\"off\">\t\t\t\t\t\t\t<span class=\"glyphicon glyphicon-pause\" aria-hidden=\"true\"></span>\t\t\t\t\t\t</label>\t\t\t\t\t\t<label class=\"btn btn-danger btn-xs active\" id=\"carousel-play\">\t\t\t\t\t\t\t<input type=\"radio\" autocomplete=\"off\" checked>\t\t\t\t\t\t\t<span class=\"glyphicon glyphicon-play\" aria-hidden=\"true\"></span>\t\t\t\t\t\t</label>\t\t\t\t\t</div>  $(document).ready(function() {\t\t\t$(\"#mycarousel\").carousel({\t\t\t\tinterval : 2000\t\t\t});\t\t\t$(\"#carousel-pause\").click(function() {\t\t\t\t$(\"#mycarousel\").carousel('pause');\t\t\t\t$(\"#carousel-pause\").toggleClass(\"btn-success btn-danger\")\t\t\t\t$(\"#carousel-play\").toggleClass(\"btn-success btn-danger\")\t\t\t});\t\t\t$(\"#carousel-play\").click(function() {\t\t\t\t$(\"#mycarousel\").carousel('cycle');\t\t\t\t$(\"#carousel-pause\").toggleClass(\"btn-success btn-danger\")\t\t\t\t$(\"#carousel-play\").toggleClass(\"btn-success btn-danger\")\t\t\t});\t\t});\n",
      "\n",
      "\n",
      "\n",
      "rubbish extracted\n",
      "ycQnChn3EeWDtQoum3sFeQ~Ae40WoPcEeWA9w4ipzCZ2Q\n",
      "Netbeans 8.1 is out!\n",
      "\n",
      "New Netbeans version 8.1 provides great support to html, css, js, nodejs, gulp... and more.You can download and try it here... \n",
      "\n",
      "\n",
      "\n",
      "did not extract\n",
      "ycQnChn3EeWDtQoum3sFeQ~DaK224TBEeWFYRKN6-z0qQ\n",
      "Confused by requirements for Assignment 4.\n",
      "\n",
      "The requirements for Assignment 4 are confusing to me.1) When I click on the Login link the loginModal is shown. After that it is unclear what is required?2) Once the modal is shown is the Login link still active? If so, if I click on the link is it supposed to now hide the modal?3) If the above 2) is correct, then if I again click on the Login Link is it now supposed to show the modal?\n",
      "\n",
      "\n",
      "\n",
      "did not extract\n",
      "ycQnChn3EeWDtQoum3sFeQ~h3eIqITQEeWaOxK2u5ceiQ\n",
      "CSS order of files matters\n",
      "\n",
      "I just learned (or re-learned) one of the CSS rules: The last rule wins! (I am sure there is a better term for that rule.) After following the exercise to the end, I re-loaded my index.html page. The jumbotron background was off-white. I check the CSS via the Firefox dev tools Inspector, #7986CB. Then I moved the link tag to the css/mystyles.css stylesheet file to below the bower_components links. Works fine, now. Interesting. Hope that helps you, too, if your changes were made in a similar fashion as mine. \n",
      "\n",
      "\n",
      "\n",
      "did not extract\n",
      "ycQnChn3EeWDtQoum3sFeQ~2hfgTYZYEeWL6AoNmIctaQ\n",
      "Come to the dark side...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "rubbish extracted\n",
      "ycQnChn3EeWDtQoum3sFeQ~i27jn4aUEeW9yQ6NDqh2LQ\n",
      "Draggable   modal  dialog \n",
      "\n",
      "I figured all the serious students would have finished week-3 by now, and would have moved onto week-4.  So, i wanted to post this here in week-4 section to get attention.  I noticed that the modal box we created in week-3 for  Reserve Table can not be moved around in screen.  Searching net, i found the code for  draggable  modal box : I copied the JS to my index.html.  But this code does not work (even after making necessary modifications in the HTML part of the  index.html file).   What should be changed in the javascript to make this work with our index.html file ?\n",
      "\n",
      "\n",
      "\n",
      "rubbish extracted\n",
      "ycQnChn3EeWDtQoum3sFeQ~JW9h0YeoEeWI7RLSfyarRQ\n",
      "knowledge sharing - Closing bootstrap modal using ESC seems not working\n",
      "\n",
      "In the bootstrap document,  the modal has the option of \"keyboard\", which has the default value as \"true\", i.e. Closes the modal when escape key is pressed.  However,  I tried and it doesn't work!!  By searching the google and here is the answer . You need to add the attribute of tabindex=\"-1\" to the modal div.  It seems to be the bug in bootstrap.  Hoping it's helpful to you all.  \n",
      "\n",
      "\n",
      "\n",
      "did not extract\n",
      "ycQnChn3EeWDtQoum3sFeQ~6RThsYhMEeWZrhI6qhNSQw\n",
      "Error with mv in node.js\n",
      "\n",
      "Here is what node.js is saying when I put mv: \n",
      "\n",
      "\n",
      "\n",
      "did not extract\n",
      "ycQnChn3EeWDtQoum3sFeQ~dxtSzIiHEeWbXQqVyRPJkw\n",
      "Content Delivery Network versus Bower\n",
      "\n",
      "If you can use a Content Delivery Network to grab bootstrap, jquery etc. then why would we use Bower to track these dependencies?\n",
      "\n",
      "\n",
      "\n",
      "did not extract\n",
      "ycQnChn3EeWDtQoum3sFeQ~D2x1YojxEeW7vBKNDxH-cw\n",
      "NPM for dummies\n",
      "\n",
      "Mr Muppala.I was following your class along and learning but NPM seems to be over my head. I got lost, and I could probably copy paste and move on. But I don't get it.Is there a simpler version on how to learn basics for NPM ?\n",
      "\n",
      "\n",
      "\n",
      "rubbish extracted\n",
      "ycQnChn3EeWDtQoum3sFeQ~sfU1qokPEeW7vBKNDxH-cw\n",
      "CSS for achieving the desired position for play-pause buttons\n",
      "\n",
      "For getting the play-pause buttons at the bottom-right corner of the carousel, the css styling provided in the course does the trick. But I do not understand that part of the code on why it provides the results that we see. Could someone please help understanding how the position result is achieved by the below css code.The result without the styling is as below.\n",
      "\n",
      "\n",
      "\n",
      "rubbish extracted\n",
      "ycQnChn3EeWDtQoum3sFeQ~jqaOZolaEeWJ8Q5UvS9Vkw\n",
      "Thank you\n",
      "\n",
      "I'd like to thank our instructor Jogesh for making it possible for us to take this course. I like the use-case approach - we built a web site from ground up. We modified it applying different Bootstrap techniques.At the end we had the opportunity to glimpse into Node.js, Less/Sass and Bower.I have one very strong suggestion. Before each module, make the source code for this module available. The same applies for the assignments - make the source code used in the assignment available for download. I noticed a lot of differences in the produced web-site and couldn't actually find where the difference comes from. Going back to the video lectures didn't help much.There are different learning styles. For me listening to lectures for hours doesn't work well. I am used to quick discovery through digging the materials and peeking the lecture materials. Having source code available (something like checkpoints) will make it possible for people with different learning styles to follow. Will also make possible the learner to drive the speed.Once again, Jogesh, great work! Thank you.See you in the next course.If you are looking for people to help, count me in.\n",
      "Hi Ivan\n",
      "I am new in this field, as i can see you are great in Web-Design.\n",
      "I would like to have you on linkedin, but I cannot find you there. If you like please add me this is my account bellow.  https://mk.linkedin.com/in/dragan-sekuloski-9a944660  \n",
      "\n",
      "\n",
      "\n",
      "rubbish extracted\n",
      "ycQnChn3EeWDtQoum3sFeQ~X9VgmIlwEeWH0w7fQkp2-w\n",
      "Difference between  .zero-margin and .zero-margin()  in less\n",
      "\n",
      "In the video on Less/Sass, there is a small note that says What is the syntactic meaning & difference  between  these two specs of .zero-margin ? Both  seem to be CSS classes to my eyes.  I guess this is applicable only to Less because it appears on the Less side of the video.\n",
      "\n",
      "\n",
      "\n",
      "did not extract\n",
      "ycQnChn3EeWDtQoum3sFeQ~PdSvOImREeWZfg6Bj_UArQ\n",
      "Capstone Project Offering\n",
      "\n",
      "I can see that the next course in this specialization will start on November 23rd, and the Capstone Project will be offered at some time in February. I will be really busy for the next one month, so I will not be able to take part in the first offering of the next course. I assume that only people who will be able to take part in the first offering of all the courses will ultimately be able to take part in the first offering of the Capstone Project as well. My query is, how often will the Capstone Projects be offered? Because it would be sad if by the time I am done with all the courses (starting out late in one of them), I will have to sit idle for several months before the Capstone is offered again. \n",
      "\n",
      "\n",
      "\n",
      "rubbish extracted\n",
      "ycQnChn3EeWDtQoum3sFeQ~p_nBAYnKEeWH0w7fQkp2-w\n",
      "Thanks to Professor for great course\n",
      "\n",
      "Thank you Professor Muppala for a great course!I just finished assignment 4 and found it to be informative and a good learning experience.   It teaches your to use web resources and how to use JavaScript  with Bootstrap buttons, modal and links.  I think that was your objective.You provided a great course and I will see your on the next set.Thank your and very best regards,Bill\n",
      "Amen to that!\n",
      "\n",
      "\n",
      "\n",
      "rubbish extracted\n",
      "ycQnChn3EeWDtQoum3sFeQ~POKepIsqEeWJvQrEo78pXQ\n",
      "A Truly Accessible and Value Added Course\n",
      "\n",
      "Thank you for taking the time to prepare accessible, relevant, and value added content for this course. I hope it means something that I and my staff have learned a lot and you have enabled us !\n",
      "\n",
      "\n",
      "\n",
      "rubbish extracted\n",
      "ycQnChn3EeWDtQoum3sFeQ~9qX6jYunEeWrDQr9ZhnP1w\n",
      "how to change path for bootstrap or font-awesome to be installed in Terminal\n",
      "\n",
      "Hi,I am using Apple and my bootstrap or font-awesome or bower_components are stored under Users/Apple/... and I am not sure where to go in Terminal to change the path for which bower_components gets stored in conFusion folder on the desktop... (ie Users/Apple/Desktop/conFusion/\n",
      "\n",
      "\n",
      "\n",
      "rubbish extracted\n",
      "ycQnChn3EeWDtQoum3sFeQ~ElTCjIvJEeW7vBKNDxH-cw\n",
      "Every single one of the submissions I went through had messed up formatting.. code is fine though!\n",
      "\n",
      "Hi,So i have a dilema, every one of the assignments I marked had a problem. I think the bootstrap for some reason was not wrapping the HTML anymore, therefore if this is common problem then I will mark them as if it was working (the code is right).If not then I will mark them down.Please help!\n",
      "if your classmates use the bower_components folder fot bootstrap, jquery, css files and you have not followed the last tutorial to create the bower folders on your PC then they will look that way.  If you follow the bower lesson and create the folder structure as the instructor has showed the situation will be fixed. Look at the code what paths they use for bootstarp, jquery etc. If the code is OK and the problem is the paths then mark them as OK.\n",
      "\n",
      "\n",
      "\n",
      "rubbish extracted\n",
      "ycQnChn3EeWDtQoum3sFeQ~XcBUE4vNEeWwEwr2ChLK8Q\n",
      "Thank You!!!!!\n",
      "\n",
      "It is the best course I've done in my life .... largely by the professor !!! Thanks Jogesh !!!\n",
      "Jogesh,Thanks for a great course.\n",
      "Agree++++\n",
      "I tried couple of times to learn html/css etc . it was always too boring for me to learn it. Now thanks to prof Jogesh and the course 1 from this specialisation I finally got it! I already did 3 simple working java script apps with bootstrap front-end.That was great approach to teach html/css/javascript. Enough information so we now know what can be done with those tools and where to find more info.Plus i can see now bootstrap glyphicons everywhere:)\n",
      "Enjoyed this class as well! Thank you!Now onto the next class. :)\n",
      "This was a very good course - I especially enjoyed the hands-on exercises and quizzes that helped me to really understand the material.  Once you do something hands-on, you don't easily forget. Thanks to Prof. Jogesh.\n",
      "\n",
      "\n",
      "\n",
      "rubbish extracted\n",
      "ycQnChn3EeWDtQoum3sFeQ~_FnWS4wBEeW7vBKNDxH-cw\n",
      "Thanks a lot for the funny course!\n",
      "\n",
      "I really had a great time in this course. I haven't known what was bower and node.js, but now I'm able to use them.Thanks so much!\n",
      "\n",
      "\n",
      "\n",
      "did not extract\n",
      "ycQnChn3EeWDtQoum3sFeQ~AaOXyoyHEeWrDQr9ZhnP1w\n",
      "Starting next course\n",
      "\n",
      "Is it possible to start next course a little later? Because I'm going to be pretty busy Thanksgiving week\n",
      "How about starting the course a week early, so everyone has a week of flexibility during the course? (Like the last course did.....)\n",
      "\n",
      "\n",
      "\n",
      "rubbish extracted\n",
      "ycQnChn3EeWDtQoum3sFeQ~V3eWdYzBEeWrDQr9ZhnP1w\n",
      "Is there a typo in the Mixins slide?\n",
      "\n",
      "Hi,In the Mixins slide (starting at 6:15) is there a typo in the Less part when using a variable with the @ symbol which is variable declaration for Sass ?\n",
      "\n",
      "\n",
      "\n",
      "rubbish extracted\n",
      "ycQnChn3EeWDtQoum3sFeQ~EaNjLY0YEeWJ8Q5UvS9Vkw\n",
      "Uppercase or lowercase letters in hex colors?\n",
      "\n",
      "Hi,I wonder if it is preferable to use lowercase or uppercase letter in hexadecimal colors?I know that it is exactly the same but just about consistency in the code and for readability what is better?I think that using capital letters is somewhat more linear in appearance (#A6C7E3 vs #a6c7e3).But I also think that lowercase is better to spot letters...What do you think?\n",
      "I like the mixed ones, easier to separate the letters, numbers. I also like to shorthand them, whenever possible: #3377aa -> #37a#000000-> #000I really dont like picking random letters out of a long string\n",
      "I prefer complete and uppercase. It is more readable to me. \n",
      "\n",
      "\n",
      "\n",
      "rubbish extracted\n",
      "ycQnChn3EeWDtQoum3sFeQ~XrVf240hEeWxBRJyFAq9aQ\n",
      "Less is it really less?\n",
      "\n",
      "Hi,As I understand Less allows to generate CSS with some functionnalities of programming languages (like variables, mixins, functions etc.).But when I saw the code from line 56 and 63 (&:hover and &:focus) it seems that the code is similar so is there a way to factorize this code as we made in the .css file with the comma , separated selectors?The code seems even more repetitive in the outer blocks (.navbar-nav>.active>a at line 53, which is the same as .navbar-nav>.open>a at line 65 and so on...)So, is this separation intended for the future in case of using different colors?Because this LESS code does not really appear to be less or even more concise than CSS...But I think that this is due to the lack of practice I cannot see now the full potential of LESS.\n",
      "\n",
      "\n",
      "\n",
      "did not extract\n",
      "ycQnChn3EeWDtQoum3sFeQ~-V8V240mEeWv8BKLWolBUQ\n",
      "Thank you! Is it possible to start next course earlier?\n",
      "\n",
      "As in title, thanks for cool course :) , and is there any option to start next courses earlier?\n",
      "\n",
      "\n",
      "\n",
      "rubbish extracted\n",
      "ycQnChn3EeWDtQoum3sFeQ~Pr-8no6hEeWD3RImE8PCdQ\n",
      "Thank you Professor Jogesh!\n",
      "\n",
      "Great course!  Learned a lot.  Very helpful for understanding and using Bootstrap.  Really enjoyed the intro to Node, NPM and Bower.  Can't wait to learn more about Yeomen and Angular in the next course!\n",
      "\n",
      "\n",
      "\n",
      "did not extract\n",
      "ycQnChn3EeWDtQoum3sFeQ~bBbSu5AmEeWJSQr5zqhI6w\n",
      "Help Professor Jogesh \n",
      "\n",
      "I would make a request that you bring your android course from  here.Because I was unable to complete the Course \n",
      "\n",
      "\n",
      "\n",
      "rubbish extracted\n",
      "ycQnChn3EeWDtQoum3sFeQ~xg-J6JCSEeWuURKvSnbbUQ\n",
      "Assignment 4: problem with assignment statement and grading criteria\n",
      "\n",
      "I'm specifically referring to the 2 grading criteria that look like this:\"The Reserve Table button has been given an id so that it can be referred to appropriately within the JavaScript code.\"The assigment instructions do not ask for ID's being assigned, and the assignment can be implemented without the ID's.  It's especially true if you look at the reference material: I think I'll lose 20 or whatever points just because of that.Also I see most folks did not implement the handling of a click outside of a modal when it's up and showing. I think that part of implementation is way more important than assigning the ID's...Just a rant. This is \"small stuff\" but I think this is worth fixing in the course for the next \"generations\"... Thanks.\n",
      "\n",
      "\n",
      "\n",
      "did not extract\n",
      "ycQnChn3EeWDtQoum3sFeQ~0oBU6ZGNEeWWpRIGHRsuuw\n",
      "No link for Angular JS course yet\n",
      "\n",
      "I have enrolled in the next course of Angular JS. I still have not got link generally sent via email. When I try to use View Course link from enrolled courses, it does not show me course content for this topic. Please help.\n",
      "\n",
      "\n",
      "\n",
      "did not extract\n",
      "ycQnChn3EeWDtQoum3sFeQ~WxuOUpHsEeWy0Q7ABZMsnQ\n",
      "WHEN IS THE NEXT COURSE STARTING???\n",
      "\n",
      "When will he angularJS course start? It's already 23rd November!!!\n",
      "I know, i'm pissed, I was anticipating this course a lot.   I was expecting the curriculum to be made available at least a week before the class actually starts and its the 23rd and the course is still not online??\n",
      "6000 students, $79USD each, about half a million USD... Coursera should be listening to the \"voice of the customer\"... build an extra week of flexibility in each class... no additional variable cost... keep people in the fold.... in the weeks between course 2 and 3 I've been watching a lot of YouTube AngularJS videos..... not bad.... and free..... food for thought......\n",
      "Apparently, it's now starting on the 30th. I read a comment from the teacher on a similar thread.And yep :/ I'm dissapointed too man :/\n",
      "Shubam.. where did you read that it is starting from Nov 30?\n",
      "Sort of gives a \"university feel\" to the program. School reserves the rights to change dates at will; student is an hour late and points disappear......\n",
      "\n",
      "\n",
      "\n",
      "did not extract\n",
      "ycQnChn3EeWDtQoum3sFeQ~KfWmCSVzEeaPnhLgQwuIcQ\n",
      "Unable to install less using npm\n",
      "\n",
      "I am having issues installing less using npm.  I used the -verbose attribute (?) as suggested online to see what was happening. It gets stuck at npm verb addRemoteTarball . One possible problem suggested was the use of https instead of http. So I tried that too and still this problem persists.C:\\Users\\MAHE>npm install less -g -verbosenpm info it worked if it ends with oknpm verb cli [ 'C:\\\\Program Files\\\\nodejs\\\\node.exe',npm verb cli   'C:\\\\Program Files\\\\nodejs\\\\node_modules\\\\npm\\\\bin\\\\npm-cli.js',npm verb cli   'install',npm verb cli   'less',npm verb cli   '-g',npm verb cli   '-verbose' ]npm info using npm@2.15.5npm info using node@v4.4.5npm verb install initial load of C:\\Users\\MAHE\\AppData\\Roaming\\npm\\package.jsonnpm verb readDependencies loading dependencies from C:\\Users\\MAHE\\AppData\\Roaming\\npm\\package.jsonnpm verb cache add spec lessnpm verb addNamed \"latest\" is being treated as a dist-tag for lessnpm info addNameTag [ 'less', 'latest' ]npm verb addNameTag registry:http://registry.npmjs.org/less not in flight; fetchingnpm verb request uri http://registry.npmjs.org/lessnpm verb request no auth needednpm info attempt registry request try #1 at 1:17:21 PMnpm verb request id 248fa514b328f080npm http request GET http://registry.npmjs.org/lessnpm http 200 http://registry.npmjs.org/lessnpm verb headers { server: 'CouchDB/1.5.0 (Erlang OTP/R16B03)',npm verb headers   etag: '\"F2DBNLEA5IFE81HHTCNOM5CV2\"',npm verb headers   'content-encoding': 'gzip',npm verb headers   'cache-control': 'max-age=300',npm verb headers   'accept-ranges': 'bytes',npm verb headers   date: 'Sun, 29 May 2016 07:50:28 GMT',npm verb headers   age: '136',npm verb headers   'x-served-by': 'cache-sin6920-SIN',npm verb headers   'x-cache': 'HIT',npm verb headers   'x-cache-hits': '1',npm verb headers   'x-timer': 'S1464508228.052765,VS0,VE0',npm verb headers   vary: 'Accept-Encoding',npm verb headers   'proxy-connection': 'keep-alive',npm verb headers   'content-type': 'application/json',npm verb headers   'content-length': '13543',npm verb headers   via: '1.1 varnish' }npm verb get saving less to C:\\Users\\MAHE\\AppData\\Roaming\\npm-cache\\registry.npmjs.org\\less\\.cache.jsonnpm verb correctMkdir C:\\Users\\MAHE\\AppData\\Roaming\\npm-cache correctMkdir not in flight; initializingnpm verb makeDirectory C:\\Users\\MAHE\\AppData\\Roaming\\npm-cache creation not in flight; initializingnpm verb makeCacheDir UID & GID are irrelevant on win32npm verb addNamed \"2.7.1\" is a plain semver version for lessnpm verb addRemoteTarball http://registry.npmjs.org/less/-/less-2.7.1.tgz not in flight; addingnpm verb addRemoteTarball [ 'http://registry.npmjs.org/less/-/less-2.7.1.tgz',npm verb addRemoteTarball   '6cbfea22b3b830304e9a5fb371d54fa480c9d7cf' ]-\n",
      "\n",
      "\n",
      "\n",
      "did not extract\n",
      "ycQnChn3EeWDtQoum3sFeQ~VzlNjy13EeaoXg7YgQ2Cfw\n",
      "bower install bootstrap -s  prompts an error\n",
      "\n",
      "Hello Guys,  I am getting the following error, Does anyone know how to fix it?bower install bootstrap -sbower ENOGIT        git is not installed or not in the PATHThanks,\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(test_result)):\n",
    "    if test_result[i] == 1 and pred_result[i] == 0:\n",
    "        print('did not extract')\n",
    "        print(testing_ids[i])\n",
    "        print(texts[id_to_index[testing_ids[i]]])\n",
    "        print('\\n')\n",
    "    elif test_result[i] == 0 and pred_result[i] == 1:\n",
    "        print('rubbish extracted')\n",
    "        print(testing_ids[i])\n",
    "        print(texts[id_to_index[testing_ids[i]]])\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.22781954 -0.09499014 -0.09280555 ...,  0.30542184 -1.25004322\n",
      "   0.52547316]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.77      0.73      0.75        77\n",
      "          1       0.42      0.47      0.44        32\n",
      "\n",
      "avg / total       0.66      0.65      0.66       109\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# exploring clustering coefficient\n",
    "\n",
    "training_vectors_mix = []\n",
    "test_vectors_mix = []\n",
    "\n",
    "training_indices = []\n",
    "test_indices = []\n",
    "\n",
    "super_clustering_score = []\n",
    "normal_clustering_score = []\n",
    "\n",
    "x = np.asarray(tmp_vectors)\n",
    "y = np.asarray(isreplied)\n",
    "\n",
    "for index in range(data_length):\n",
    "    \n",
    "    oneid = ids[index]\n",
    "    origin_list = x[index].tolist()\n",
    "    starter = starter_dic[oneid]\n",
    "    forum_id = thread_forum_dic[oneid]\n",
    "    g = forum_graph_dic[forum_id]\n",
    "    user = starter_dic[oneid]\n",
    "    time = thread_posted_time_dic[oneid]\n",
    "    intervened = is_replied[oneid]\n",
    "    clu = get_chronological_clustering_coef(time, g, user)\n",
    "    if intervened:\n",
    "        super_clustering_score.append(clu)\n",
    "    else:\n",
    "        normal_clustering_score.append(clu)\n",
    "    origin_list.append(clu)\n",
    "\n",
    "    if forum_type_dic[thread_forum_dic[oneid]] <= middleInd:\n",
    "        training_vectors_mix.append(origin_list)\n",
    "\n",
    "    else:\n",
    "        test_vectors_mix.append(origin_list)\n",
    "\n",
    "\n",
    "LogReg = LogisticRegression(class_weight = 'balanced')\n",
    "LogReg.fit(training_vectors_mix, training_result)\n",
    "\n",
    "print(LogReg.coef_)\n",
    "pred_result_mix = LogReg.predict(test_vectors_mix)\n",
    "with open('EDM.txt', 'w') as f:\n",
    "    print(classification_report(test_result, pred_result_mix), file = f)\n",
    "print(classification_report(test_result, pred_result_mix))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAegAAAFJCAYAAABQEL5HAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJiIikhADNBERkYT+P3IyK28awLkMAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x118290f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAecAAAFJCAYAAAChG+XKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngqFxJoQQQgRD40wIIYQIhsaZEEIIEcxfvlbIAQiqOD8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x118200a20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(super_clustering_score, hist = False, rug = True);\n",
    "plt.show()\n",
    "\n",
    "sns.distplot(normal_clustering_score, hist = False, rug = True);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.02091583 -0.02658977 -0.03983184 ...,  0.01638936 -0.0427159\n",
      "  -1.29993875]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.63      0.72       119\n",
      "          1       0.44      0.69      0.54        49\n",
      "\n",
      "avg / total       0.72      0.65      0.66       168\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# exploring pgrank\n",
    "\n",
    "training_vectors_mix = []\n",
    "test_vectors_mix = []\n",
    "\n",
    "training_indices = []\n",
    "test_indices = []\n",
    "\n",
    "\n",
    "x = np.asarray(tmp_vectors)\n",
    "y = np.asarray(isreplied)\n",
    "\n",
    "for index in range(data_length):\n",
    "    \n",
    "    oneid = ids[index]\n",
    "    origin_list = x[index].tolist()\n",
    "    starter = starter_dic[oneid]\n",
    "    forum_id = thread_forum_dic[oneid]\n",
    "    g = forum_digraph_dic[forum_id]\n",
    "    user = starter_dic[oneid]\n",
    "    time = thread_posted_time_dic[oneid]\n",
    "    intervened = is_replied[oneid]\n",
    "    pgrank = get_chronological_pgrank(time, g, user)\n",
    "    \n",
    "    '''\n",
    "    if intervened:\n",
    "        super_clustering_score.append(clu)\n",
    "    else:\n",
    "        normal_clustering_score.append(clu)\n",
    "    origin_list.append(clu)\n",
    "    '''\n",
    "\n",
    "    if forum_type_dic[thread_forum_dic[oneid]] <= middleInd:\n",
    "        training_vectors_mix.append(origin_list)\n",
    "\n",
    "    else:\n",
    "        test_vectors_mix.append(origin_list)\n",
    "\n",
    "\n",
    "LogReg = LogisticRegression(class_weight = 'balanced')\n",
    "LogReg.fit(training_vectors_mix, training_result)\n",
    "\n",
    "print(LogReg.coef_)\n",
    "pred_result_mix = LogReg.predict(test_vectors_mix)\n",
    "with open('EDM.txt', 'w') as f:\n",
    "    print(classification_report(test_result, pred_result_mix), file = f)\n",
    "print(classification_report(test_result, pred_result_mix))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.031446540880503145, 0.0, 0.006535947712418301, 0.0, 0.005847953216374269, 0.0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.005847953216374269, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.006289308176100629, 0.0, 0.006535947712418301, 0.0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.07602339181286549, 0.02564102564102564, 0, 0, 0, 0, 0, 0]\n",
      "[0.07602339181286549, 0.02564102564102564, 0, 0, 0, 0, 0, 0]\n",
      "[0.07602339181286549, 0.02564102564102564, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.006535947712418301, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.03508771929824561, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.032679738562091505, 0.0, 0.07602339181286549, 0.02564102564102564, 0, 0, 0, 0]\n",
      "[0.07017543859649122, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.05847953216374269, 0.022222222222222223, 0, 0, 0, 0, 0, 0]\n",
      "[0.03508771929824561, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.005847953216374269, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.011695906432748537, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.03508771929824561, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.08496732026143791, 0.01282051282051282, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0.0196078431372549, 0.0, 0.017543859649122806, 0.0, 0, 0]\n",
      "[0.006535947712418301, 0.0, 0.005847953216374269, 0.0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.05847953216374269, 0.022222222222222223, 0, 0, 0, 0, 0, 0]\n",
      "[0.029239766081871343, 0.1, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.06918238993710692, 0.0, 0.032679738562091505, 0.0, 0.07602339181286549, 0.02564102564102564, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.005847953216374269, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.029239766081871343, 0.1, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.005847953216374269, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.029239766081871343, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.005847953216374269, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.017543859649122806, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.012578616352201259, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.006289308176100629, 0.0, 0.013071895424836602, 1.0, 0.04093567251461988, 0.047619047619047616, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.04093567251461988, 0.047619047619047616, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.04093567251461988, 0.047619047619047616, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.05847953216374269, 0.022222222222222223, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.011695906432748537, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.032679738562091505, 0.0, 0.011695906432748537, 0.0, 0, 0, 0, 0]\n",
      "[0.05847953216374269, 0.022222222222222223, 0, 0, 0, 0, 0, 0]\n",
      "[0.005847953216374269, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.05847953216374269, 0.022222222222222223, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.005847953216374269, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.032679738562091505, 0.0, 0.011695906432748537, 0.0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.017543859649122806, 0.6666666666666666, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.032679738562091505, 0.0, 0.011695906432748537, 0.0, 0, 0, 0, 0]\n",
      "[0.011695906432748537, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.006289308176100629, 0.0, 0.006535947712418301, 0.0, 0.03508771929824561, 0.0, 0, 0]\n",
      "[0.029239766081871343, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.011695906432748537, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.029239766081871343, 0.1, 0, 0, 0, 0, 0, 0]\n",
      "[0.04678362573099415, 0.14285714285714285, 0, 0, 0, 0, 0, 0]\n",
      "[0.017543859649122806, 0.3333333333333333, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.005847953216374269, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.011695906432748537, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.05660377358490566, 0.0, 0, 0, 0.005847953216374269, 0.0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.017543859649122806, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.01886792452830189, 0.0, 0, 0, 0.005847953216374269, 0.0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.005847953216374269, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0.032679738562091505, 0.0, 0.011695906432748537, 0.0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.011695906432748537, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.005847953216374269, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.029239766081871343, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.006289308176100629, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.029239766081871343, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.005847953216374269, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.005847953216374269, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.011695906432748537, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.005847953216374269, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.017543859649122806, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.011695906432748537, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.08187134502923976, 0.04395604395604396, 0, 0, 0, 0, 0, 0]\n",
      "[0.08187134502923976, 0.04395604395604396, 0, 0, 0, 0, 0, 0]\n",
      "[0.0196078431372549, 0.3333333333333333, 0.005847953216374269, 0.0, 0, 0, 0, 0]\n",
      "[0.013071895424836602, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.029239766081871343, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.011695906432748537, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.005847953216374269, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.011695906432748537, 1.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.006535947712418301, 0.0, 0.011695906432748537, 0.0, 0, 0, 0, 0]\n",
      "[0.011695906432748537, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.006289308176100629, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.005847953216374269, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.017543859649122806, 0.3333333333333333, 0, 0, 0, 0, 0, 0]\n",
      "[0.08187134502923976, 0.04395604395604396, 0, 0, 0, 0, 0, 0]\n",
      "[0.005847953216374269, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.026143790849673203, 0.16666666666666666, 0, 0, 0, 0, 0, 0]\n",
      "[0.023391812865497075, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.08187134502923976, 0.04395604395604396, 0, 0, 0, 0, 0, 0]\n",
      "[0.011695906432748537, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.006289308176100629, 0.0, 0.0196078431372549, 0.0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.005847953216374269, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0.011695906432748537, 0.0, 0, 0, 0, 0]\n",
      "[0.0196078431372549, 0.3333333333333333, 0, 0, 0, 0, 0, 0]\n",
      "[0.005847953216374269, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.011695906432748537, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.029239766081871343, 0.1, 0, 0, 0, 0, 0, 0]\n",
      "[0.026143790849673203, 0.16666666666666666, 0.04093567251461988, 0.047619047619047616, 0, 0, 0, 0]\n",
      "[0.005847953216374269, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.023391812865497075, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.017543859649122806, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.005847953216374269, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.006535947712418301, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.011695906432748537, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.005847953216374269, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.011695906432748537, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.005847953216374269, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.005847953216374269, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.005847953216374269, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.029239766081871343, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.005847953216374269, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.029239766081871343, 0.1, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.005847953216374269, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.005847953216374269, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.017543859649122806, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.005847953216374269, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.08187134502923976, 0.04395604395604396, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.026143790849673203, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.005847953216374269, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.005847953216374269, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.006535947712418301, 0.0, 0.011695906432748537, 0.0, 0, 0, 0, 0]\n",
      "[0.005847953216374269, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.12418300653594772, 0.017543859649122806, 0, 0, 0, 0, 0, 0]\n",
      "[0.032679738562091505, 0.2, 0.005847953216374269, 0.0, 0, 0, 0, 0]\n",
      "[0.005847953216374269, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.01886792452830189, 0.0, 0.026143790849673203, 0.16666666666666666, 0.04093567251461988, 0.047619047619047616, 0, 0]\n",
      "[0.026143790849673203, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.006535947712418301, 0.0, 0.017543859649122806, 0.6666666666666666, 0, 0, 0, 0]\n",
      "[0.006535947712418301, 0.0, 0.017543859649122806, 0.6666666666666666, 0, 0, 0, 0]\n",
      "[0.0196078431372549, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.006535947712418301, 0.0, 0.011695906432748537, 0.0, 0, 0, 0, 0]\n",
      "[0.013071895424836602, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.029239766081871343, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.006535947712418301, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.006535947712418301, 0.0, 0.017543859649122806, 0.6666666666666666, 0, 0, 0, 0]\n",
      "[0.0196078431372549, 0.0, 0.005847953216374269, 0.0, 0, 0, 0, 0]\n",
      "[0.006535947712418301, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.12418300653594772, 0.017543859649122806, 0, 0, 0, 0, 0, 0]\n",
      "[0.12418300653594772, 0.017543859649122806, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.006535947712418301, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.12418300653594772, 0.017543859649122806, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.0196078431372549, 0.3333333333333333, 0.005847953216374269, 0.0, 0, 0, 0, 0]\n",
      "[0.032679738562091505, 0.1, 0.011695906432748537, 0.0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.04575163398692811, 0.047619047619047616, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.22222222222222224, 0.016042780748663103, 0.08187134502923976, 0.04395604395604396, 0, 0, 0, 0]\n",
      "[0.0196078431372549, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.005847953216374269, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.006289308176100629, 0.0, 0.0196078431372549, 0.0, 0, 0, 0, 0]\n",
      "[0.006289308176100629, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.026143790849673203, 0.5, 0, 0, 0, 0, 0, 0]\n",
      "[0.026143790849673203, 0.16666666666666666, 0.005847953216374269, 0.0, 0, 0, 0, 0]\n",
      "[0.026143790849673203, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.032679738562091505, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.013071895424836602, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.0392156862745098, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.12418300653594772, 0.017543859649122806, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.006289308176100629, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.006535947712418301, 0.0, 0.011695906432748537, 0.0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.006289308176100629, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.023391812865497075, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.026143790849673203, 0.16666666666666666, 0.005847953216374269, 0.0, 0, 0, 0, 0]\n",
      "[0.05228758169934641, 0.21428571428571427, 0, 0, 0, 0, 0, 0]\n",
      "[0.011695906432748537, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.04575163398692811, 0.047619047619047616, 0, 0, 0, 0, 0, 0]\n",
      "[0.12418300653594772, 0.017543859649122806, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.006535947712418301, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.013071895424836602, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.08176100628930819, 0.038461538461538464, 0.0392156862745098, 0.06666666666666667, 0.011695906432748537, 1.0, 0, 0]\n",
      "[0.006535947712418301, 0.0, 0.011695906432748537, 0.0, 0, 0, 0, 0]\n",
      "[0.006535947712418301, 0.0, 0.011695906432748537, 0.0, 0, 0, 0, 0]\n",
      "[0.006535947712418301, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.0196078431372549, 0.0, 0.005847953216374269, 0.0, 0, 0, 0, 0]\n",
      "[0.025157232704402517, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.025157232704402517, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.0196078431372549, 0.3333333333333333, 0, 0, 0, 0, 0, 0]\n",
      "[0.013071895424836602, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.22222222222222224, 0.016042780748663103, 0.08187134502923976, 0.04395604395604396, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.032679738562091505, 0.1, 0.011695906432748537, 0.0, 0, 0, 0, 0]\n",
      "[0.01886792452830189, 0.0, 0.006535947712418301, 0.0, 0.017543859649122806, 0.0, 0, 0]\n",
      "[0.006289308176100629, 0.0, 0.12418300653594772, 0.017543859649122806, 0, 0, 0, 0]\n",
      "[0.032679738562091505, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.005847953216374269, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.006289308176100629, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.006535947712418301, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.0196078431372549, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.0196078431372549, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.013071895424836602, 0.0, 0.029239766081871343, 0.0, 0, 0, 0, 0]\n",
      "[0.0196078431372549, 0.0, 0.011695906432748537, 0.0, 0, 0, 0, 0]\n",
      "[0.006535947712418301, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.0196078431372549, 0.0, 0.011695906432748537, 0.0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0.005847953216374269, 0.0, 0, 0]\n",
      "[0.0392156862745098, 0.13333333333333333, 0.023391812865497075, 0.0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.22222222222222224, 0.016042780748663103, 0.08187134502923976, 0.04395604395604396, 0, 0, 0, 0]\n",
      "[0.05228758169934641, 0.03571428571428571, 0.005847953216374269, 0.0, 0, 0, 0, 0]\n",
      "[0.013071895424836602, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.22222222222222224, 0.016042780748663103, 0.08187134502923976, 0.04395604395604396, 0, 0, 0, 0]\n",
      "[0.01886792452830189, 0.3333333333333333, 0, 0, 0, 0, 0, 0]\n",
      "[0.026143790849673203, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.0196078431372549, 0.3333333333333333, 0.017543859649122806, 0.0, 0, 0, 0, 0]\n",
      "[0.013071895424836602, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.013071895424836602, 1.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.006289308176100629, 0.0, 0.0196078431372549, 0.3333333333333333, 0, 0, 0, 0]\n",
      "[0.013071895424836602, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.006535947712418301, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.050314465408805034, 0.07142857142857142, 0.006535947712418301, 0.0, 0, 0, 0, 0]\n",
      "[0.006289308176100629, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.013071895424836602, 0.0, 0.011695906432748537, 0.0, 0, 0, 0, 0]\n",
      "[0.08176100628930819, 0.038461538461538464, 0.0392156862745098, 0.06666666666666667, 0.011695906432748537, 1.0, 0, 0]\n",
      "[0.006535947712418301, 0.0, 0.017543859649122806, 0.0, 0, 0, 0, 0]\n",
      "[0.22222222222222224, 0.016042780748663103, 0.08187134502923976, 0.04395604395604396, 0, 0, 0, 0]\n",
      "[0.005847953216374269, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.025157232704402517, 0.3333333333333333, 0.22222222222222224, 0.016042780748663103, 0.08187134502923976, 0.04395604395604396, 0, 0]\n",
      "[0.006535947712418301, 0.0, 0.023391812865497075, 0.0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0.005847953216374269, 0.0, 0, 0]\n",
      "[0.08176100628930819, 0.038461538461538464, 0.0392156862745098, 0.06666666666666667, 0.011695906432748537, 1.0, 0, 0]\n",
      "[0.012578616352201259, 1.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0.011695906432748537, 0.0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0.006535947712418301, 0.0, 0.017543859649122806, 0.6666666666666666, 0, 0]\n",
      "[0.012578616352201259, 1.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.01886792452830189, 0.0, 0.006535947712418301, 0.0, 0.005847953216374269, 0.0, 0, 0]\n",
      "[0.012578616352201259, 0.0, 0.0196078431372549, 0.3333333333333333, 0.017543859649122806, 0.0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.006535947712418301, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.006289308176100629, 0.0, 0.032679738562091505, 0.2, 0.005847953216374269, 0.0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0.006535947712418301, 0.0, 0.011695906432748537, 0.0, 0, 0]\n",
      "[0, 0, 0.006535947712418301, 0.0, 0.023391812865497075, 0.0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.012578616352201259, 0.0, 0.013071895424836602, 0.0, 0, 0, 0, 0]\n",
      "[0.005847953216374269, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0.005847953216374269, 0.0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.025157232704402517, 0.3333333333333333, 0.22222222222222224, 0.016042780748663103, 0.08187134502923976, 0.04395604395604396, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.006289308176100629, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.012578616352201259, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.006535947712418301, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.006289308176100629, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.01886792452830189, 0.0, 0.006535947712418301, 0.0, 0.005847953216374269, 0.0, 0, 0]\n",
      "[0.012578616352201259, 0.0, 0.05228758169934641, 0.21428571428571427, 0, 0, 0, 0]\n",
      "[0.006535947712418301, 0.0, 0.005847953216374269, 0.0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.0392156862745098, 0.13333333333333333, 0.023391812865497075, 0.0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.01886792452830189, 0.0, 0, 0, 0.011695906432748537, 0.0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.006535947712418301, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.012578616352201259, 0.0, 0.013071895424836602, 0.0, 0, 0, 0, 0]\n",
      "[0.025157232704402517, 0.3333333333333333, 0.22222222222222224, 0.016042780748663103, 0.08187134502923976, 0.04395604395604396, 0, 0]\n",
      "[0.006535947712418301, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.006289308176100629, 0.0, 0.013071895424836602, 1.0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.012578616352201259, 0.0, 0.006535947712418301, 0.0, 0.005847953216374269, 0.0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.025157232704402517, 0.3333333333333333, 0.22222222222222224, 0.016042780748663103, 0.08187134502923976, 0.04395604395604396, 0, 0]\n",
      "[0.025157232704402517, 0.3333333333333333, 0.22222222222222224, 0.016042780748663103, 0.08187134502923976, 0.04395604395604396, 0, 0]\n",
      "[0.025157232704402517, 0.0, 0.006535947712418301, 0.0, 0.017543859649122806, 0.3333333333333333, 0, 0]\n",
      "[0.012578616352201259, 0.0, 0.006535947712418301, 0.0, 0, 0, 0, 0]\n",
      "[0, 0, 0.006535947712418301, 0.0, 0.011695906432748537, 0.0, 0, 0]\n",
      "[0.006535947712418301, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.01886792452830189, 0.0, 0, 0, 0.005847953216374269, 0.0, 0, 0]\n",
      "[0.005847953216374269, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.012578616352201259, 0.0, 0.006535947712418301, 0.0, 0, 0, 0, 0]\n",
      "[0.01886792452830189, 0.0, 0, 0, 0.005847953216374269, 0.0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.032679738562091505, 0.1, 0.011695906432748537, 0.0, 0, 0, 0, 0]\n",
      "[0.012578616352201259, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.012578616352201259, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.032679738562091505, 0.1, 0.011695906432748537, 0.0, 0, 0, 0, 0]\n",
      "[0.006289308176100629, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0.026143790849673203, 0.0, 0, 0, 0, 0]\n",
      "[0.025157232704402517, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.006289308176100629, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0.006535947712418301, 0.0, 0.017543859649122806, 0.0, 0, 0]\n",
      "[0, 0, 0.013071895424836602, 0.0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.006289308176100629, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.04402515723270441, 0.0, 0.006535947712418301, 0.0, 0, 0, 0, 0]\n",
      "[0, 0, 0.0196078431372549, 0.0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.012578616352201259, 0.0, 0, 0, 0.005847953216374269, 0.0, 0, 0]\n",
      "[0.012578616352201259, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.006289308176100629, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.025157232704402517, 0.0, 0.032679738562091505, 0.1, 0.011695906432748537, 0.0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.025157232704402517, 0.0, 0.032679738562091505, 0.1, 0.011695906432748537, 0.0, 0, 0]\n",
      "[0.025157232704402517, 0.0, 0.032679738562091505, 0.1, 0.011695906432748537, 0.0, 0, 0]\n",
      "[0, 0, 0.006535947712418301, 0.0, 0, 0, 0, 0]\n",
      "[0.006289308176100629, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.025157232704402517, 0.0, 0, 0, 0.03508771929824561, 0.06666666666666667, 0, 0]\n",
      "[0.006289308176100629, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.01886792452830189, 0.0, 0.006535947712418301, 0.0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.006289308176100629, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.012578616352201259, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.006289308176100629, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.025157232704402517, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.031446540880503145, 0.1, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.09433962264150944, 0.009523809523809525, 0, 0, 0, 0, 0, 0]\n",
      "[0.050314465408805034, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0.03508771929824561, 0.0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.0196078431372549, 0.0, 0.03508771929824561, 0.0, 0, 0, 0, 0]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.72      0.79      0.75        77\n",
      "          1       0.33      0.25      0.29        32\n",
      "\n",
      "avg / total       0.60      0.63      0.62       109\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.75      0.74      0.75        77\n",
      "          1       0.39      0.41      0.40        32\n",
      "\n",
      "avg / total       0.65      0.64      0.64       109\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_vectors_mix = []\n",
    "test_vectors_mix = []\n",
    "\n",
    "training_vectors_cen = []\n",
    "test_vectors_cen = []\n",
    "\n",
    "training_indices = []\n",
    "test_indices = []\n",
    "\n",
    "\n",
    "\n",
    "for index in range(data_length):\n",
    "    li = []\n",
    "    oneid = ids[index]\n",
    "    origin_list = x_edm[index].tolist()\n",
    "    starter = starter_dic[oneid]\n",
    "    forum_id = thread_forum_dic[oneid]\n",
    "    forum_index = forum_type_dic[forum_id]\n",
    "    for i in range(0, 4):\n",
    "        current_forum_index = forum_index - i\n",
    "        if current_forum_index <= 0:\n",
    "            li.append(0)\n",
    "            li.append(0)\n",
    "            continue\n",
    "        else:\n",
    "            g = forum_graph_dic[forum_id_list[current_forum_index]]\n",
    "            cent = nx.degree_centrality(g)\n",
    "            clu = nx.clustering(g)\n",
    "            #modified\n",
    "            if starter in cent:\n",
    "                li.append(cent[starter])\n",
    "                li.append(clu[starter])\n",
    "            else:\n",
    "                li.append(0)\n",
    "                li.append(0)\n",
    "\n",
    "\n",
    "    print(li)\n",
    "    new_list = origin_list + li\n",
    "\n",
    "    if forum_type_dic[thread_forum_dic[oneid]] <= middleInd:\n",
    "        training_vectors_mix.append(new_list)\n",
    "        training_vectors_cen.append(li)\n",
    "\n",
    "    else:\n",
    "        test_vectors_mix.append(new_list)\n",
    "        test_vectors_cen.append(li)\n",
    "\n",
    "\n",
    "\n",
    "#x = np.asarray(latest_vectors)      \n",
    "y = np.asarray(isreplied)\n",
    "\n",
    "'''\n",
    "kf = KFold(data_length, n_folds = 5, shuffle=False, random_state = 18)\n",
    "print(kf)\n",
    "for train_index, test_index in kf:\n",
    "    x_train, x_test = x[train_index], x[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "'''  \n",
    "LogReg = LogisticRegression(class_weight = 'balanced')\n",
    "LogReg.fit(training_vectors_cen, training_result)\n",
    "pred_result_cen = LogReg.predict(test_vectors_cen)\n",
    "with open('EDM.txt', 'w') as f:\n",
    "    print(classification_report(test_result, pred_result_cen), file = f)\n",
    "print(classification_report(test_result, pred_result_cen))\n",
    "\n",
    "\n",
    "\n",
    "LogReg = LogisticRegression(class_weight = 'balanced')\n",
    "LogReg.fit(training_vectors_mix, training_result)\n",
    "pred_result_mix = LogReg.predict(test_vectors_mix)\n",
    "with open('EDM.txt', 'w') as f:\n",
    "    print(classification_report(test_result, pred_result_mix), file = f)\n",
    "print(classification_report(test_result, pred_result_mix))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.13889355 -0.083231   -0.03194887 ...,  0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(LogReg.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{9400121: 0.05002841573897636, 3637530: 0.05029585798816568, 124313: 0.050345234093795294, 541944: 0.05067174870921526, 3677: 0.05091847698037227, 5534303: 0.0541665295266197, 13296001: 0.0546357099465717, 6935156: 0.05578220073918687, 11814913: 0.056224724808254786, 14925104: 0.057416267942583726, 5146477: 0.06220095693779904, 5781489: 0.06339174849146433, 8963841: 0.06440520749882908, 11822375: 0.06461726902821757, 15597251: 0.06472733610901375, 6206340: 0.06546896686195389, 15052810: 0.06687119284698288, 15731817: 0.07251127634182558, 3868043: 0.07309253463874824, 5621541: 0.0736970547922308, 15199263: 0.07828508096126294, 151006: 0.08952844593036649, 7470685: 0.09290358788901258, 3853969: 0.09725469749509735, 4404: 0.10377982912807587, 4375186: 0.10781750985420463, 15662154: 0.11940140946324501, 13157464: 0.1301775147928994, 14893116: 0.19333024323252423, 11069561: 0.23441599886532868}\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "\n",
    "user_cent_sum_dic = {}\n",
    "for forum_id in forum_graph_dic:\n",
    "    cent = nx.degree_centrality(forum_graph_dic[forum_id])\n",
    "    for user in cent:\n",
    "        if user in user_table and user_table[user] != 'Student':\n",
    "            continue\n",
    "            \n",
    "        if user not in user_cent_sum_dic:\n",
    "            user_cent_sum_dic[user] = 0\n",
    "        user_cent_sum_dic[user] += cent[user]\n",
    " \n",
    "user_cent_sum_dic = sorted(user_cent_sum_dic.items(), key=operator.itemgetter(1))\n",
    "good_student_dic = {}\n",
    "for tup in user_cent_sum_dic:\n",
    "    if tup[1] > 0.05:\n",
    "        good_student_dic[tup[0]] = tup[1]\n",
    "\n",
    "print(good_student_dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we try to run the code again but with extra truncation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# demographics of all threads\n",
    "course_name_set = set()\n",
    "course_id_set = set()\n",
    "course_num_weeks_dic = {}\n",
    "course_num_threads_dic = {}\n",
    "course_num_posts_dic = {}\n",
    "course_num_comments_dic = {}\n",
    "course_num_pos_threads_dic = {}\n",
    "course_num_neg_threads_dic = {}\n",
    "\n",
    "\n",
    "\n",
    "conn = util.create_connection(database)\n",
    "with conn:\n",
    "    cur = conn.cursor()\n",
    "    forum_message = 'select id, forumname, courseid, coursename, numthreads from forum'\n",
    "    cur.execute(forum_message)\n",
    "    forums = cur.fetchall()\n",
    "    for each_forum in forums:\n",
    "        \n",
    "        # All the info from this select operation\n",
    "        forum_id = each_forum[0]\n",
    "        forum_name = each_forum[1]\n",
    "        course_id = each_forum[2]\n",
    "        course_name = each_forum[3]\n",
    "        num_threads = each_forum[4]\n",
    "        course_name_set.add(course_name)\n",
    "        course_id_set.add(course_id)\n",
    "        if course_name not in course_num_threads_dic:\n",
    "            \n",
    "            course_num_weeks_dic[course_name] = 0\n",
    "            course_num_threads_dic[course_name] = 0\n",
    "            course_num_neg_threads_dic[course_name] = 0\n",
    "            course_num_pos_threads_dic[course_name] = 0\n",
    "            course_num_posts_dic[course_name] = 0\n",
    "            course_num_comments_dic[course_name] = 0\n",
    "        \n",
    "        if 'Week' in forum_name:\n",
    "            \n",
    "            course_num_weeks_dic[course_name] += 1\n",
    "            course_num_threads_dic[course_name] += num_threads\n",
    "            thread_message = 'select inst_replied from thread where forumid == \\''\n",
    "            thread_message += forum_id\n",
    "            thread_message += '\\''\n",
    "            cur.execute(thread_message)\n",
    "            firsts = cur.fetchall()\n",
    "            for inst in firsts:\n",
    "                if inst[0] == 1:\n",
    "                    course_num_pos_threads_dic[course_name] += 1\n",
    "                else:\n",
    "                    course_num_neg_threads_dic[course_name] += 1\n",
    "\n",
    "            post_message = 'select * from post where forumid == \\''\n",
    "            post_message += forum_id\n",
    "            post_message += '\\''\n",
    "            cur.execute(post_message)\n",
    "            seconds = cur.fetchall()\n",
    "            course_num_posts_dic[course_name] += len(seconds)\n",
    "            \n",
    "            comment_message = 'select * from comment where forumid == \\''\n",
    "            comment_message += forum_id\n",
    "            comment_message += '\\''\n",
    "            cur.execute(comment_message)\n",
    "            thirds = cur.fetchall()\n",
    "            course_num_comments_dic[course_name] += len(thirds)\n",
    "\n",
    "\n",
    "        \n",
    "course_num_threads_dic = sorted(course_num_threads_dic.items(), key=operator.itemgetter(1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wharton-operations-analytics; num of threads: 0; num of pos: 0; num of neg: 0; num of posts: 0\n",
      "corporate-entrepreneurs-opportunity; num of threads: 1; num of pos: 0; num of neg: 1; num of posts: 2\n",
      "motion-and-kinetics; num of threads: 1; num of pos: 1; num of neg: 0; num of posts: 6\n",
      "wharton-people-analytics; num of threads: 1; num of pos: 0; num of neg: 1; num of posts: 1\n",
      "cloud-applications-part1; num of threads: 2; num of pos: 0; num of neg: 2; num of posts: 36\n",
      "accounting-analytics; num of threads: 3; num of pos: 1; num of neg: 2; num of posts: 6\n",
      "competitive-strategy; num of threads: 3; num of pos: 3; num of neg: 0; num of posts: 6\n",
      "photography-techniques; num of threads: 3; num of pos: 0; num of neg: 3; num of posts: 15\n",
      "text-mining; num of threads: 3; num of pos: 2; num of neg: 1; num of posts: 7\n",
      "typography; num of threads: 3; num of pos: 0; num of neg: 3; num of posts: 5\n",
      "weight-loss-plan; num of threads: 3; num of pos: 0; num of neg: 3; num of posts: 271\n",
      "how-to-write-a-resume; num of threads: 4; num of pos: 0; num of neg: 4; num of posts: 59\n",
      "professional-emails-english; num of threads: 4; num of pos: 0; num of neg: 4; num of posts: 283\n",
      "advanced-excel; num of threads: 5; num of pos: 0; num of neg: 5; num of posts: 119\n",
      "datavisualization; num of threads: 5; num of pos: 0; num of neg: 5; num of posts: 22\n",
      "photo-composition; num of threads: 5; num of pos: 0; num of neg: 5; num of posts: 6\n",
      "teach-light-color; num of threads: 5; num of pos: 1; num of neg: 4; num of posts: 8\n",
      "bioinformatics; num of threads: 6; num of pos: 0; num of neg: 6; num of posts: 9\n",
      "image-making; num of threads: 6; num of pos: 1; num of neg: 5; num of posts: 143\n",
      "antimicrobial-resistance; num of threads: 7; num of pos: 5; num of neg: 2; num of posts: 30\n",
      "cs-tech-interview; num of threads: 7; num of pos: 0; num of neg: 7; num of posts: 48\n",
      "excel-analysis; num of threads: 7; num of pos: 1; num of neg: 6; num of posts: 240\n",
      "ignite-creativity; num of threads: 7; num of pos: 1; num of neg: 6; num of posts: 450\n",
      "algorithmic-thinking-1; num of threads: 8; num of pos: 8; num of neg: 0; num of posts: 20\n",
      "camera-control; num of threads: 8; num of pos: 3; num of neg: 5; num of posts: 15\n",
      "decision-making; num of threads: 8; num of pos: 0; num of neg: 8; num of posts: 585\n",
      "clinical-terminology; num of threads: 9; num of pos: 2; num of neg: 7; num of posts: 40\n",
      "digital-analytics; num of threads: 9; num of pos: 1; num of neg: 8; num of posts: 145\n",
      "engineering-mechanics-statics; num of threads: 9; num of pos: 6; num of neg: 3; num of posts: 26\n",
      "plato-dialogues; num of threads: 9; num of pos: 0; num of neg: 9; num of posts: 16\n",
      "business-english-intro; num of threads: 10; num of pos: 0; num of neg: 10; num of posts: 403\n",
      "how-to-write-a-scientific-paper; num of threads: 10; num of pos: 0; num of neg: 10; num of posts: 186\n",
      "neuromarketing; num of threads: 10; num of pos: 0; num of neg: 10; num of posts: 28\n",
      "fundamentals-of-graphic-design; num of threads: 11; num of pos: 1; num of neg: 10; num of posts: 23\n",
      "negotiation-skills; num of threads: 11; num of pos: 2; num of neg: 9; num of posts: 39\n",
      "personal-branding; num of threads: 11; num of pos: 3; num of neg: 8; num of posts: 772\n",
      "sales-strategies; num of threads: 11; num of pos: 0; num of neg: 11; num of posts: 809\n",
      "advanced-data-structures; num of threads: 12; num of pos: 3; num of neg: 9; num of posts: 47\n",
      "how-to-create-a-website; num of threads: 12; num of pos: 0; num of neg: 12; num of posts: 322\n",
      "wharton-contagious-viral-marketing; num of threads: 12; num of pos: 1; num of neg: 11; num of posts: 15\n",
      "wharton-customer-analytics; num of threads: 12; num of pos: 3; num of neg: 9; num of posts: 20\n",
      "android-app; num of threads: 13; num of pos: 4; num of neg: 9; num of posts: 642\n",
      "java-programming-arrays-lists-data; num of threads: 13; num of pos: 9; num of neg: 4; num of posts: 34\n",
      "prresearch; num of threads: 13; num of pos: 10; num of neg: 3; num of posts: 40\n",
      "american-law; num of threads: 14; num of pos: 13; num of neg: 1; num of posts: 37\n",
      "cancer; num of threads: 14; num of pos: 9; num of neg: 5; num of posts: 30\n",
      "clinical-trials; num of threads: 14; num of pos: 0; num of neg: 14; num of posts: 236\n",
      "data-structures; num of threads: 14; num of pos: 1; num of neg: 13; num of posts: 520\n",
      "guitar; num of threads: 14; num of pos: 0; num of neg: 14; num of posts: 33\n",
      "journalism; num of threads: 14; num of pos: 10; num of neg: 4; num of posts: 189\n",
      "microeconomics; num of threads: 14; num of pos: 0; num of neg: 14; num of posts: 305\n",
      "big-data-management; num of threads: 15; num of pos: 3; num of neg: 12; num of posts: 417\n",
      "excel-data-analysis; num of threads: 15; num of pos: 4; num of neg: 11; num of posts: 35\n",
      "ml-clustering-and-retrieval; num of threads: 15; num of pos: 13; num of neg: 2; num of posts: 37\n",
      "plantknows; num of threads: 15; num of pos: 10; num of neg: 5; num of posts: 15\n",
      "cyber-security-domain; num of threads: 16; num of pos: 8; num of neg: 8; num of posts: 36\n",
      "global-diplomacy; num of threads: 16; num of pos: 3; num of neg: 13; num of posts: 46\n",
      "hadoop; num of threads: 16; num of pos: 3; num of neg: 13; num of posts: 215\n",
      "html; num of threads: 16; num of pos: 0; num of neg: 16; num of posts: 52\n",
      "intro-programming; num of threads: 16; num of pos: 11; num of neg: 5; num of posts: 48\n",
      "marketing-digital; num of threads: 16; num of pos: 4; num of neg: 12; num of posts: 135\n",
      "supply-chain-logistics; num of threads: 16; num of pos: 1; num of neg: 15; num of posts: 934\n",
      "build-a-computer; num of threads: 17; num of pos: 12; num of neg: 5; num of posts: 77\n",
      "data-visualization-tableau; num of threads: 17; num of pos: 2; num of neg: 15; num of posts: 709\n",
      "introcss; num of threads: 17; num of pos: 6; num of neg: 11; num of posts: 42\n",
      "learn-chinese; num of threads: 17; num of pos: 9; num of neg: 8; num of posts: 64\n",
      "photography; num of threads: 17; num of pos: 7; num of neg: 10; num of posts: 333\n",
      "algorithms-greedy; num of threads: 18; num of pos: 0; num of neg: 18; num of posts: 49\n",
      "erasmus-econometrics; num of threads: 18; num of pos: 8; num of neg: 10; num of posts: 99\n",
      "exposure-photography; num of threads: 18; num of pos: 12; num of neg: 6; num of posts: 36\n",
      "logical-fallacies; num of threads: 18; num of pos: 0; num of neg: 18; num of posts: 231\n",
      "moralities; num of threads: 18; num of pos: 4; num of neg: 14; num of posts: 56\n",
      "single-variable-calculus; num of threads: 18; num of pos: 18; num of neg: 0; num of posts: 48\n",
      "systematic-review; num of threads: 18; num of pos: 12; num of neg: 6; num of posts: 42\n",
      "website-coding; num of threads: 18; num of pos: 16; num of neg: 2; num of posts: 53\n",
      "algorithms-divide-conquer; num of threads: 19; num of pos: 4; num of neg: 15; num of posts: 49\n",
      "food-and-health; num of threads: 19; num of pos: 13; num of neg: 6; num of posts: 66\n",
      "gamification; num of threads: 19; num of pos: 9; num of neg: 10; num of posts: 585\n",
      "magic-middle-ages; num of threads: 19; num of pos: 1; num of neg: 18; num of posts: 81\n",
      "c-plus-plus-a; num of threads: 20; num of pos: 11; num of neg: 9; num of posts: 43\n",
      "ml-classification; num of threads: 20; num of pos: 9; num of neg: 11; num of posts: 34\n",
      "bayesian-statistics; num of threads: 21; num of pos: 5; num of neg: 16; num of posts: 196\n",
      "big-data-machine-learning; num of threads: 21; num of pos: 4; num of neg: 17; num of posts: 377\n",
      "r-programming-environment; num of threads: 21; num of pos: 0; num of neg: 21; num of posts: 40\n",
      "modern-art-ideas; num of threads: 22; num of pos: 9; num of neg: 13; num of posts: 596\n",
      "uva-darden-market-analytics; num of threads: 22; num of pos: 12; num of neg: 10; num of posts: 350\n",
      "uva-darden-project-management; num of threads: 22; num of pos: 9; num of neg: 13; num of posts: 82\n",
      "algorithms-on-graphs; num of threads: 23; num of pos: 0; num of neg: 23; num of posts: 217\n",
      "financial-markets; num of threads: 23; num of pos: 0; num of neg: 23; num of posts: 82\n",
      "research-methods; num of threads: 23; num of pos: 19; num of neg: 4; num of posts: 375\n",
      "analytics-tableau; num of threads: 24; num of pos: 10; num of neg: 14; num of posts: 326\n",
      "develop-your-musicianship; num of threads: 24; num of pos: 15; num of neg: 9; num of posts: 60\n",
      "inferential-statistics-intro; num of threads: 25; num of pos: 23; num of neg: 2; num of posts: 94\n",
      "mafash; num of threads: 25; num of pos: 8; num of neg: 17; num of posts: 111\n",
      "parprog1; num of threads: 25; num of pos: 12; num of neg: 13; num of posts: 89\n",
      "mandarin-chinese-1; num of threads: 26; num of pos: 6; num of neg: 20; num of posts: 652\n",
      "ml-regression; num of threads: 26; num of pos: 5; num of neg: 21; num of posts: 118\n",
      "english-principles; num of threads: 27; num of pos: 2; num of neg: 25; num of posts: 62\n",
      "java-for-android; num of threads: 28; num of pos: 24; num of neg: 4; num of posts: 76\n",
      "bayesian; num of threads: 29; num of pos: 15; num of neg: 14; num of posts: 139\n",
      "law-student; num of threads: 29; num of pos: 0; num of neg: 29; num of posts: 225\n",
      "business; num of threads: 31; num of pos: 8; num of neg: 23; num of posts: 109\n",
      "careerdevelopment; num of threads: 31; num of pos: 12; num of neg: 19; num of posts: 117\n",
      "analytics-business-metrics; num of threads: 34; num of pos: 20; num of neg: 14; num of posts: 905\n",
      "understanding-arguments; num of threads: 34; num of pos: 0; num of neg: 34; num of posts: 3677\n",
      "analytics-excel; num of threads: 35; num of pos: 22; num of neg: 13; num of posts: 458\n",
      "memory-and-movies; num of threads: 35; num of pos: 29; num of neg: 6; num of posts: 185\n",
      "dsp; num of threads: 36; num of pos: 29; num of neg: 7; num of posts: 81\n",
      "game-development; num of threads: 36; num of pos: 3; num of neg: 33; num of posts: 144\n",
      "single-page-web-apps-with-angularjs; num of threads: 36; num of pos: 28; num of neg: 8; num of posts: 85\n",
      "robotics-flight; num of threads: 39; num of pos: 3; num of neg: 36; num of posts: 86\n",
      "database-management; num of threads: 40; num of pos: 0; num of neg: 40; num of posts: 76\n",
      "classical-composition; num of threads: 41; num of pos: 2; num of neg: 39; num of posts: 147\n",
      "public-relations; num of threads: 42; num of pos: 14; num of neg: 28; num of posts: 158\n",
      "crypto; num of threads: 43; num of pos: 30; num of neg: 13; num of posts: 161\n",
      "big-data-introduction; num of threads: 44; num of pos: 22; num of neg: 22; num of posts: 2084\n",
      "probabilistic-graphical-models; num of threads: 44; num of pos: 10; num of neg: 34; num of posts: 126\n",
      "teach-online; num of threads: 45; num of pos: 27; num of neg: 18; num of posts: 859\n",
      "game-programming; num of threads: 53; num of pos: 8; num of neg: 45; num of posts: 148\n",
      "probability-intro; num of threads: 54; num of pos: 43; num of neg: 11; num of posts: 247\n",
      "analytics-mysql; num of threads: 58; num of pos: 21; num of neg: 37; num of posts: 199\n",
      "android-programming; num of threads: 59; num of pos: 2; num of neg: 57; num of posts: 136\n",
      "ml-foundations; num of threads: 59; num of pos: 15; num of neg: 44; num of posts: 293\n",
      "programming-languages; num of threads: 62; num of pos: 30; num of neg: 32; num of posts: 369\n",
      "science-of-meditation; num of threads: 67; num of pos: 56; num of neg: 11; num of posts: 249\n",
      "uva-darden-design-thinking-innovation; num of threads: 69; num of pos: 35; num of neg: 34; num of posts: 220\n",
      "interactive-python-1; num of threads: 72; num of pos: 22; num of neg: 50; num of posts: 415\n",
      "algorithmic-toolbox; num of threads: 80; num of pos: 31; num of neg: 49; num of posts: 448\n",
      "object-oriented-java; num of threads: 81; num of pos: 21; num of neg: 60; num of posts: 482\n",
      "python-databases; num of threads: 103; num of pos: 30; num of neg: 73; num of posts: 482\n",
      "algorithm-design-analysis; num of threads: 114; num of pos: 21; num of neg: 93; num of posts: 418\n",
      "algorithms-part1; num of threads: 133; num of pos: 25; num of neg: 108; num of posts: 336\n",
      "calculus1; num of threads: 150; num of pos: 74; num of neg: 76; num of posts: 315\n",
      "html-css-javascript; num of threads: 151; num of pos: 3; num of neg: 148; num of posts: 441\n",
      "python-network-data; num of threads: 205; num of pos: 145; num of neg: 60; num of posts: 564\n",
      "learning-how-to-learn; num of threads: 238; num of pos: 49; num of neg: 189; num of posts: 1127\n",
      "python; num of threads: 263; num of pos: 171; num of neg: 92; num of posts: 804\n",
      "hybrid-mobile-development; num of threads: 357; num of pos: 75; num of neg: 282; num of posts: 1013\n",
      "server-side-development; num of threads: 526; num of pos: 190; num of neg: 336; num of posts: 1419\n",
      "web-frameworks; num of threads: 538; num of pos: 189; num of neg: 349; num of posts: 1688\n",
      "angular-js; num of threads: 784; num of pos: 142; num of neg: 642; num of posts: 2242\n",
      "machine-learning; num of threads: 2177; num of pos: 1486; num of neg: 691; num of posts: 6060\n"
     ]
    }
   ],
   "source": [
    "course_column = []\n",
    "week_column = []\n",
    "thread_column = []\n",
    "pos_column = []\n",
    "neg_column = []\n",
    "post_column = []\n",
    "comment_column = []\n",
    "ratio_column = []\n",
    "\n",
    "for tup in course_num_threads_dic:\n",
    "    print(tup[0] + '; num of threads: ' + str(tup[1]) + '; num of pos: ' + str(course_num_pos_threads_dic[tup[0]]) + '; num of neg: ' + str(course_num_neg_threads_dic[tup[0]]) + '; num of posts: ' + str(course_num_posts_dic[tup[0]]))\n",
    "    course_column.append(tup[0])\n",
    "    week_column.append(course_num_weeks_dic[tup[0]])\n",
    "    thread_column.append(tup[1])\n",
    "    pos_column.append(course_num_pos_threads_dic[tup[0]])\n",
    "    neg_column.append(course_num_neg_threads_dic[tup[0]])\n",
    "    post_column.append(course_num_posts_dic[tup[0]])\n",
    "    comment_column.append(course_num_comments_dic[tup[0]])\n",
    "    if course_num_neg_threads_dic[tup[0]] == 0:\n",
    "        ratio_column.append('N')\n",
    "    else:\n",
    "        ratio_column.append(float(course_num_pos_threads_dic[tup[0]]) / course_num_neg_threads_dic[tup[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dic = {}\n",
    "\n",
    "data_dic['course'] = course_column\n",
    "data_dic['week'] = week_column\n",
    "data_dic['threads'] = thread_column\n",
    "data_dic['pos_threads'] = pos_column\n",
    "data_dic['neg_threads'] = neg_column\n",
    "data_dic['ratio'] = ratio_column\n",
    "data_dic['posts'] = post_column\n",
    "data_dic['comments'] = comment_column\n",
    "\n",
    "\n",
    "df = DataFrame(data_dic)\n",
    "writer = pd.ExcelWriter('demographical_data.xlsx', engine='xlsxwriter')\n",
    "df.to_excel(writer, sheet_name='Sheet1')\n",
    "writer.save()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "bower does not work\n",
    "\n",
    "<co-content><text>Hi Guys,</text><text>I'm trying to install angular route ( I'm working on windows ), but when I type in 'bower install....' then I get the error message \"bower is not recognised as an internal or external command, operable program or batch file\". Even when I type in the full path to where bower is, I get the same message.</text><text>Does anyone know what is going on?</text><text>Thankx!</text><text>Niki</text></co-content>\n",
    "\n",
    "<co-content><text>You have probably have not installed bower globally</text><text>Try to open cmd and type: <strong>npm install -g bower </strong></text><text>Make sure you are running the command at the location that bower.json lives / presents.</text></co-content>\n",
    "\n",
    "<co-content><text>yup, that did the trick.</text><text/><text>Thank you!</text></co-content>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = [[1,2,17],[3,4,5]]\n",
    "a = np.asarray(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.31622777,  0.4472136 ,  0.9593655 ],\n",
       "       [ 0.9486833 ,  0.89442719,  0.28216632]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.preprocessing.normalize(a, norm='l2', axis=0, copy=True, return_norm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2, 17],\n",
       "       [ 3,  4,  5]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
